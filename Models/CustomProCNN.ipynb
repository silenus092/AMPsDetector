{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48641189/fitting-3d-data-as-input-into-keras-sequential-model-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ubuntu/miniconda3/envs/py3\n",
      "\n",
      "  added / updated specs:\n",
      "    - keras=2.3.1\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2020.12.5          |   py37h06a4308_0         141 KB\n",
      "    keras-2.3.1                |                0          12 KB\n",
      "    keras-base-2.3.1           |           py37_0         495 KB\n",
      "    openssl-1.1.1i             |       h27cfd23_0         2.5 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  keras-base         pkgs/main/linux-64::keras-base-2.3.1-py37_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2020.12.~ --> pkgs/main::ca-certificates-2021.1.19-h06a4308_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge::certifi-2020.12.5-py37h8~ --> pkgs/main::certifi-2020.12.5-py37h06a4308_0\n",
      "  keras                conda-forge/noarch::keras-2.4.3-py_0 --> pkgs/main/linux-64::keras-2.3.1-0\n",
      "  openssl            conda-forge::openssl-1.1.1i-h7f98852_0 --> pkgs/main::openssl-1.1.1i-h27cfd23_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.1.1i       | 2.5 MB    | ##################################### | 100% \n",
      "keras-base-2.3.1     | 495 KB    | ##################################### | 100% \n",
      "keras-2.3.1          | 12 KB     | ##################################### | 100% \n",
      "certifi-2020.12.5    | 141 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install keras=2.3.1 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Use only CPU\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15360368458749329693\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16134426514430374056\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11756146409991432083\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 31595870336\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14256696921772055789\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:00:06.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4aBzk8QXHS9S"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "\n",
    "#keras\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv1D, Add, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GlobalMaxPooling1D\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# CuDNNLSTM error; The error was because from TensorFlow 2 you do not need to specify CuDNNLSTM. \n",
    "# You can just use LSTM with no activation function and it will automatically use the CuDNN version. You do have to install CuDNN first.\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FNhtXHJrfEE4"
   },
   "outputs": [],
   "source": [
    "# small old ../datasets/AMPsNonAMPs_df.239.plk\n",
    "# /home/ubuntu/data/AMPsNonAMPs_df.plk old dataset\n",
    "# /mnt/vdb/thesis/jax/AMPNonAMP.final.reps new dataset\n",
    "import pickle5 as pickle\n",
    "with open( \"/mnt/vdb/thesis/jax/AMPNonAMP.V4_sim95.reps\", 'rb') as file:\n",
    "    AMPs_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "mGiMSzzPfj2t",
    "outputId": "092eb611-b89c-4cac-c8c4-0557799510cf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>length</th>\n",
       "      <th>class</th>\n",
       "      <th>reps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>10015_dbaasp,10016_dbaasp|dbaasp_peptides</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.20464208722114563, -0.055944692343473434, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>10026_dbaasp|dbaasp_peptides</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1005186140537262, 0.0014500601682811975, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>10029_dbaasp|dbaasp_peptides</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14606480300426483, 0.04153195396065712, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>1003,1011,1019,1027,1035|CancerPPD_l_natural</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.02989775501191616, -0.004465686157345772, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>10030_dbaasp|dbaasp_peptides</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.11731283366680145, 0.022457238286733627, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>dbAMP_12148</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2212764024734497, 0.15402714908123016, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>dbAMP_12158</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.07279127091169357, 0.05830632895231247, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>dbAMP_12161</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.17415067553520203, 0.11548949033021927, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>dbAMP_12203</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.03923531994223595, -0.0253727026283741, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dbAMP_12362</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.01778259314596653, -0.03611695393919945, 7....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22910 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ID  length  class  \\\n",
       "1111     10015_dbaasp,10016_dbaasp|dbaasp_peptides      11      0   \n",
       "973                   10026_dbaasp|dbaasp_peptides      22      0   \n",
       "524                   10029_dbaasp|dbaasp_peptides      14      0   \n",
       "1979  1003,1011,1019,1027,1035|CancerPPD_l_natural      20      0   \n",
       "1917                  10030_dbaasp|dbaasp_peptides      18      0   \n",
       "...                                            ...     ...    ...   \n",
       "1609                                   dbAMP_12148      13      0   \n",
       "617                                    dbAMP_12158      17      0   \n",
       "1051                                   dbAMP_12161      16      0   \n",
       "1821                                   dbAMP_12203      15      0   \n",
       "15                                     dbAMP_12362      30      0   \n",
       "\n",
       "                                                   reps  \n",
       "1111  [0.20464208722114563, -0.055944692343473434, 0...  \n",
       "973   [0.1005186140537262, 0.0014500601682811975, 0....  \n",
       "524   [0.14606480300426483, 0.04153195396065712, 0.0...  \n",
       "1979  [0.02989775501191616, -0.004465686157345772, -...  \n",
       "1917  [0.11731283366680145, 0.022457238286733627, 0....  \n",
       "...                                                 ...  \n",
       "1609  [0.2212764024734497, 0.15402714908123016, 0.12...  \n",
       "617   [0.07279127091169357, 0.05830632895231247, -0....  \n",
       "1051  [0.17415067553520203, 0.11548949033021927, 0.0...  \n",
       "1821  [0.03923531994223595, -0.0253727026283741, -0....  \n",
       "15    [0.01778259314596653, -0.03611695393919945, 7....  \n",
       "\n",
       "[22910 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AMPs_df.drop_duplicates(subset=['Sequence'],inplace=True)\n",
    "AMPs_df =AMPs_df[AMPs_df[\"length\"] <=30 ]\n",
    "AMPs_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2i7Tk41aDZ5"
   },
   "source": [
    "### Utility function: plot_history, display_model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7F7ykQsDVxHO"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  # dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  x = range(1, len(acc) + 1)\n",
    "\n",
    "  plt.figure(figsize=(12, 5))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(x, acc, 'b', label='Training acc')\n",
    "  plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(x, loss, 'b', label='Training loss')\n",
    "  plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.legend()\n",
    "\n",
    "# Display model score(Loss & Accuracy) across all sets.\n",
    "def display_model_score(model, train, val, test):\n",
    "  train_score = model.evaluate(train[0], train[1], verbose=1)\n",
    "  print('Train loss: ', train_score[0])\n",
    "  print('Train accuracy: ', train_score[1])\n",
    "  print('-'*70)\n",
    "  val_score = model.evaluate(val[0], val[1], verbose=1)\n",
    "  print('Val loss: ', val_score[0])\n",
    "  print('Val accuracy: ', val_score[1])\n",
    "  print('-'*70)\n",
    "  test_score = model.evaluate(test[0], test[1], verbose=1)\n",
    "  print('Test loss: ', test_score[0])\n",
    "  print('Test accuracy: ', test_score[1])\n",
    "\n",
    "def plot_history_CV(cv, estimator,x,y):\n",
    "  # plot arrows\n",
    "  fig1 = plt.figure(figsize=[12,12])\n",
    "  ax1 = fig1.add_subplot(111,aspect = 'equal')\n",
    "  ax1.add_patch(\n",
    "      patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
    "      )\n",
    "  ax1.add_patch(\n",
    "      patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
    "      )\n",
    "\n",
    "  tprs = []\n",
    "  aucs = []\n",
    "  mean_fpr = np.linspace(0,1,100)\n",
    "  i = 1\n",
    "  for train,test in cv.split(x,y):\n",
    "      model = create_Modelbaseline()\n",
    "      model.fit(x[train],y.iloc[train],\n",
    "            epochs=30,\n",
    "            shuffle=True,verbose=0)\n",
    "      prediction = model.predict(x[test])\n",
    "      fpr, tpr, t = roc_curve(y[test], prediction[:, 1])\n",
    "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "      roc_auc = auc(fpr, tpr)\n",
    "      aucs.append(roc_auc)\n",
    "      plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "      i= i+1\n",
    "\n",
    "  plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "  mean_tpr = np.mean(tprs, axis=0)\n",
    "  mean_auc = auc(mean_fpr, mean_tpr)\n",
    "  plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "          label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.title('ROC')\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "  plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI-_ZAvfIb5A"
   },
   "source": [
    "# Split Train/ Test / Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lAAQLx4UIptD"
   },
   "outputs": [],
   "source": [
    "#X= np.array(AMPs_df['reps'].to_list())\n",
    "#y= np.array(AMPs_df['class'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UWQ2IZWgIbST"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(AMPs_df['reps'].to_list()), np.array(AMPs_df['class'].to_list()), test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "#del X\n",
    "#del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "IM7Scdevkwpp",
    "outputId": "55fa7ea6-a5f9-4e23-bf42-f1a4bf64f8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  128565\n",
      "Val size:  42855\n",
      "Test size:  42856\n"
     ]
    }
   ],
   "source": [
    "# Given data size\n",
    "print('Train size: ', len(X_train))\n",
    "print('Val size: ', len(X_val))\n",
    "print('Test size: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dHnNnR7IAAs"
   },
   "source": [
    "# Model 4: Deep-AmPEP30 + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d dimension\n",
    "# Batchs, n_timesteps, n_features\n",
    "\n",
    "# Images 3d dimension\n",
    "# width , heigth , channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53214, 1900, 1)\n",
      "(17739, 1900, 1)\n",
      "(17739, 1900, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# https://stackoverflow.com/questions/52803989/how-to-correct-shape-of-keras-input-into-a-3d-array/52804200\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
    "print(X_train.shape)\n",
    "X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\n",
    "print(X_test.shape)\n",
    "X_val = np.reshape(X_val,(X_val.shape[0],X_val.shape[1],1))\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original method \n",
    "def create_Modelbaseline():\n",
    "    x_input = Input(shape=(1900,1)) # n_timesteps, n_features\n",
    "    # Conv\n",
    "    conv = Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu')(x_input) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu')(conv) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    # Flatten NN\n",
    "    flat = Flatten()(conv)\n",
    "    \n",
    "    layer_3 = Dense(128)(flat)\n",
    "    dropout_3 = Dropout(0.2)(layer_3)\n",
    "    layer_4 = Dense(10)(dropout_3)\n",
    "    dropout_4 = Dropout(0.2)(layer_4)\n",
    "    x_output = Dense(1, activation='sigmoid', name='output_layer', kernel_regularizer=l2(0.0001))(dropout_4)\n",
    "\n",
    "    model = Model(inputs=x_input, outputs=x_output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_Modelbaseline()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_value(model.optimizer.learning_rate, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      " Learning rate:  0.01\n",
      "Epoch 1/100\n",
      "  2/832 [..............................] - ETA: 1:35 - loss: 1005.9172 - accuracy: 0.5312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0714s vs `on_train_batch_end` time: 0.1584s). Check your callbacks.\n",
      "832/832 [==============================] - ETA: 0s - loss: 3.6077 - accuracy: 0.6687\n",
      "Epoch 00001: loss improved from inf to 3.60769, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 225s 270ms/step - loss: 3.6077 - accuracy: 0.6687 - val_loss: 0.7354 - val_accuracy: 0.5756\n",
      " Learning rate:  0.01\n",
      "Epoch 2/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.4792 - accuracy: 0.7826\n",
      "Epoch 00002: loss improved from 3.60769 to 0.47920, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 267ms/step - loss: 0.4792 - accuracy: 0.7826 - val_loss: 0.4271 - val_accuracy: 0.8109\n",
      " Learning rate:  0.01\n",
      "Epoch 3/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.4290 - accuracy: 0.8138\n",
      "Epoch 00003: loss improved from 0.47920 to 0.42904, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.4290 - accuracy: 0.8138 - val_loss: 0.4231 - val_accuracy: 0.8265\n",
      " Learning rate:  0.01\n",
      "Epoch 4/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3968 - accuracy: 0.8314\n",
      "Epoch 00004: loss improved from 0.42904 to 0.39679, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 267ms/step - loss: 0.3968 - accuracy: 0.8314 - val_loss: 0.4213 - val_accuracy: 0.8233\n",
      " Learning rate:  0.01\n",
      "Epoch 5/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3747 - accuracy: 0.8414\n",
      "Epoch 00005: loss improved from 0.39679 to 0.37471, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.3747 - accuracy: 0.8414 - val_loss: 0.3564 - val_accuracy: 0.8529\n",
      " Learning rate:  0.01\n",
      "Epoch 6/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3653 - accuracy: 0.8476\n",
      "Epoch 00006: loss improved from 0.37471 to 0.36531, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.3653 - accuracy: 0.8476 - val_loss: 0.3555 - val_accuracy: 0.8554\n",
      " Learning rate:  0.01\n",
      "Epoch 7/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3545 - accuracy: 0.8526\n",
      "Epoch 00007: loss improved from 0.36531 to 0.35447, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 263ms/step - loss: 0.3545 - accuracy: 0.8526 - val_loss: 0.4054 - val_accuracy: 0.8318\n",
      " Learning rate:  0.01\n",
      "Epoch 8/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3490 - accuracy: 0.8532\n",
      "Epoch 00008: loss improved from 0.35447 to 0.34902, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.3490 - accuracy: 0.8532 - val_loss: 0.3620 - val_accuracy: 0.8473\n",
      " Learning rate:  0.01\n",
      "Epoch 9/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8566\n",
      "Epoch 00009: loss improved from 0.34902 to 0.34539, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 265ms/step - loss: 0.3454 - accuracy: 0.8566 - val_loss: 0.3567 - val_accuracy: 0.8469\n",
      " Learning rate:  0.01\n",
      "Epoch 10/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.8571\n",
      "Epoch 00010: loss improved from 0.34539 to 0.34447, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.3445 - accuracy: 0.8571 - val_loss: 0.3408 - val_accuracy: 0.8531\n",
      " Learning rate:  0.01\n",
      "Epoch 11/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8577\n",
      "Epoch 00011: loss improved from 0.34447 to 0.34197, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 224s 269ms/step - loss: 0.3420 - accuracy: 0.8577 - val_loss: 0.3372 - val_accuracy: 0.8577\n",
      " Learning rate:  0.01\n",
      "Epoch 12/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3394 - accuracy: 0.8598\n",
      "Epoch 00012: loss improved from 0.34197 to 0.33937, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.3394 - accuracy: 0.8598 - val_loss: 0.3412 - val_accuracy: 0.8550\n",
      " Learning rate:  0.01\n",
      "Epoch 13/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.8410\n",
      "Epoch 00013: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3749 - accuracy: 0.8410 - val_loss: 0.3348 - val_accuracy: 0.8603\n",
      " Learning rate:  0.01\n",
      "Epoch 14/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.8604\n",
      "Epoch 00014: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3437 - accuracy: 0.8604 - val_loss: 0.3791 - val_accuracy: 0.8596\n",
      " Learning rate:  0.01\n",
      "Epoch 15/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3463 - accuracy: 0.8559\n",
      "Epoch 00015: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3463 - accuracy: 0.8559 - val_loss: 0.3580 - val_accuracy: 0.8560\n",
      " Learning rate:  0.01\n",
      "Epoch 16/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3460 - accuracy: 0.8581\n",
      "Epoch 00016: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3460 - accuracy: 0.8581 - val_loss: 0.3562 - val_accuracy: 0.8490\n",
      " Learning rate:  0.01\n",
      "Epoch 17/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.8570\n",
      "Epoch 00017: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3523 - accuracy: 0.8570 - val_loss: 0.3415 - val_accuracy: 0.8527\n",
      " Learning rate:  0.01\n",
      "Epoch 18/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3503 - accuracy: 0.8549\n",
      "Epoch 00018: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3503 - accuracy: 0.8549 - val_loss: 0.3723 - val_accuracy: 0.8369\n",
      " Learning rate:  0.01\n",
      "Epoch 19/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.8547\n",
      "Epoch 00019: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3564 - accuracy: 0.8547 - val_loss: 0.3603 - val_accuracy: 0.8598\n",
      " Learning rate:  0.01\n",
      "Epoch 20/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3609 - accuracy: 0.8521\n",
      "Epoch 00020: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3609 - accuracy: 0.8521 - val_loss: 0.3592 - val_accuracy: 0.8476\n",
      " Learning rate:  0.01\n",
      "Epoch 21/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3603 - accuracy: 0.8507\n",
      "Epoch 00021: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3603 - accuracy: 0.8507 - val_loss: 0.3464 - val_accuracy: 0.8507\n",
      " Learning rate:  0.001\n",
      "Epoch 22/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.8706\n",
      "Epoch 00022: loss improved from 0.33937 to 0.31001, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 224s 269ms/step - loss: 0.3100 - accuracy: 0.8706 - val_loss: 0.3242 - val_accuracy: 0.8657\n",
      " Learning rate:  0.001\n",
      "Epoch 23/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.8730\n",
      "Epoch 00023: loss improved from 0.31001 to 0.30292, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.3029 - accuracy: 0.8730 - val_loss: 0.3265 - val_accuracy: 0.8655\n",
      " Learning rate:  0.001\n",
      "Epoch 24/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3004 - accuracy: 0.8744\n",
      "Epoch 00024: loss improved from 0.30292 to 0.30037, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 223s 268ms/step - loss: 0.3004 - accuracy: 0.8744 - val_loss: 0.3209 - val_accuracy: 0.8656\n",
      " Learning rate:  0.001\n",
      "Epoch 25/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.8747\n",
      "Epoch 00025: loss improved from 0.30037 to 0.29902, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.2990 - accuracy: 0.8747 - val_loss: 0.3222 - val_accuracy: 0.8665\n",
      " Learning rate:  0.001\n",
      "Epoch 26/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.8768\n",
      "Epoch 00026: loss improved from 0.29902 to 0.29633, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 265ms/step - loss: 0.2963 - accuracy: 0.8768 - val_loss: 0.3168 - val_accuracy: 0.8685\n",
      " Learning rate:  0.001\n",
      "Epoch 27/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.8767\n",
      "Epoch 00027: loss improved from 0.29633 to 0.29571, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 263ms/step - loss: 0.2957 - accuracy: 0.8767 - val_loss: 0.3149 - val_accuracy: 0.8664\n",
      " Learning rate:  0.001\n",
      "Epoch 28/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2937 - accuracy: 0.8768\n",
      "Epoch 00028: loss improved from 0.29571 to 0.29370, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 265ms/step - loss: 0.2937 - accuracy: 0.8768 - val_loss: 0.3119 - val_accuracy: 0.8702\n",
      " Learning rate:  0.001\n",
      "Epoch 29/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2928 - accuracy: 0.8782\n",
      "Epoch 00029: loss improved from 0.29370 to 0.29282, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 263ms/step - loss: 0.2928 - accuracy: 0.8782 - val_loss: 0.3199 - val_accuracy: 0.8645\n",
      " Learning rate:  0.001\n",
      "Epoch 30/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.8786\n",
      "Epoch 00030: loss improved from 0.29282 to 0.29246, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 264ms/step - loss: 0.2925 - accuracy: 0.8786 - val_loss: 0.3162 - val_accuracy: 0.8701\n",
      " Learning rate:  0.001\n",
      "Epoch 31/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8790\n",
      "Epoch 00031: loss improved from 0.29246 to 0.29131, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 265ms/step - loss: 0.2913 - accuracy: 0.8790 - val_loss: 0.3120 - val_accuracy: 0.8695\n",
      " Learning rate:  0.001\n",
      "Epoch 32/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.8801\n",
      "Epoch 00032: loss improved from 0.29131 to 0.29082, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.2908 - accuracy: 0.8801 - val_loss: 0.3170 - val_accuracy: 0.8684\n",
      " Learning rate:  0.001\n",
      "Epoch 33/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.8787\n",
      "Epoch 00033: loss improved from 0.29082 to 0.29058, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 267ms/step - loss: 0.2906 - accuracy: 0.8787 - val_loss: 0.3130 - val_accuracy: 0.8710\n",
      " Learning rate:  0.001\n",
      "Epoch 34/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.8794\n",
      "Epoch 00034: loss improved from 0.29058 to 0.28897, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 263ms/step - loss: 0.2890 - accuracy: 0.8794 - val_loss: 0.3128 - val_accuracy: 0.8692\n",
      " Learning rate:  0.001\n",
      "Epoch 35/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.8801\n",
      "Epoch 00035: loss improved from 0.28897 to 0.28866, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 266ms/step - loss: 0.2887 - accuracy: 0.8801 - val_loss: 0.3187 - val_accuracy: 0.8666\n",
      " Learning rate:  0.001\n",
      "Epoch 36/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.8808\n",
      "Epoch 00036: loss improved from 0.28866 to 0.28828, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 265ms/step - loss: 0.2883 - accuracy: 0.8808 - val_loss: 0.3126 - val_accuracy: 0.8689\n",
      " Learning rate:  0.001\n",
      "Epoch 37/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.8813\n",
      "Epoch 00037: loss improved from 0.28828 to 0.28631, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.2863 - accuracy: 0.8813 - val_loss: 0.3093 - val_accuracy: 0.8732\n",
      " Learning rate:  0.001\n",
      "Epoch 38/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.8808\n",
      "Epoch 00038: loss did not improve from 0.28631\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2865 - accuracy: 0.8808 - val_loss: 0.3167 - val_accuracy: 0.8692\n",
      " Learning rate:  0.001\n",
      "Epoch 39/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.8813\n",
      "Epoch 00039: loss did not improve from 0.28631\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2864 - accuracy: 0.8813 - val_loss: 0.3122 - val_accuracy: 0.8714\n",
      " Learning rate:  0.001\n",
      "Epoch 40/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8813\n",
      "Epoch 00040: loss improved from 0.28631 to 0.28562, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 266ms/step - loss: 0.2856 - accuracy: 0.8813 - val_loss: 0.3213 - val_accuracy: 0.8701\n",
      " Learning rate:  0.001\n",
      "Epoch 41/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.8817\n",
      "Epoch 00041: loss improved from 0.28562 to 0.28477, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 264ms/step - loss: 0.2848 - accuracy: 0.8817 - val_loss: 0.3114 - val_accuracy: 0.8687\n",
      " Learning rate:  0.001\n",
      "Epoch 42/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2851 - accuracy: 0.8819\n",
      "Epoch 00042: loss did not improve from 0.28477\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2851 - accuracy: 0.8819 - val_loss: 0.3123 - val_accuracy: 0.8679\n",
      " Learning rate:  0.001\n",
      "Epoch 43/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.8829\n",
      "Epoch 00043: loss improved from 0.28477 to 0.28339, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.2834 - accuracy: 0.8829 - val_loss: 0.3209 - val_accuracy: 0.8719\n",
      " Learning rate:  0.001\n",
      "Epoch 44/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.8812\n",
      "Epoch 00044: loss did not improve from 0.28339\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2843 - accuracy: 0.8812 - val_loss: 0.3082 - val_accuracy: 0.8714\n",
      " Learning rate:  0.001\n",
      "Epoch 45/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2827 - accuracy: 0.8820\n",
      "Epoch 00045: loss improved from 0.28339 to 0.28268, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 266ms/step - loss: 0.2827 - accuracy: 0.8820 - val_loss: 0.3120 - val_accuracy: 0.8707\n",
      " Learning rate:  0.001\n",
      "Epoch 46/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2818 - accuracy: 0.8830\n",
      "Epoch 00046: loss improved from 0.28268 to 0.28184, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 223s 268ms/step - loss: 0.2818 - accuracy: 0.8830 - val_loss: 0.3202 - val_accuracy: 0.8699\n",
      " Learning rate:  0.001\n",
      "Epoch 47/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.8833\n",
      "Epoch 00047: loss improved from 0.28184 to 0.28160, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.2816 - accuracy: 0.8833 - val_loss: 0.3067 - val_accuracy: 0.8735\n",
      " Learning rate:  0.001\n",
      "Epoch 48/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2824 - accuracy: 0.8833\n",
      "Epoch 00048: loss did not improve from 0.28160\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2824 - accuracy: 0.8833 - val_loss: 0.3203 - val_accuracy: 0.8715\n",
      " Learning rate:  0.001\n",
      "Epoch 49/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2827 - accuracy: 0.8843\n",
      "Epoch 00049: loss did not improve from 0.28160\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2827 - accuracy: 0.8843 - val_loss: 0.3164 - val_accuracy: 0.8710\n",
      " Learning rate:  0.001\n",
      "Epoch 50/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2821 - accuracy: 0.8823\n",
      "Epoch 00050: loss did not improve from 0.28160\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2821 - accuracy: 0.8823 - val_loss: 0.3103 - val_accuracy: 0.8718\n",
      " Learning rate:  0.001\n",
      "Epoch 51/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2806 - accuracy: 0.8839\n",
      "Epoch 00051: loss improved from 0.28160 to 0.28060, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 223s 268ms/step - loss: 0.2806 - accuracy: 0.8839 - val_loss: 0.3094 - val_accuracy: 0.8720\n",
      " Learning rate:  3e-05\n",
      "Epoch 52/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.8866\n",
      "Epoch 00052: loss improved from 0.28060 to 0.27374, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.2737 - accuracy: 0.8866 - val_loss: 0.3078 - val_accuracy: 0.8731\n",
      " Learning rate:  3e-05\n",
      "Epoch 53/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.8879\n",
      "Epoch 00053: loss improved from 0.27374 to 0.27212, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.2721 - accuracy: 0.8879 - val_loss: 0.3075 - val_accuracy: 0.8726\n",
      " Learning rate:  3e-05\n",
      "Epoch 54/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.8879\n",
      "Epoch 00054: loss improved from 0.27212 to 0.27145, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 264ms/step - loss: 0.2714 - accuracy: 0.8879 - val_loss: 0.3068 - val_accuracy: 0.8729\n",
      " Learning rate:  3e-05\n",
      "Epoch 55/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.8878\n",
      "Epoch 00055: loss did not improve from 0.27145\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2715 - accuracy: 0.8878 - val_loss: 0.3069 - val_accuracy: 0.8723\n",
      " Learning rate:  3e-05\n",
      "Epoch 56/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.8876\n",
      "Epoch 00056: loss improved from 0.27145 to 0.27118, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 267ms/step - loss: 0.2712 - accuracy: 0.8876 - val_loss: 0.3077 - val_accuracy: 0.8727\n",
      " Learning rate:  3e-05\n",
      "Epoch 57/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.8877\n",
      "Epoch 00057: loss improved from 0.27118 to 0.27085, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.2709 - accuracy: 0.8877 - val_loss: 0.3072 - val_accuracy: 0.8725\n",
      " Learning rate:  3e-05\n",
      "Epoch 58/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.8878\n",
      "Epoch 00058: loss improved from 0.27085 to 0.27029, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 223s 268ms/step - loss: 0.2703 - accuracy: 0.8878 - val_loss: 0.3075 - val_accuracy: 0.8727\n",
      " Learning rate:  3e-05\n",
      "Epoch 59/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2707 - accuracy: 0.8883\n",
      "Epoch 00059: loss did not improve from 0.27029\n",
      "832/832 [==============================] - 207s 249ms/step - loss: 0.2707 - accuracy: 0.8883 - val_loss: 0.3077 - val_accuracy: 0.8721\n",
      " Learning rate:  3e-05\n",
      "Epoch 60/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.8883\n",
      "Epoch 00060: loss did not improve from 0.27029\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2712 - accuracy: 0.8883 - val_loss: 0.3078 - val_accuracy: 0.8727\n",
      " Learning rate:  3e-05\n",
      "Epoch 61/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2708 - accuracy: 0.8884\n",
      "Epoch 00061: loss did not improve from 0.27029\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2708 - accuracy: 0.8884 - val_loss: 0.3081 - val_accuracy: 0.8723\n",
      " Learning rate:  3e-05\n",
      "Epoch 62/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.8887\n",
      "Epoch 00062: loss did not improve from 0.27029\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2705 - accuracy: 0.8887 - val_loss: 0.3079 - val_accuracy: 0.8725\n",
      " Learning rate:  3e-05\n",
      "Epoch 63/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2708 - accuracy: 0.8886\n",
      "Epoch 00063: loss did not improve from 0.27029\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2708 - accuracy: 0.8886 - val_loss: 0.3076 - val_accuracy: 0.8727\n",
      " Learning rate:  3e-05\n",
      "Epoch 64/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2701 - accuracy: 0.8886\n",
      "Epoch 00064: loss improved from 0.27029 to 0.27012, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 267ms/step - loss: 0.2701 - accuracy: 0.8886 - val_loss: 0.3080 - val_accuracy: 0.8725\n",
      " Learning rate:  3e-05\n",
      "Epoch 65/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.8878\n",
      "Epoch 00065: loss did not improve from 0.27012\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2705 - accuracy: 0.8878 - val_loss: 0.3083 - val_accuracy: 0.8720\n",
      " Learning rate:  3e-05\n",
      "Epoch 66/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.8886\n",
      "Epoch 00066: loss did not improve from 0.27012\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2703 - accuracy: 0.8886 - val_loss: 0.3060 - val_accuracy: 0.8727\n",
      " Learning rate:  3e-05\n",
      "Epoch 67/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.8885\n",
      "Epoch 00067: loss did not improve from 0.27012\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2704 - accuracy: 0.8885 - val_loss: 0.3085 - val_accuracy: 0.8723\n",
      " Learning rate:  3e-05\n",
      "Epoch 68/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.8879\n",
      "Epoch 00068: loss did not improve from 0.27012\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2706 - accuracy: 0.8879 - val_loss: 0.3084 - val_accuracy: 0.8724\n",
      " Learning rate:  3e-05\n",
      "Epoch 69/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.8883\n",
      "Epoch 00069: loss improved from 0.27012 to 0.26948, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 224s 269ms/step - loss: 0.2695 - accuracy: 0.8883 - val_loss: 0.3090 - val_accuracy: 0.8721\n",
      " Learning rate:  3e-05\n",
      "Epoch 70/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.8878\n",
      "Epoch 00070: loss did not improve from 0.26948\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2704 - accuracy: 0.8878 - val_loss: 0.3084 - val_accuracy: 0.8721\n",
      " Learning rate:  3e-05\n",
      "Epoch 71/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.8891\n",
      "Epoch 00071: loss did not improve from 0.26948\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2710 - accuracy: 0.8891 - val_loss: 0.3076 - val_accuracy: 0.8720\n",
      " Learning rate:  5e-06\n",
      "Epoch 72/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2693 - accuracy: 0.8889\n",
      "Epoch 00072: loss improved from 0.26948 to 0.26927, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 223s 268ms/step - loss: 0.2693 - accuracy: 0.8889 - val_loss: 0.3078 - val_accuracy: 0.8721\n",
      " Learning rate:  5e-06\n",
      "Epoch 73/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.8891\n",
      "Epoch 00073: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2696 - accuracy: 0.8891 - val_loss: 0.3078 - val_accuracy: 0.8721\n",
      " Learning rate:  5e-06\n",
      "Epoch 74/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.8890\n",
      "Epoch 00074: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2695 - accuracy: 0.8890 - val_loss: 0.3078 - val_accuracy: 0.8722\n",
      " Learning rate:  5e-06\n",
      "Epoch 75/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.8882\n",
      "Epoch 00075: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2705 - accuracy: 0.8882 - val_loss: 0.3078 - val_accuracy: 0.8720\n",
      " Learning rate:  5e-06\n",
      "Epoch 76/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.8887\n",
      "Epoch 00076: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2697 - accuracy: 0.8887 - val_loss: 0.3080 - val_accuracy: 0.8720\n",
      " Learning rate:  5e-06\n",
      "Epoch 77/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2698 - accuracy: 0.8882\n",
      "Epoch 00077: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2698 - accuracy: 0.8882 - val_loss: 0.3079 - val_accuracy: 0.8721\n",
      " Learning rate:  5e-06\n",
      "Epoch 78/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.8878\n",
      "Epoch 00078: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2697 - accuracy: 0.8878 - val_loss: 0.3079 - val_accuracy: 0.8724\n",
      " Learning rate:  5e-06\n",
      "Epoch 79/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.8894\n",
      "Epoch 00079: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2700 - accuracy: 0.8894 - val_loss: 0.3079 - val_accuracy: 0.8722\n",
      " Learning rate:  5e-06\n",
      "Epoch 80/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.8886\n",
      "Epoch 00080: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2704 - accuracy: 0.8886 - val_loss: 0.3077 - val_accuracy: 0.8723\n",
      " Learning rate:  5e-06\n",
      "Epoch 81/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.8881\n",
      "Epoch 00081: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2697 - accuracy: 0.8881 - val_loss: 0.3079 - val_accuracy: 0.8723\n",
      " Learning rate:  5e-06\n",
      "Epoch 82/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.8881\n",
      "Epoch 00082: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2702 - accuracy: 0.8881 - val_loss: 0.3079 - val_accuracy: 0.8721\n",
      "Epoch 00082: early stopping\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"/mnt/vdb/thesis/AmPPEP30.1900.hdf5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "# Early Stopping\n",
    "es = EarlyStopping(monitor='loss', patience=8, verbose=1)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    batch_size=64, validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint, es,lr_scheduler], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1663/1663 [==============================] - 63s 38ms/step - loss: 0.2671 - accuracy: 0.8893\n",
      "Train loss:  0.26714572310447693\n",
      "Train accuracy:  0.8892960548400879\n",
      "----------------------------------------------------------------------\n",
      "555/555 [==============================] - 21s 38ms/step - loss: 0.3079 - accuracy: 0.8721\n",
      "Val loss:  0.307868629693985\n",
      "Val accuracy:  0.8721461296081543\n",
      "----------------------------------------------------------------------\n",
      "555/555 [==============================] - 21s 38ms/step - loss: 0.3020 - accuracy: 0.8752\n",
      "Test loss:  0.3020343482494354\n",
      "Test accuracy:  0.8752466440200806\n"
     ]
    }
   ],
   "source": [
    "display_model_score(model,\n",
    "    [X_train, y_train],\n",
    "    [X_val, y_val],\n",
    "    [X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88      8928\n",
      "           1       0.87      0.88      0.87      8811\n",
      "\n",
      "    accuracy                           0.88     17739\n",
      "   macro avg       0.88      0.88      0.88     17739\n",
      "weighted avg       0.88      0.88      0.88     17739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_probas = model.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_predict = np.where(y_probas > threshold, 1, 0)\n",
    "\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 239, 1900)]       0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 239, 1900, 1900)   45600     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 862790000)         0         \n",
      "=================================================================\n",
      "Total params: 45,600\n",
      "Trainable params: 45,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_input=Input(shape=(input_shape),dtype='float64')  \n",
    "\n",
    "# creating the embedding\n",
    "word_embedding=Embedding(input_dim=24,output_dim=1900,input_length=1900)(word_input)\n",
    "\n",
    "word_vec=Flatten()(word_embedding) # flatten\n",
    "embed_model =Model([word_input],word_vec) # combining all into a Keras model\n",
    "embed_model.compile(optimizer=keras.optimizers.Adam(lr=1e-3),loss='binary_crossentropy',metrics=['acc']) \n",
    "# compiling the model. parameters can be tuned as always.\n",
    "print(embed_model.summary()) # summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 239, 1900) for input Tensor(\"input_4:0\", shape=(None, 239, 1900), dtype=float64), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    }
   ],
   "source": [
    "embeddings=embed_model.predict(X_train[0]) # finally getting the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01733692, -0.0335286 ,  0.01197986, ...,  0.05971117,\n",
       "        0.06566247, -0.02294011])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02140281  0.0160512  -0.01652263 ... -0.01446537 -0.03235978\n",
      "   0.00240989]\n",
      " [ 0.02140281  0.0160512  -0.01652263 ... -0.01446537 -0.03235978\n",
      "   0.00240989]\n",
      " [ 0.02140281  0.0160512  -0.01652263 ... -0.01446537 -0.03235978\n",
      "   0.00240989]\n",
      " ...\n",
      " [ 0.02140281  0.0160512  -0.01652263 ... -0.01446537 -0.03235978\n",
      "   0.00240989]\n",
      " [ 0.02140281  0.0160512  -0.01652263 ... -0.01446537 -0.03235978\n",
      "   0.00240989]\n",
      " [ 0.02140281  0.0160512  -0.01652263 ... -0.01446537 -0.03235978\n",
      "   0.00240989]]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1900"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900, 1900)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 1, 1900)\n",
      "(48, 1, 1900)\n",
      "(48, 1, 1900)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# https://stackoverflow.com/questions/52803989/how-to-correct-shape-of-keras-input-into-a-3d-array/52804200\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],1,X_train.shape[1]))\n",
    "print(X_train.shape)\n",
    "X_test = np.reshape(X_test,(X_test.shape[0],1,X_test.shape[1]))\n",
    "print(X_test.shape)\n",
    "X_val = np.reshape(X_val,(X_val.shape[0],1,X_val.shape[1]))\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04603789, -0.07072274, -0.03860193, ..., -0.01830798,\n",
       "         0.05091501,  0.05827223]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1yvj_MHIA6-"
   },
   "source": [
    "# Model 3: ProtCNN\n",
    "https://www.biorxiv.org/content/10.1101/626507v3.full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128565, 1900, 1)\n",
      "(42856, 1900, 1)\n",
      "(42855, 1900, 1)\n"
     ]
    }
   ],
   "source": [
    "# convert format \n",
    "import numpy as np\n",
    "# https://stackoverflow.com/questions/52803989/how-to-correct-shape-of-keras-input-into-a-3d-array/52804200\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
    "print(X_train.shape)\n",
    "X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\n",
    "print(X_test.shape)\n",
    "X_val = np.reshape(X_val,(X_val.shape[0],X_val.shape[1],1))\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uIIiKYP8IRYE"
   },
   "outputs": [],
   "source": [
    "def residual_block(data, filters, d_rate):\n",
    "  \"\"\"\n",
    "  residual_block consist of two resnet layers\n",
    "  For dilation_rate  # https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\n",
    "  \n",
    "  _data: input\n",
    "  _filters: convolution filters\n",
    "  _d_rate: dilation rate\n",
    "  \"\"\"\n",
    "\n",
    "  shortcut = data\n",
    "\n",
    "  bn1 = BatchNormalization()(data)\n",
    "  act1 = Activation('relu')(bn1)\n",
    "  conv1 = Conv1D(filters, 1, dilation_rate=d_rate, padding='same', kernel_regularizer=l2(0.001))(act1)\n",
    "\n",
    "  #bottleneck convolution\n",
    "  bn2 = BatchNormalization()(conv1)\n",
    "  act2 = Activation('relu')(bn2)\n",
    "  conv2 = Conv1D(filters, 3, padding='same', kernel_regularizer=l2(0.001))(act2)\n",
    "\n",
    "  #skip connection\n",
    "  x = Add()([conv2, shortcut])\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 1900, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1900, 128)    256         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 1900, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 1900, 128)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 1900, 128)    16512       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 1900, 128)    512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 1900, 128)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 1900, 128)    49280       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 1900, 128)    0           conv1d_40[0][0]                  \n",
      "                                                                 conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 1900, 128)    512         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 1900, 128)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 1900, 128)    16512       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 1900, 128)    512         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 1900, 128)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1900, 128)    49280       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 1900, 128)    0           conv1d_42[0][0]                  \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 633, 128)     0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 633, 128)     0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 81024)        0           dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            81025       flatten_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 214,913\n",
      "Trainable params: 213,889\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_input = Input(shape=(1900,1))\n",
    "\n",
    "#initial conv\n",
    "conv = Conv1D(128, 1, padding='same')(x_input) \n",
    "\n",
    "# per-residue representation\n",
    "res1 = residual_block(conv, 128, 2)\n",
    "res2 = residual_block(res1, 128, 3)\n",
    "\n",
    "x = MaxPooling1D(3)(res2)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# softmax classifier\n",
    "x = Flatten()(x)\n",
    "x_output = Dense(1000, activation='sigmoid', kernel_regularizer=l2(0.0001))(x)\n",
    "#x_output = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "model_ProtCNN = Model(inputs=x_input, outputs=x_output)\n",
    "model_ProtCNN.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_ProtCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good model for detect nonAMP\n",
    "\n",
    "x_input = Input(shape=(1900,1))\n",
    "\n",
    "#initial conv\n",
    "conv = Conv1D(256, 1, padding='same')(x_input) \n",
    "\n",
    "# per-residue representation\n",
    "res1 = residual_block(conv, 256, 2)\n",
    "res2 = residual_block(res1, 256, 3)\n",
    "\n",
    "x = MaxPooling1D(3)(res2)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# softmax classifier\n",
    "x = Flatten()(x)\n",
    "layer_4 = Dense(1211 , activation='relu')(x)\n",
    "layer_4 = Dropout(0.5)(layer_4)\n",
    "layer_4 = Dense(1211 , activation='relu')(layer_4)\n",
    "layer_4 = Dropout(0.5)(layer_4)\n",
    "layer_4 = Dense(1211 , activation='relu')(layer_4)\n",
    "layer_4 = Dropout(0.5)(layer_4)\n",
    "layer_4 = Dense(1211 , activation='relu')(layer_4)\n",
    "x_output = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0001))(layer_4)\n",
    "\n",
    "model_ProtCNN = Model(inputs=x_input, outputs=x_output)\n",
    "model_ProtCNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_ProtCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iFyHyapJ4EKF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1900, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 1900, 256)    512         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1900, 256)    1024        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1900, 256)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1900, 256)    65792       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1900, 256)    1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1900, 256)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1900, 256)    196864      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1900, 256)    0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1900, 256)    1024        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1900, 256)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1900, 256)    65792       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1900, 256)    1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1900, 256)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1900, 256)    196864      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1900, 256)    0           conv1d_4[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 633, 256)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 633, 256)     1024        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 162048)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1211)         196241339   flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1211)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          620544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            513         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 197,393,340\n",
      "Trainable params: 197,390,780\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "x_input = Input(shape=(1900,1))\n",
    "\n",
    "#initial conv\n",
    "conv = Conv1D(256, 1, padding='same')(x_input) \n",
    "\n",
    "# per-residue representation\n",
    "res1 = residual_block(conv, 256, 2)\n",
    "res2 = residual_block(res1, 256, 3)\n",
    "\n",
    "x = MaxPooling1D(3)(res2)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# softmax classifier\n",
    "x = Flatten()(x)\n",
    "layer_4 = Dense(1211 , activation='relu')(x)\n",
    "layer_4 = Dropout(0.5)(layer_4)\n",
    "layer_4 = Dense(512 , activation='relu')(layer_4)\n",
    "layer_4 = Dropout(0.5)(layer_4)\n",
    "x_output = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0001))(layer_4)\n",
    "\n",
    "model_ProtCNN = Model(inputs=x_input, outputs=x_output)\n",
    "model_ProtCNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_ProtCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import backend as K\n",
    "#K.set_value(model_ProtCNN.optimizer.learning_rate, 0.00001)\n",
    "def lr_schedule(epoch):\n",
    "    \n",
    "    lr = 1e-3\n",
    "    if epoch > 80:\n",
    "        lr = 0.1e-6\n",
    "    elif epoch > 50:    \n",
    "        lr = 0.3e-5\n",
    "    elif epoch > 20:\n",
    "        lr = 1e-4\n",
    "        \n",
    "    print(' Learning rate: ', lr)    \n",
    "    return lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      " Learning rate:  0.001\n",
      "Epoch 1/100\n",
      "   2/1005 [..............................] - ETA: 1:26 - loss: 48.1993 - accuracy: 0.5312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0603s vs `on_train_batch_end` time: 0.1105s). Check your callbacks.\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 2.3199 - accuracy: 0.6805\n",
      "Epoch 00001: loss improved from inf to 2.31993, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 198s 197ms/step - loss: 2.3199 - accuracy: 0.6805 - val_loss: 0.9736 - val_accuracy: 0.8333\n",
      " Learning rate:  0.001\n",
      "Epoch 2/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.8859 - accuracy: 0.7680\n",
      "Epoch 00002: loss improved from 2.31993 to 0.88589, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 198s 197ms/step - loss: 0.8859 - accuracy: 0.7680 - val_loss: 0.5668 - val_accuracy: 0.8797\n",
      " Learning rate:  0.001\n",
      "Epoch 3/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.6033 - accuracy: 0.7859\n",
      "Epoch 00003: loss improved from 0.88589 to 0.60332, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 195s 194ms/step - loss: 0.6033 - accuracy: 0.7859 - val_loss: 0.4529 - val_accuracy: 0.8837\n",
      " Learning rate:  0.001\n",
      "Epoch 4/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.4600 - accuracy: 0.7975\n",
      "Epoch 00004: loss improved from 0.60332 to 0.45996, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 195s 194ms/step - loss: 0.4600 - accuracy: 0.7975 - val_loss: 0.3444 - val_accuracy: 0.8986\n",
      " Learning rate:  0.001\n",
      "Epoch 5/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.4000 - accuracy: 0.8181\n",
      "Epoch 00005: loss improved from 0.45996 to 0.40000, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.4000 - accuracy: 0.8181 - val_loss: 0.3282 - val_accuracy: 0.8880\n",
      " Learning rate:  0.001\n",
      "Epoch 6/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.3790 - accuracy: 0.8347\n",
      "Epoch 00006: loss improved from 0.40000 to 0.37897, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 195s 194ms/step - loss: 0.3790 - accuracy: 0.8347 - val_loss: 0.3623 - val_accuracy: 0.8883\n",
      " Learning rate:  0.001\n",
      "Epoch 7/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.3692 - accuracy: 0.8397\n",
      "Epoch 00007: loss improved from 0.37897 to 0.36923, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 195s 194ms/step - loss: 0.3692 - accuracy: 0.8397 - val_loss: 0.2949 - val_accuracy: 0.8983\n",
      " Learning rate:  0.001\n",
      "Epoch 8/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.3857 - accuracy: 0.8417\n",
      "Epoch 00008: loss did not improve from 0.36923\n",
      "1005/1005 [==============================] - 188s 187ms/step - loss: 0.3857 - accuracy: 0.8417 - val_loss: 0.3187 - val_accuracy: 0.8998\n",
      " Learning rate:  0.001\n",
      "Epoch 9/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.8484\n",
      "Epoch 00009: loss improved from 0.36923 to 0.35635, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 196s 195ms/step - loss: 0.3564 - accuracy: 0.8484 - val_loss: 0.2944 - val_accuracy: 0.9007\n",
      " Learning rate:  0.001\n",
      "Epoch 10/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.3492 - accuracy: 0.8514\n",
      "Epoch 00010: loss improved from 0.35635 to 0.34918, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 197s 196ms/step - loss: 0.3492 - accuracy: 0.8514 - val_loss: 0.3149 - val_accuracy: 0.8967\n",
      " Learning rate:  0.001\n",
      "Epoch 11/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.3433 - accuracy: 0.8548\n",
      "Epoch 00011: loss improved from 0.34918 to 0.34327, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.3433 - accuracy: 0.8548 - val_loss: 0.3201 - val_accuracy: 0.8988\n",
      " Learning rate:  0.001\n",
      "Epoch 12/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.3375 - accuracy: 0.8580\n",
      "Epoch 00012: loss improved from 0.34327 to 0.33751, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.3375 - accuracy: 0.8580 - val_loss: 0.2762 - val_accuracy: 0.9119\n",
      " Learning rate:  0.001\n",
      "Epoch 13/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.3259 - accuracy: 0.8667\n",
      "Epoch 00013: loss improved from 0.33751 to 0.32587, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 195s 194ms/step - loss: 0.3259 - accuracy: 0.8667 - val_loss: 0.2608 - val_accuracy: 0.9101\n",
      " Learning rate:  0.001\n",
      "Epoch 14/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.3120 - accuracy: 0.8739\n",
      "Epoch 00014: loss improved from 0.32587 to 0.31204, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.3120 - accuracy: 0.8739 - val_loss: 0.2938 - val_accuracy: 0.8979\n",
      " Learning rate:  0.001\n",
      "Epoch 15/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.8791\n",
      "Epoch 00015: loss improved from 0.31204 to 0.30238, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.3024 - accuracy: 0.8791 - val_loss: 0.2633 - val_accuracy: 0.9127\n",
      " Learning rate:  0.001\n",
      "Epoch 16/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2937 - accuracy: 0.8823\n",
      "Epoch 00016: loss improved from 0.30238 to 0.29367, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2937 - accuracy: 0.8823 - val_loss: 0.2464 - val_accuracy: 0.9182\n",
      " Learning rate:  0.001\n",
      "Epoch 17/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.8860\n",
      "Epoch 00017: loss improved from 0.29367 to 0.28427, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2843 - accuracy: 0.8860 - val_loss: 0.2619 - val_accuracy: 0.9120\n",
      " Learning rate:  0.001\n",
      "Epoch 18/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.8885\n",
      "Epoch 00018: loss improved from 0.28427 to 0.28388, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2839 - accuracy: 0.8885 - val_loss: 0.2735 - val_accuracy: 0.9133\n",
      " Learning rate:  0.001\n",
      "Epoch 19/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.8893\n",
      "Epoch 00019: loss improved from 0.28388 to 0.27830, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2783 - accuracy: 0.8893 - val_loss: 0.2500 - val_accuracy: 0.9212\n",
      " Learning rate:  0.001\n",
      "Epoch 20/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.8930\n",
      "Epoch 00020: loss improved from 0.27830 to 0.26962, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2696 - accuracy: 0.8930 - val_loss: 0.2461 - val_accuracy: 0.9218\n",
      " Learning rate:  0.001\n",
      "Epoch 21/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2672 - accuracy: 0.8945\n",
      "Epoch 00021: loss improved from 0.26962 to 0.26723, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2672 - accuracy: 0.8945 - val_loss: 0.2194 - val_accuracy: 0.9209\n",
      " Learning rate:  0.0001\n",
      "Epoch 22/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.9037\n",
      "Epoch 00022: loss improved from 0.26723 to 0.24174, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2417 - accuracy: 0.9037 - val_loss: 0.2159 - val_accuracy: 0.9285\n",
      " Learning rate:  0.0001\n",
      "Epoch 23/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.9084\n",
      "Epoch 00023: loss improved from 0.24174 to 0.23259, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2326 - accuracy: 0.9084 - val_loss: 0.2145 - val_accuracy: 0.9287\n",
      " Learning rate:  0.0001\n",
      "Epoch 24/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.9103\n",
      "Epoch 00024: loss improved from 0.23259 to 0.22779, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2278 - accuracy: 0.9103 - val_loss: 0.2127 - val_accuracy: 0.9295\n",
      " Learning rate:  0.0001\n",
      "Epoch 25/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2251 - accuracy: 0.9107\n",
      "Epoch 00025: loss improved from 0.22779 to 0.22513, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2251 - accuracy: 0.9107 - val_loss: 0.2109 - val_accuracy: 0.9298\n",
      " Learning rate:  0.0001\n",
      "Epoch 26/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2217 - accuracy: 0.9121\n",
      "Epoch 00026: loss improved from 0.22513 to 0.22170, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2217 - accuracy: 0.9121 - val_loss: 0.1993 - val_accuracy: 0.9308\n",
      " Learning rate:  0.0001\n",
      "Epoch 27/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2208 - accuracy: 0.9115\n",
      "Epoch 00027: loss improved from 0.22170 to 0.22076, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2208 - accuracy: 0.9115 - val_loss: 0.2102 - val_accuracy: 0.9317\n",
      " Learning rate:  0.0001\n",
      "Epoch 28/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2177 - accuracy: 0.9138\n",
      "Epoch 00028: loss improved from 0.22076 to 0.21767, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2177 - accuracy: 0.9138 - val_loss: 0.2117 - val_accuracy: 0.9315\n",
      " Learning rate:  0.0001\n",
      "Epoch 29/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2132 - accuracy: 0.9157\n",
      "Epoch 00029: loss improved from 0.21767 to 0.21318, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 195s 194ms/step - loss: 0.2132 - accuracy: 0.9157 - val_loss: 0.2093 - val_accuracy: 0.9303\n",
      " Learning rate:  0.0001\n",
      "Epoch 30/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2097 - accuracy: 0.9179\n",
      "Epoch 00030: loss improved from 0.21318 to 0.20968, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2097 - accuracy: 0.9179 - val_loss: 0.2083 - val_accuracy: 0.9299\n",
      " Learning rate:  0.0001\n",
      "Epoch 31/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.2060 - accuracy: 0.9201\n",
      "Epoch 00031: loss improved from 0.20968 to 0.20603, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.2060 - accuracy: 0.9201 - val_loss: 0.2103 - val_accuracy: 0.9300\n",
      " Learning rate:  0.0001\n",
      "Epoch 32/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.1975 - accuracy: 0.9243\n",
      "Epoch 00032: loss improved from 0.20603 to 0.19752, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 193s 192ms/step - loss: 0.1975 - accuracy: 0.9243 - val_loss: 0.2123 - val_accuracy: 0.9299\n",
      " Learning rate:  0.0001\n",
      "Epoch 33/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.1922 - accuracy: 0.9275\n",
      "Epoch 00033: loss improved from 0.19752 to 0.19222, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.1922 - accuracy: 0.9275 - val_loss: 0.2020 - val_accuracy: 0.9325\n",
      " Learning rate:  0.0001\n",
      "Epoch 34/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.1917 - accuracy: 0.9280\n",
      "Epoch 00034: loss improved from 0.19222 to 0.19167, saving model to /mnt/vdb/thesis/ProtCNN.V4.h5\n",
      "1005/1005 [==============================] - 194s 193ms/step - loss: 0.1917 - accuracy: 0.9280 - val_loss: 0.2061 - val_accuracy: 0.9312\n",
      "Epoch 00034: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Early Stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=15, verbose=1)\n",
    "# val_loss\n",
    "# ProtCNN_256X2_NN1221X4.V2.h5\n",
    "checkpoint = ModelCheckpoint(\"/mnt/vdb/thesis/ProtCNN.V4.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "# val_accuracy\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "history2 = model_ProtCNN.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100, batch_size=128,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[checkpoint, es,lr_scheduler]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ProtCNN.save(\"/mnt/vdb/ProtCNN.bestmodel.h5\")\n",
    "model_ProtCNN= keras.models.load_model(\"/mnt/vdb/ProtCNN.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     21596\n",
      "           1       0.94      0.91      0.93     21260\n",
      "\n",
      "    accuracy                           0.93     42856\n",
      "   macro avg       0.93      0.93      0.93     42856\n",
      "weighted avg       0.93      0.93      0.93     42856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_probas = model_ProtCNN.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_predict = np.where(y_probas > threshold, 1, 0)\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kZO0R1iHSm4T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95     25564\n",
      "           1       0.94      0.95      0.95     25244\n",
      "\n",
      "    accuracy                           0.95     50808\n",
      "   macro avg       0.95      0.95      0.95     50808\n",
      "weighted avg       0.95      0.95      0.95     50808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_probas = model_ProtCNN.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_predict = np.where(y_probas > threshold, 1, 0)\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pMIGguFR4EHD"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_3iiV1sM4D6d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ProtCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6ffcb3a07e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m display_model_score(model_ProtCNN,\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     [X_test, y_test])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ProtCNN' is not defined"
     ]
    }
   ],
   "source": [
    "display_model_score(model_ProtCNN,\n",
    "    [X_train, y_train],\n",
    "    [X_val, y_val],\n",
    "    [X_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fALQEI9g4Ejn"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Gh0Zvl9j4E38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1900, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 1900, 256)    512         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1900, 256)    1024        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1900, 256)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1900, 256)    65792       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1900, 256)    1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1900, 256)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1900, 256)    196864      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1900, 256)    0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1900, 256)    1024        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1900, 256)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1900, 256)    65792       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1900, 256)    1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1900, 256)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1900, 256)    196864      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1900, 256)    0           conv1d_4[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 633, 256)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 633, 256)     0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 162048)       0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1000)         162049000   flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            1001        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 162,579,921\n",
      "Trainable params: 162,577,873\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "with tf.device('/cpu:0'):\n",
    "        ProtCNN= load_model(\"/mnt/vdb/ProtCNN.bestmodel.h5\")\n",
    "ProtCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CustomDNNModel.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
