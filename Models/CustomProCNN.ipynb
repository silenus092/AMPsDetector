{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48641189/fitting-3d-data-as-input-into-keras-sequential-model-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install keras=2.3.1 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check avialable CPU and GPU\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Use only CPU\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4aBzk8QXHS9S"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "\n",
    "#keras\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv1D, Add, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GlobalMaxPooling1D\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# CuDNNLSTM error; The error was because from TensorFlow 2 you do not need to specify CuDNNLSTM. \n",
    "# You can just use LSTM with no activation function and it will automatically use the CuDNN version. You do have to install CuDNN first.\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FNhtXHJrfEE4"
   },
   "outputs": [],
   "source": [
    "# small old ../datasets/AMPsNonAMPs_df.239.plk\n",
    "# /home/ubuntu/data/AMPsNonAMPs_df.plk old dataset\n",
    "# /mnt/vdb/thesis/jax/AMPNonAMP.final.reps new dataset\n",
    "with open( \"/mnt/vdb/thesis/jax/AMPNonAMP.V5_C08_sim60.reps\", 'rb') as file:\n",
    "    AMPs_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For PWS\n",
    "with open( '/mnt/vdb/thesis/pwm/AMPnonAMP.sim60_c08.pwm.pkl', 'rb') as file:\n",
    "    AMPs_df = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "mGiMSzzPfj2t",
    "outputId": "092eb611-b89c-4cac-c8c4-0557799510cf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>length</th>\n",
       "      <th>class</th>\n",
       "      <th>reps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>10015_dbaasp,10016_dbaasp|dbaasp_peptides</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.20464208722114563, -0.055944692343473434, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>10026_dbaasp|dbaasp_peptides</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.1005186140537262, 0.0014500601682811975, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>10029_dbaasp|dbaasp_peptides</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.14606480300426483, 0.04153195396065712, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>1003,1011,1019,1027,1035|CancerPPD_l_natural</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.02989775501191616, -0.004465686157345772, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>10030_dbaasp|dbaasp_peptides</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.11731283366680145, 0.022457238286733627, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>dbAMP_12148</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.2212764024734497, 0.15402714908123016, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>dbAMP_12158</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.07279127091169357, 0.05830632895231247, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>dbAMP_12161</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.17415067553520203, 0.11548949033021927, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>dbAMP_12203</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.03923531994223595, -0.0253727026283741, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dbAMP_12362</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.01778259314596653, -0.03611695393919945, 7....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22910 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ID  length  class  \\\n",
       "1111     10015_dbaasp,10016_dbaasp|dbaasp_peptides      11      0   \n",
       "973                   10026_dbaasp|dbaasp_peptides      22      0   \n",
       "524                   10029_dbaasp|dbaasp_peptides      14      0   \n",
       "1979  1003,1011,1019,1027,1035|CancerPPD_l_natural      20      0   \n",
       "1917                  10030_dbaasp|dbaasp_peptides      18      0   \n",
       "...                                            ...     ...    ...   \n",
       "1609                                   dbAMP_12148      13      0   \n",
       "617                                    dbAMP_12158      17      0   \n",
       "1051                                   dbAMP_12161      16      0   \n",
       "1821                                   dbAMP_12203      15      0   \n",
       "15                                     dbAMP_12362      30      0   \n",
       "\n",
       "                                                   reps  \n",
       "1111  [0.20464208722114563, -0.055944692343473434, 0...  \n",
       "973   [0.1005186140537262, 0.0014500601682811975, 0....  \n",
       "524   [0.14606480300426483, 0.04153195396065712, 0.0...  \n",
       "1979  [0.02989775501191616, -0.004465686157345772, -...  \n",
       "1917  [0.11731283366680145, 0.022457238286733627, 0....  \n",
       "...                                                 ...  \n",
       "1609  [0.2212764024734497, 0.15402714908123016, 0.12...  \n",
       "617   [0.07279127091169357, 0.05830632895231247, -0....  \n",
       "1051  [0.17415067553520203, 0.11548949033021927, 0.0...  \n",
       "1821  [0.03923531994223595, -0.0253727026283741, -0....  \n",
       "15    [0.01778259314596653, -0.03611695393919945, 7....  \n",
       "\n",
       "[22910 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AMPs_df.drop_duplicates(subset=['Sequence'],inplace=True)\n",
    "AMPs_df =AMPs_df[AMPs_df[\"length\"] <=30 ]\n",
    "AMPs_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2i7Tk41aDZ5"
   },
   "source": [
    "### Utility function: plot_history, display_model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7F7ykQsDVxHO"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  # dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  x = range(1, len(acc) + 1)\n",
    "\n",
    "  plt.figure(figsize=(12, 5))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(x, acc, 'b', label='Training acc')\n",
    "  plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(x, loss, 'b', label='Training loss')\n",
    "  plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.legend()\n",
    "\n",
    "# Display model score(Loss & Accuracy) across all sets.\n",
    "def display_model_score(model, train, val, test):\n",
    "  train_score = model.evaluate(train[0], train[1], verbose=1)\n",
    "  print('Train loss: ', train_score[0])\n",
    "  print('Train accuracy: ', train_score[1])\n",
    "  print('-'*70)\n",
    "  val_score = model.evaluate(val[0], val[1], verbose=1)\n",
    "  print('Val loss: ', val_score[0])\n",
    "  print('Val accuracy: ', val_score[1])\n",
    "  print('-'*70)\n",
    "  test_score = model.evaluate(test[0], test[1], verbose=1)\n",
    "  print('Test loss: ', test_score[0])\n",
    "  print('Test accuracy: ', test_score[1])\n",
    "\n",
    "def plot_history_CV(cv, estimator,x,y):\n",
    "  # plot arrows\n",
    "  fig1 = plt.figure(figsize=[12,12])\n",
    "  ax1 = fig1.add_subplot(111,aspect = 'equal')\n",
    "  ax1.add_patch(\n",
    "      patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
    "      )\n",
    "  ax1.add_patch(\n",
    "      patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
    "      )\n",
    "\n",
    "  tprs = []\n",
    "  aucs = []\n",
    "  mean_fpr = np.linspace(0,1,100)\n",
    "  i = 1\n",
    "  for train,test in cv.split(x,y):\n",
    "      model = create_Modelbaseline()\n",
    "      model.fit(x[train],y.iloc[train],\n",
    "            epochs=30,\n",
    "            shuffle=True,verbose=0)\n",
    "      prediction = model.predict(x[test])\n",
    "      fpr, tpr, t = roc_curve(y[test], prediction[:, 1])\n",
    "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "      roc_auc = auc(fpr, tpr)\n",
    "      aucs.append(roc_auc)\n",
    "      plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "      i= i+1\n",
    "\n",
    "  plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "  mean_tpr = np.mean(tprs, axis=0)\n",
    "  mean_auc = auc(mean_fpr, mean_tpr)\n",
    "  plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "          label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.title('ROC')\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "  plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI-_ZAvfIb5A"
   },
   "source": [
    "# Split Train/ Test / Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137389, 400)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AMPs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lAAQLx4UIptD"
   },
   "outputs": [],
   "source": [
    "# For PWM -400 dimension\n",
    "window_sizes = 20\n",
    "\n",
    "X = AMPs_df[:,0:window_sizes*20].reshape(len(AMPs_df),1,20,window_sizes)\n",
    "Y = np.array([0] * 68869 + [1] * 68520)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UWQ2IZWgIbST"
   },
   "outputs": [],
   "source": [
    "# For jax unirep \n",
    "#X= np.array(AMPs_df['reps'].to_list())\n",
    "#y= np.array(AMPs_df['class'].to_list())\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.array(AMPs_df['reps'].to_list()), np.array(AMPs_df['class'].to_list()), test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "#del X\n",
    "#del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "IM7Scdevkwpp",
    "outputId": "55fa7ea6-a5f9-4e23-bf42-f1a4bf64f8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  82433\n",
      "Val size:  27478\n",
      "Test size:  27478\n"
     ]
    }
   ],
   "source": [
    "# Given data size\n",
    "print('Train size: ', len(X_train))\n",
    "print('Val size: ', len(X_val))\n",
    "print('Test size: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dHnNnR7IAAs"
   },
   "source": [
    "# Model 4: Deep-AmPEP30 + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d dimension\n",
    "# Batchs, n_timesteps, n_features\n",
    "\n",
    "# Images 3d dimension\n",
    "# width , heigth , channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53214, 1900, 1)\n",
      "(17739, 1900, 1)\n",
      "(17739, 1900, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# https://stackoverflow.com/questions/52803989/how-to-correct-shape-of-keras-input-into-a-3d-array/52804200\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
    "print(X_train.shape)\n",
    "X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\n",
    "print(X_test.shape)\n",
    "X_val = np.reshape(X_val,(X_val.shape[0],X_val.shape[1],1))\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original method \n",
    "def create_Modelbaseline():\n",
    "    x_input = Input(shape=(1900,1)) # n_timesteps, n_features\n",
    "    # Conv\n",
    "    conv = Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu')(x_input) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    conv = Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu')(conv) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "    # Flatten NN\n",
    "    flat = Flatten()(conv)\n",
    "    \n",
    "    layer_3 = Dense(128)(flat)\n",
    "    dropout_3 = Dropout(0.2)(layer_3)\n",
    "    layer_4 = Dense(10)(dropout_3)\n",
    "    dropout_4 = Dropout(0.2)(layer_4)\n",
    "    x_output = Dense(1, activation='sigmoid', name='output_layer', kernel_regularizer=l2(0.0001))(dropout_4)\n",
    "\n",
    "    model = Model(inputs=x_input, outputs=x_output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_Modelbaseline()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_value(model.optimizer.learning_rate, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      " Learning rate:  0.01\n",
      "Epoch 1/100\n",
      "  2/832 [..............................] - ETA: 1:35 - loss: 1005.9172 - accuracy: 0.5312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0714s vs `on_train_batch_end` time: 0.1584s). Check your callbacks.\n",
      "832/832 [==============================] - ETA: 0s - loss: 3.6077 - accuracy: 0.6687\n",
      "Epoch 00001: loss improved from inf to 3.60769, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 225s 270ms/step - loss: 3.6077 - accuracy: 0.6687 - val_loss: 0.7354 - val_accuracy: 0.5756\n",
      " Learning rate:  0.01\n",
      "Epoch 2/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.4792 - accuracy: 0.7826\n",
      "Epoch 00002: loss improved from 3.60769 to 0.47920, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 267ms/step - loss: 0.4792 - accuracy: 0.7826 - val_loss: 0.4271 - val_accuracy: 0.8109\n",
      " Learning rate:  0.01\n",
      "Epoch 3/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.4290 - accuracy: 0.8138\n",
      "Epoch 00003: loss improved from 0.47920 to 0.42904, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.4290 - accuracy: 0.8138 - val_loss: 0.4231 - val_accuracy: 0.8265\n",
      " Learning rate:  0.01\n",
      "Epoch 4/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3968 - accuracy: 0.8314\n",
      "Epoch 00004: loss improved from 0.42904 to 0.39679, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 267ms/step - loss: 0.3968 - accuracy: 0.8314 - val_loss: 0.4213 - val_accuracy: 0.8233\n",
      " Learning rate:  0.01\n",
      "Epoch 5/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3747 - accuracy: 0.8414\n",
      "Epoch 00005: loss improved from 0.39679 to 0.37471, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.3747 - accuracy: 0.8414 - val_loss: 0.3564 - val_accuracy: 0.8529\n",
      " Learning rate:  0.01\n",
      "Epoch 6/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3653 - accuracy: 0.8476\n",
      "Epoch 00006: loss improved from 0.37471 to 0.36531, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.3653 - accuracy: 0.8476 - val_loss: 0.3555 - val_accuracy: 0.8554\n",
      " Learning rate:  0.01\n",
      "Epoch 7/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3545 - accuracy: 0.8526\n",
      "Epoch 00007: loss improved from 0.36531 to 0.35447, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 263ms/step - loss: 0.3545 - accuracy: 0.8526 - val_loss: 0.4054 - val_accuracy: 0.8318\n",
      " Learning rate:  0.01\n",
      "Epoch 8/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3490 - accuracy: 0.8532\n",
      "Epoch 00008: loss improved from 0.35447 to 0.34902, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.3490 - accuracy: 0.8532 - val_loss: 0.3620 - val_accuracy: 0.8473\n",
      " Learning rate:  0.01\n",
      "Epoch 9/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.8566\n",
      "Epoch 00009: loss improved from 0.34902 to 0.34539, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 265ms/step - loss: 0.3454 - accuracy: 0.8566 - val_loss: 0.3567 - val_accuracy: 0.8469\n",
      " Learning rate:  0.01\n",
      "Epoch 10/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.8571\n",
      "Epoch 00010: loss improved from 0.34539 to 0.34447, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.3445 - accuracy: 0.8571 - val_loss: 0.3408 - val_accuracy: 0.8531\n",
      " Learning rate:  0.01\n",
      "Epoch 11/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8577\n",
      "Epoch 00011: loss improved from 0.34447 to 0.34197, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 224s 269ms/step - loss: 0.3420 - accuracy: 0.8577 - val_loss: 0.3372 - val_accuracy: 0.8577\n",
      " Learning rate:  0.01\n",
      "Epoch 12/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3394 - accuracy: 0.8598\n",
      "Epoch 00012: loss improved from 0.34197 to 0.33937, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.3394 - accuracy: 0.8598 - val_loss: 0.3412 - val_accuracy: 0.8550\n",
      " Learning rate:  0.01\n",
      "Epoch 13/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3749 - accuracy: 0.8410\n",
      "Epoch 00013: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3749 - accuracy: 0.8410 - val_loss: 0.3348 - val_accuracy: 0.8603\n",
      " Learning rate:  0.01\n",
      "Epoch 14/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3437 - accuracy: 0.8604\n",
      "Epoch 00014: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3437 - accuracy: 0.8604 - val_loss: 0.3791 - val_accuracy: 0.8596\n",
      " Learning rate:  0.01\n",
      "Epoch 15/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3463 - accuracy: 0.8559\n",
      "Epoch 00015: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3463 - accuracy: 0.8559 - val_loss: 0.3580 - val_accuracy: 0.8560\n",
      " Learning rate:  0.01\n",
      "Epoch 16/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3460 - accuracy: 0.8581\n",
      "Epoch 00016: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3460 - accuracy: 0.8581 - val_loss: 0.3562 - val_accuracy: 0.8490\n",
      " Learning rate:  0.01\n",
      "Epoch 17/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.8570\n",
      "Epoch 00017: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3523 - accuracy: 0.8570 - val_loss: 0.3415 - val_accuracy: 0.8527\n",
      " Learning rate:  0.01\n",
      "Epoch 18/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3503 - accuracy: 0.8549\n",
      "Epoch 00018: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3503 - accuracy: 0.8549 - val_loss: 0.3723 - val_accuracy: 0.8369\n",
      " Learning rate:  0.01\n",
      "Epoch 19/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.8547\n",
      "Epoch 00019: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3564 - accuracy: 0.8547 - val_loss: 0.3603 - val_accuracy: 0.8598\n",
      " Learning rate:  0.01\n",
      "Epoch 20/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3609 - accuracy: 0.8521\n",
      "Epoch 00020: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3609 - accuracy: 0.8521 - val_loss: 0.3592 - val_accuracy: 0.8476\n",
      " Learning rate:  0.01\n",
      "Epoch 21/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3603 - accuracy: 0.8507\n",
      "Epoch 00021: loss did not improve from 0.33937\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.3603 - accuracy: 0.8507 - val_loss: 0.3464 - val_accuracy: 0.8507\n",
      " Learning rate:  0.001\n",
      "Epoch 22/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.8706\n",
      "Epoch 00022: loss improved from 0.33937 to 0.31001, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 224s 269ms/step - loss: 0.3100 - accuracy: 0.8706 - val_loss: 0.3242 - val_accuracy: 0.8657\n",
      " Learning rate:  0.001\n",
      "Epoch 23/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.8730\n",
      "Epoch 00023: loss improved from 0.31001 to 0.30292, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.3029 - accuracy: 0.8730 - val_loss: 0.3265 - val_accuracy: 0.8655\n",
      " Learning rate:  0.001\n",
      "Epoch 24/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3004 - accuracy: 0.8744\n",
      "Epoch 00024: loss improved from 0.30292 to 0.30037, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 223s 268ms/step - loss: 0.3004 - accuracy: 0.8744 - val_loss: 0.3209 - val_accuracy: 0.8656\n",
      " Learning rate:  0.001\n",
      "Epoch 25/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.8747\n",
      "Epoch 00025: loss improved from 0.30037 to 0.29902, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.2990 - accuracy: 0.8747 - val_loss: 0.3222 - val_accuracy: 0.8665\n",
      " Learning rate:  0.001\n",
      "Epoch 26/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2963 - accuracy: 0.8768\n",
      "Epoch 00026: loss improved from 0.29902 to 0.29633, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 265ms/step - loss: 0.2963 - accuracy: 0.8768 - val_loss: 0.3168 - val_accuracy: 0.8685\n",
      " Learning rate:  0.001\n",
      "Epoch 27/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2957 - accuracy: 0.8767\n",
      "Epoch 00027: loss improved from 0.29633 to 0.29571, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 263ms/step - loss: 0.2957 - accuracy: 0.8767 - val_loss: 0.3149 - val_accuracy: 0.8664\n",
      " Learning rate:  0.001\n",
      "Epoch 28/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2937 - accuracy: 0.8768\n",
      "Epoch 00028: loss improved from 0.29571 to 0.29370, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 265ms/step - loss: 0.2937 - accuracy: 0.8768 - val_loss: 0.3119 - val_accuracy: 0.8702\n",
      " Learning rate:  0.001\n",
      "Epoch 29/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2928 - accuracy: 0.8782\n",
      "Epoch 00029: loss improved from 0.29370 to 0.29282, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 263ms/step - loss: 0.2928 - accuracy: 0.8782 - val_loss: 0.3199 - val_accuracy: 0.8645\n",
      " Learning rate:  0.001\n",
      "Epoch 30/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2925 - accuracy: 0.8786\n",
      "Epoch 00030: loss improved from 0.29282 to 0.29246, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 264ms/step - loss: 0.2925 - accuracy: 0.8786 - val_loss: 0.3162 - val_accuracy: 0.8701\n",
      " Learning rate:  0.001\n",
      "Epoch 31/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8790\n",
      "Epoch 00031: loss improved from 0.29246 to 0.29131, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 265ms/step - loss: 0.2913 - accuracy: 0.8790 - val_loss: 0.3120 - val_accuracy: 0.8695\n",
      " Learning rate:  0.001\n",
      "Epoch 32/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2908 - accuracy: 0.8801\n",
      "Epoch 00032: loss improved from 0.29131 to 0.29082, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.2908 - accuracy: 0.8801 - val_loss: 0.3170 - val_accuracy: 0.8684\n",
      " Learning rate:  0.001\n",
      "Epoch 33/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2906 - accuracy: 0.8787\n",
      "Epoch 00033: loss improved from 0.29082 to 0.29058, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 267ms/step - loss: 0.2906 - accuracy: 0.8787 - val_loss: 0.3130 - val_accuracy: 0.8710\n",
      " Learning rate:  0.001\n",
      "Epoch 34/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.8794\n",
      "Epoch 00034: loss improved from 0.29058 to 0.28897, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 263ms/step - loss: 0.2890 - accuracy: 0.8794 - val_loss: 0.3128 - val_accuracy: 0.8692\n",
      " Learning rate:  0.001\n",
      "Epoch 35/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.8801\n",
      "Epoch 00035: loss improved from 0.28897 to 0.28866, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 266ms/step - loss: 0.2887 - accuracy: 0.8801 - val_loss: 0.3187 - val_accuracy: 0.8666\n",
      " Learning rate:  0.001\n",
      "Epoch 36/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2883 - accuracy: 0.8808\n",
      "Epoch 00036: loss improved from 0.28866 to 0.28828, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 265ms/step - loss: 0.2883 - accuracy: 0.8808 - val_loss: 0.3126 - val_accuracy: 0.8689\n",
      " Learning rate:  0.001\n",
      "Epoch 37/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.8813\n",
      "Epoch 00037: loss improved from 0.28828 to 0.28631, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.2863 - accuracy: 0.8813 - val_loss: 0.3093 - val_accuracy: 0.8732\n",
      " Learning rate:  0.001\n",
      "Epoch 38/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2865 - accuracy: 0.8808\n",
      "Epoch 00038: loss did not improve from 0.28631\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2865 - accuracy: 0.8808 - val_loss: 0.3167 - val_accuracy: 0.8692\n",
      " Learning rate:  0.001\n",
      "Epoch 39/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.8813\n",
      "Epoch 00039: loss did not improve from 0.28631\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2864 - accuracy: 0.8813 - val_loss: 0.3122 - val_accuracy: 0.8714\n",
      " Learning rate:  0.001\n",
      "Epoch 40/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8813\n",
      "Epoch 00040: loss improved from 0.28631 to 0.28562, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 266ms/step - loss: 0.2856 - accuracy: 0.8813 - val_loss: 0.3213 - val_accuracy: 0.8701\n",
      " Learning rate:  0.001\n",
      "Epoch 41/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 0.8817\n",
      "Epoch 00041: loss improved from 0.28562 to 0.28477, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 264ms/step - loss: 0.2848 - accuracy: 0.8817 - val_loss: 0.3114 - val_accuracy: 0.8687\n",
      " Learning rate:  0.001\n",
      "Epoch 42/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2851 - accuracy: 0.8819\n",
      "Epoch 00042: loss did not improve from 0.28477\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2851 - accuracy: 0.8819 - val_loss: 0.3123 - val_accuracy: 0.8679\n",
      " Learning rate:  0.001\n",
      "Epoch 43/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.8829\n",
      "Epoch 00043: loss improved from 0.28477 to 0.28339, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.2834 - accuracy: 0.8829 - val_loss: 0.3209 - val_accuracy: 0.8719\n",
      " Learning rate:  0.001\n",
      "Epoch 44/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2843 - accuracy: 0.8812\n",
      "Epoch 00044: loss did not improve from 0.28339\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2843 - accuracy: 0.8812 - val_loss: 0.3082 - val_accuracy: 0.8714\n",
      " Learning rate:  0.001\n",
      "Epoch 45/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2827 - accuracy: 0.8820\n",
      "Epoch 00045: loss improved from 0.28339 to 0.28268, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 266ms/step - loss: 0.2827 - accuracy: 0.8820 - val_loss: 0.3120 - val_accuracy: 0.8707\n",
      " Learning rate:  0.001\n",
      "Epoch 46/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2818 - accuracy: 0.8830\n",
      "Epoch 00046: loss improved from 0.28268 to 0.28184, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 223s 268ms/step - loss: 0.2818 - accuracy: 0.8830 - val_loss: 0.3202 - val_accuracy: 0.8699\n",
      " Learning rate:  0.001\n",
      "Epoch 47/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2816 - accuracy: 0.8833\n",
      "Epoch 00047: loss improved from 0.28184 to 0.28160, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.2816 - accuracy: 0.8833 - val_loss: 0.3067 - val_accuracy: 0.8735\n",
      " Learning rate:  0.001\n",
      "Epoch 48/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2824 - accuracy: 0.8833\n",
      "Epoch 00048: loss did not improve from 0.28160\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2824 - accuracy: 0.8833 - val_loss: 0.3203 - val_accuracy: 0.8715\n",
      " Learning rate:  0.001\n",
      "Epoch 49/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2827 - accuracy: 0.8843\n",
      "Epoch 00049: loss did not improve from 0.28160\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2827 - accuracy: 0.8843 - val_loss: 0.3164 - val_accuracy: 0.8710\n",
      " Learning rate:  0.001\n",
      "Epoch 50/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2821 - accuracy: 0.8823\n",
      "Epoch 00050: loss did not improve from 0.28160\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2821 - accuracy: 0.8823 - val_loss: 0.3103 - val_accuracy: 0.8718\n",
      " Learning rate:  0.001\n",
      "Epoch 51/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2806 - accuracy: 0.8839\n",
      "Epoch 00051: loss improved from 0.28160 to 0.28060, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 223s 268ms/step - loss: 0.2806 - accuracy: 0.8839 - val_loss: 0.3094 - val_accuracy: 0.8720\n",
      " Learning rate:  3e-05\n",
      "Epoch 52/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.8866\n",
      "Epoch 00052: loss improved from 0.28060 to 0.27374, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 221s 266ms/step - loss: 0.2737 - accuracy: 0.8866 - val_loss: 0.3078 - val_accuracy: 0.8731\n",
      " Learning rate:  3e-05\n",
      "Epoch 53/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.8879\n",
      "Epoch 00053: loss improved from 0.27374 to 0.27212, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.2721 - accuracy: 0.8879 - val_loss: 0.3075 - val_accuracy: 0.8726\n",
      " Learning rate:  3e-05\n",
      "Epoch 54/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2714 - accuracy: 0.8879\n",
      "Epoch 00054: loss improved from 0.27212 to 0.27145, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 219s 264ms/step - loss: 0.2714 - accuracy: 0.8879 - val_loss: 0.3068 - val_accuracy: 0.8729\n",
      " Learning rate:  3e-05\n",
      "Epoch 55/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.8878\n",
      "Epoch 00055: loss did not improve from 0.27145\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2715 - accuracy: 0.8878 - val_loss: 0.3069 - val_accuracy: 0.8723\n",
      " Learning rate:  3e-05\n",
      "Epoch 56/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.8876\n",
      "Epoch 00056: loss improved from 0.27145 to 0.27118, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 267ms/step - loss: 0.2712 - accuracy: 0.8876 - val_loss: 0.3077 - val_accuracy: 0.8727\n",
      " Learning rate:  3e-05\n",
      "Epoch 57/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.8877\n",
      "Epoch 00057: loss improved from 0.27118 to 0.27085, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 220s 264ms/step - loss: 0.2709 - accuracy: 0.8877 - val_loss: 0.3072 - val_accuracy: 0.8725\n",
      " Learning rate:  3e-05\n",
      "Epoch 58/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.8878\n",
      "Epoch 00058: loss improved from 0.27085 to 0.27029, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 223s 268ms/step - loss: 0.2703 - accuracy: 0.8878 - val_loss: 0.3075 - val_accuracy: 0.8727\n",
      " Learning rate:  3e-05\n",
      "Epoch 59/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2707 - accuracy: 0.8883\n",
      "Epoch 00059: loss did not improve from 0.27029\n",
      "832/832 [==============================] - 207s 249ms/step - loss: 0.2707 - accuracy: 0.8883 - val_loss: 0.3077 - val_accuracy: 0.8721\n",
      " Learning rate:  3e-05\n",
      "Epoch 60/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2712 - accuracy: 0.8883\n",
      "Epoch 00060: loss did not improve from 0.27029\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2712 - accuracy: 0.8883 - val_loss: 0.3078 - val_accuracy: 0.8727\n",
      " Learning rate:  3e-05\n",
      "Epoch 61/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2708 - accuracy: 0.8884\n",
      "Epoch 00061: loss did not improve from 0.27029\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2708 - accuracy: 0.8884 - val_loss: 0.3081 - val_accuracy: 0.8723\n",
      " Learning rate:  3e-05\n",
      "Epoch 62/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.8887\n",
      "Epoch 00062: loss did not improve from 0.27029\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2705 - accuracy: 0.8887 - val_loss: 0.3079 - val_accuracy: 0.8725\n",
      " Learning rate:  3e-05\n",
      "Epoch 63/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2708 - accuracy: 0.8886\n",
      "Epoch 00063: loss did not improve from 0.27029\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2708 - accuracy: 0.8886 - val_loss: 0.3076 - val_accuracy: 0.8727\n",
      " Learning rate:  3e-05\n",
      "Epoch 64/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2701 - accuracy: 0.8886\n",
      "Epoch 00064: loss improved from 0.27029 to 0.27012, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 222s 267ms/step - loss: 0.2701 - accuracy: 0.8886 - val_loss: 0.3080 - val_accuracy: 0.8725\n",
      " Learning rate:  3e-05\n",
      "Epoch 65/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.8878\n",
      "Epoch 00065: loss did not improve from 0.27012\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2705 - accuracy: 0.8878 - val_loss: 0.3083 - val_accuracy: 0.8720\n",
      " Learning rate:  3e-05\n",
      "Epoch 66/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2703 - accuracy: 0.8886\n",
      "Epoch 00066: loss did not improve from 0.27012\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2703 - accuracy: 0.8886 - val_loss: 0.3060 - val_accuracy: 0.8727\n",
      " Learning rate:  3e-05\n",
      "Epoch 67/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.8885\n",
      "Epoch 00067: loss did not improve from 0.27012\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2704 - accuracy: 0.8885 - val_loss: 0.3085 - val_accuracy: 0.8723\n",
      " Learning rate:  3e-05\n",
      "Epoch 68/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.8879\n",
      "Epoch 00068: loss did not improve from 0.27012\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2706 - accuracy: 0.8879 - val_loss: 0.3084 - val_accuracy: 0.8724\n",
      " Learning rate:  3e-05\n",
      "Epoch 69/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.8883\n",
      "Epoch 00069: loss improved from 0.27012 to 0.26948, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 224s 269ms/step - loss: 0.2695 - accuracy: 0.8883 - val_loss: 0.3090 - val_accuracy: 0.8721\n",
      " Learning rate:  3e-05\n",
      "Epoch 70/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.8878\n",
      "Epoch 00070: loss did not improve from 0.26948\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2704 - accuracy: 0.8878 - val_loss: 0.3084 - val_accuracy: 0.8721\n",
      " Learning rate:  3e-05\n",
      "Epoch 71/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2710 - accuracy: 0.8891\n",
      "Epoch 00071: loss did not improve from 0.26948\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2710 - accuracy: 0.8891 - val_loss: 0.3076 - val_accuracy: 0.8720\n",
      " Learning rate:  5e-06\n",
      "Epoch 72/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2693 - accuracy: 0.8889\n",
      "Epoch 00072: loss improved from 0.26948 to 0.26927, saving model to /mnt/vdb/thesis/AmPPEP30.1900.hdf5\n",
      "832/832 [==============================] - 223s 268ms/step - loss: 0.2693 - accuracy: 0.8889 - val_loss: 0.3078 - val_accuracy: 0.8721\n",
      " Learning rate:  5e-06\n",
      "Epoch 73/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.8891\n",
      "Epoch 00073: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2696 - accuracy: 0.8891 - val_loss: 0.3078 - val_accuracy: 0.8721\n",
      " Learning rate:  5e-06\n",
      "Epoch 74/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2695 - accuracy: 0.8890\n",
      "Epoch 00074: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2695 - accuracy: 0.8890 - val_loss: 0.3078 - val_accuracy: 0.8722\n",
      " Learning rate:  5e-06\n",
      "Epoch 75/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.8882\n",
      "Epoch 00075: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2705 - accuracy: 0.8882 - val_loss: 0.3078 - val_accuracy: 0.8720\n",
      " Learning rate:  5e-06\n",
      "Epoch 76/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.8887\n",
      "Epoch 00076: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2697 - accuracy: 0.8887 - val_loss: 0.3080 - val_accuracy: 0.8720\n",
      " Learning rate:  5e-06\n",
      "Epoch 77/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2698 - accuracy: 0.8882\n",
      "Epoch 00077: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2698 - accuracy: 0.8882 - val_loss: 0.3079 - val_accuracy: 0.8721\n",
      " Learning rate:  5e-06\n",
      "Epoch 78/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.8878\n",
      "Epoch 00078: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 249ms/step - loss: 0.2697 - accuracy: 0.8878 - val_loss: 0.3079 - val_accuracy: 0.8724\n",
      " Learning rate:  5e-06\n",
      "Epoch 79/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.8894\n",
      "Epoch 00079: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2700 - accuracy: 0.8894 - val_loss: 0.3079 - val_accuracy: 0.8722\n",
      " Learning rate:  5e-06\n",
      "Epoch 80/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.8886\n",
      "Epoch 00080: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2704 - accuracy: 0.8886 - val_loss: 0.3077 - val_accuracy: 0.8723\n",
      " Learning rate:  5e-06\n",
      "Epoch 81/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2697 - accuracy: 0.8881\n",
      "Epoch 00081: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2697 - accuracy: 0.8881 - val_loss: 0.3079 - val_accuracy: 0.8723\n",
      " Learning rate:  5e-06\n",
      "Epoch 82/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.8881\n",
      "Epoch 00082: loss did not improve from 0.26927\n",
      "832/832 [==============================] - 208s 250ms/step - loss: 0.2702 - accuracy: 0.8881 - val_loss: 0.3079 - val_accuracy: 0.8721\n",
      "Epoch 00082: early stopping\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"/mnt/vdb/thesis/AmPPEP30.1900.hdf5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "# Early Stopping\n",
    "es = EarlyStopping(monitor='loss', patience=8, verbose=1)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    batch_size=64, validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint, es,lr_scheduler], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1663/1663 [==============================] - 63s 38ms/step - loss: 0.2671 - accuracy: 0.8893\n",
      "Train loss:  0.26714572310447693\n",
      "Train accuracy:  0.8892960548400879\n",
      "----------------------------------------------------------------------\n",
      "555/555 [==============================] - 21s 38ms/step - loss: 0.3079 - accuracy: 0.8721\n",
      "Val loss:  0.307868629693985\n",
      "Val accuracy:  0.8721461296081543\n",
      "----------------------------------------------------------------------\n",
      "555/555 [==============================] - 21s 38ms/step - loss: 0.3020 - accuracy: 0.8752\n",
      "Test loss:  0.3020343482494354\n",
      "Test accuracy:  0.8752466440200806\n"
     ]
    }
   ],
   "source": [
    "display_model_score(model,\n",
    "    [X_train, y_train],\n",
    "    [X_val, y_val],\n",
    "    [X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.88      8928\n",
      "           1       0.87      0.88      0.87      8811\n",
      "\n",
      "    accuracy                           0.88     17739\n",
      "   macro avg       0.88      0.88      0.88     17739\n",
      "weighted avg       0.88      0.88      0.88     17739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_probas = model.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_predict = np.where(y_probas > threshold, 1, 0)\n",
    "\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1yvj_MHIA6-"
   },
   "source": [
    "# Model 3: ProtCNN\n",
    "https://www.biorxiv.org/content/10.1101/626507v3.full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82433, 1900, 1)\n",
      "(27478, 1900, 1)\n",
      "(27478, 1900, 1)\n"
     ]
    }
   ],
   "source": [
    "# convert format \n",
    "import numpy as np\n",
    "# https://stackoverflow.com/questions/52803989/how-to-correct-shape-of-keras-input-into-a-3d-array/52804200\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
    "print(X_train.shape)\n",
    "X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\n",
    "print(X_test.shape)\n",
    "X_val = np.reshape(X_val,(X_val.shape[0],X_val.shape[1],1))\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "uIIiKYP8IRYE"
   },
   "outputs": [],
   "source": [
    "def residual_block(data, filters, d_rate):\n",
    "  \"\"\"\n",
    "  residual_block consist of two resnet layers\n",
    "  For dilation_rate  # https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d\n",
    "  \n",
    "  _data: input\n",
    "  _filters: convolution filters\n",
    "  _d_rate: dilation rate\n",
    "  \"\"\"\n",
    "\n",
    "  shortcut = data\n",
    "\n",
    "  bn1 = BatchNormalization()(data)\n",
    "  act1 = Activation('relu')(bn1)\n",
    "  conv1 = Conv1D(filters, 1, dilation_rate=d_rate, padding='same', kernel_regularizer=l2(0.001))(act1)\n",
    "\n",
    "  #bottleneck convolution\n",
    "  bn2 = BatchNormalization()(conv1)\n",
    "  act2 = Activation('relu')(bn2)\n",
    "  conv2 = Conv1D(filters, 3, padding='same', kernel_regularizer=l2(0.001))(act2)\n",
    "\n",
    "  #skip connection\n",
    "  x = Add()([conv2, shortcut])\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 1900, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, 1900, 128)    256         input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 1900, 128)    512         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 1900, 128)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, 1900, 128)    16512       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 1900, 128)    512         conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 1900, 128)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 1900, 128)    49280       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 1900, 128)    0           conv1d_40[0][0]                  \n",
      "                                                                 conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 1900, 128)    512         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 1900, 128)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 1900, 128)    16512       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 1900, 128)    512         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 1900, 128)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 1900, 128)    49280       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 1900, 128)    0           conv1d_42[0][0]                  \n",
      "                                                                 add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 633, 128)     0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 633, 128)     0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 81024)        0           dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            81025       flatten_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 214,913\n",
      "Trainable params: 213,889\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_input = Input(shape=(1900,1))\n",
    "\n",
    "#initial conv\n",
    "conv = Conv1D(128, 1, padding='same')(x_input) \n",
    "\n",
    "# per-residue representation\n",
    "res1 = residual_block(conv, 128, 2)\n",
    "res2 = residual_block(res1, 128, 3)\n",
    "\n",
    "x = MaxPooling1D(3)(res2)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# softmax classifier\n",
    "x = Flatten()(x)\n",
    "x_output = Dense(1000, activation='sigmoid', kernel_regularizer=l2(0.0001))(x)\n",
    "#x_output = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "model_ProtCNN = Model(inputs=x_input, outputs=x_output)\n",
    "model_ProtCNN.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_ProtCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good model for detect nonAMP\n",
    "\n",
    "x_input = Input(shape=(1900,1))\n",
    "\n",
    "#initial conv\n",
    "conv = Conv1D(256, 1, padding='same')(x_input) \n",
    "\n",
    "# per-residue representation\n",
    "res1 = residual_block(conv, 256, 2)\n",
    "res2 = residual_block(res1, 256, 3)\n",
    "\n",
    "x = MaxPooling1D(3)(res2)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# softmax classifier\n",
    "x = Flatten()(x)\n",
    "layer_4 = Dense(1211 , activation='relu')(x)\n",
    "layer_4 = Dropout(0.5)(layer_4)\n",
    "layer_4 = Dense(1211 , activation='relu')(layer_4)\n",
    "layer_4 = Dropout(0.5)(layer_4)\n",
    "layer_4 = Dense(1211 , activation='relu')(layer_4)\n",
    "layer_4 = Dropout(0.5)(layer_4)\n",
    "layer_4 = Dense(1211 , activation='relu')(layer_4)\n",
    "x_output = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0001))(layer_4)\n",
    "\n",
    "model_ProtCNN = Model(inputs=x_input, outputs=x_output)\n",
    "model_ProtCNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_ProtCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iFyHyapJ4EKF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1900, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 1900, 256)    512         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1900, 256)    1024        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1900, 256)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1900, 256)    65792       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1900, 256)    1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1900, 256)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1900, 256)    196864      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1900, 256)    0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1900, 256)    1024        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1900, 256)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1900, 256)    65792       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1900, 256)    1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1900, 256)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1900, 256)    196864      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1900, 256)    0           conv1d_4[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 633, 256)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 633, 256)     1024        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 162048)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1211)         196241339   flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1211)         0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          620544      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            513         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 197,393,340\n",
      "Trainable params: 197,390,780\n",
      "Non-trainable params: 2,560\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "x_input = Input(shape=(1900,1))\n",
    "\n",
    "#initial conv\n",
    "conv = Conv1D(256, 1, padding='same')(x_input) \n",
    "\n",
    "# per-residue representation\n",
    "res1 = residual_block(conv, 256, 2)\n",
    "res2 = residual_block(res1, 256, 3)\n",
    "\n",
    "x = MaxPooling1D(3)(res2)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# softmax classifier\n",
    "x = Flatten()(x)\n",
    "layer_4 = Dense(1211 , activation='relu')(x)\n",
    "layer_4 = Dropout(0.5)(layer_4)\n",
    "layer_4 = Dense(512 , activation='relu')(layer_4)\n",
    "layer_4 = Dropout(0.5)(layer_4)\n",
    "x_output = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0001))(layer_4)\n",
    "\n",
    "model_ProtCNN = Model(inputs=x_input, outputs=x_output)\n",
    "model_ProtCNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_ProtCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import backend as K\n",
    "#K.set_value(model_ProtCNN.optimizer.learning_rate, 0.00001)\n",
    "def lr_schedule(epoch):\n",
    "    \n",
    "    lr = 1e-3\n",
    "    if epoch > 80:\n",
    "        lr = 0.1e-6\n",
    "    elif epoch > 50:    \n",
    "        lr = 0.3e-5\n",
    "    elif epoch > 20:\n",
    "        lr = 1e-4\n",
    "        \n",
    "    print(' Learning rate: ', lr)    \n",
    "    return lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      " Learning rate:  0.001\n",
      "Epoch 1/100\n",
      "  2/645 [..............................] - ETA: 55s - loss: 73.7406 - accuracy: 0.4727WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0614s vs `on_train_batch_end` time: 0.1107s). Check your callbacks.\n",
      "644/645 [============================>.] - ETA: 0s - loss: 2.7184 - accuracy: 0.6246\n",
      "Epoch 00001: loss improved from inf to 2.71835, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 127s 196ms/step - loss: 2.7184 - accuracy: 0.6245 - val_loss: 1.3044 - val_accuracy: 0.7311\n",
      " Learning rate:  0.001\n",
      "Epoch 2/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 1.1884 - accuracy: 0.6522\n",
      "Epoch 00002: loss improved from 2.71835 to 1.18844, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 1.1884 - accuracy: 0.6522 - val_loss: 0.8606 - val_accuracy: 0.8189\n",
      " Learning rate:  0.001\n",
      "Epoch 3/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.8685 - accuracy: 0.6792\n",
      "Epoch 00003: loss improved from 1.18844 to 0.86846, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 0.8685 - accuracy: 0.6792 - val_loss: 0.6233 - val_accuracy: 0.8605\n",
      " Learning rate:  0.001\n",
      "Epoch 4/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.7214 - accuracy: 0.7038\n",
      "Epoch 00004: loss improved from 0.86846 to 0.72138, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.7214 - accuracy: 0.7038 - val_loss: 0.5386 - val_accuracy: 0.8645\n",
      " Learning rate:  0.001\n",
      "Epoch 5/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.5808 - accuracy: 0.7418\n",
      "Epoch 00005: loss improved from 0.72138 to 0.58084, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 130s 201ms/step - loss: 0.5808 - accuracy: 0.7418 - val_loss: 0.4378 - val_accuracy: 0.8696\n",
      " Learning rate:  0.001\n",
      "Epoch 6/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4936 - accuracy: 0.7915\n",
      "Epoch 00006: loss improved from 0.58084 to 0.49361, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 0.4936 - accuracy: 0.7915 - val_loss: 0.3762 - val_accuracy: 0.8622\n",
      " Learning rate:  0.001\n",
      "Epoch 7/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4846 - accuracy: 0.7804\n",
      "Epoch 00007: loss improved from 0.49361 to 0.48456, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 199ms/step - loss: 0.4846 - accuracy: 0.7804 - val_loss: 0.3551 - val_accuracy: 0.8700\n",
      " Learning rate:  0.001\n",
      "Epoch 8/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4458 - accuracy: 0.8089\n",
      "Epoch 00008: loss improved from 0.48456 to 0.44575, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 0.4457 - accuracy: 0.8089 - val_loss: 0.3541 - val_accuracy: 0.8661\n",
      " Learning rate:  0.001\n",
      "Epoch 9/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4347 - accuracy: 0.8140\n",
      "Epoch 00009: loss improved from 0.44575 to 0.43475, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 199ms/step - loss: 0.4347 - accuracy: 0.8140 - val_loss: 0.3606 - val_accuracy: 0.8503\n",
      " Learning rate:  0.001\n",
      "Epoch 10/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4687 - accuracy: 0.8091\n",
      "Epoch 00010: loss did not improve from 0.43475\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.4687 - accuracy: 0.8091 - val_loss: 0.3571 - val_accuracy: 0.8787\n",
      " Learning rate:  0.001\n",
      "Epoch 11/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4332 - accuracy: 0.8198\n",
      "Epoch 00011: loss improved from 0.43475 to 0.43319, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.4332 - accuracy: 0.8198 - val_loss: 0.4093 - val_accuracy: 0.8272\n",
      " Learning rate:  0.001\n",
      "Epoch 12/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4464 - accuracy: 0.8159\n",
      "Epoch 00012: loss did not improve from 0.43319\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.4464 - accuracy: 0.8159 - val_loss: 0.3543 - val_accuracy: 0.8677\n",
      " Learning rate:  0.001\n",
      "Epoch 13/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4251 - accuracy: 0.8247\n",
      "Epoch 00013: loss improved from 0.43319 to 0.42510, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 201ms/step - loss: 0.4251 - accuracy: 0.8247 - val_loss: 0.3663 - val_accuracy: 0.8692\n",
      " Learning rate:  0.001\n",
      "Epoch 14/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4261 - accuracy: 0.8232\n",
      "Epoch 00014: loss did not improve from 0.42510\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.4262 - accuracy: 0.8231 - val_loss: 0.5096 - val_accuracy: 0.7364\n",
      " Learning rate:  0.001\n",
      "Epoch 15/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.5000 - accuracy: 0.8020\n",
      "Epoch 00015: loss did not improve from 0.42510\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.5000 - accuracy: 0.8020 - val_loss: 0.3837 - val_accuracy: 0.8674\n",
      " Learning rate:  0.001\n",
      "Epoch 16/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4432 - accuracy: 0.8173\n",
      "Epoch 00016: loss did not improve from 0.42510\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.4432 - accuracy: 0.8173 - val_loss: 0.4267 - val_accuracy: 0.8128\n",
      " Learning rate:  0.001\n",
      "Epoch 17/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4196 - accuracy: 0.8283\n",
      "Epoch 00017: loss improved from 0.42510 to 0.41955, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 0.4196 - accuracy: 0.8283 - val_loss: 0.3318 - val_accuracy: 0.8730\n",
      " Learning rate:  0.001\n",
      "Epoch 18/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4044 - accuracy: 0.8327\n",
      "Epoch 00018: loss improved from 0.41955 to 0.40442, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.4044 - accuracy: 0.8327 - val_loss: 0.3173 - val_accuracy: 0.8826\n",
      " Learning rate:  0.001\n",
      "Epoch 19/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4035 - accuracy: 0.8351\n",
      "Epoch 00019: loss improved from 0.40442 to 0.40348, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 199ms/step - loss: 0.4035 - accuracy: 0.8352 - val_loss: 0.3222 - val_accuracy: 0.8806\n",
      " Learning rate:  0.001\n",
      "Epoch 20/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3981 - accuracy: 0.8357\n",
      "Epoch 00020: loss improved from 0.40348 to 0.39811, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 0.3981 - accuracy: 0.8357 - val_loss: 0.3734 - val_accuracy: 0.8499\n",
      " Learning rate:  0.001\n",
      "Epoch 21/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.4387 - accuracy: 0.8294\n",
      "Epoch 00021: loss did not improve from 0.39811\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.4387 - accuracy: 0.8294 - val_loss: 0.3547 - val_accuracy: 0.8741\n",
      " Learning rate:  0.0001\n",
      "Epoch 22/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3661 - accuracy: 0.8503\n",
      "Epoch 00022: loss improved from 0.39811 to 0.36607, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 0.3661 - accuracy: 0.8503 - val_loss: 0.3041 - val_accuracy: 0.8898\n",
      " Learning rate:  0.0001\n",
      "Epoch 23/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3533 - accuracy: 0.8550\n",
      "Epoch 00023: loss improved from 0.36607 to 0.35327, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.3533 - accuracy: 0.8550 - val_loss: 0.3050 - val_accuracy: 0.8958\n",
      " Learning rate:  0.0001\n",
      "Epoch 24/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3439 - accuracy: 0.8585\n",
      "Epoch 00024: loss improved from 0.35327 to 0.34385, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.3439 - accuracy: 0.8585 - val_loss: 0.2903 - val_accuracy: 0.8985\n",
      " Learning rate:  0.0001\n",
      "Epoch 25/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3417 - accuracy: 0.8585\n",
      "Epoch 00025: loss improved from 0.34385 to 0.34174, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 199ms/step - loss: 0.3417 - accuracy: 0.8585 - val_loss: 0.2850 - val_accuracy: 0.8993\n",
      " Learning rate:  0.0001\n",
      "Epoch 26/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.8614\n",
      "Epoch 00026: loss improved from 0.34174 to 0.33548, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 0.3355 - accuracy: 0.8614 - val_loss: 0.2876 - val_accuracy: 0.8964\n",
      " Learning rate:  0.0001\n",
      "Epoch 27/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3335 - accuracy: 0.8615\n",
      "Epoch 00027: loss improved from 0.33548 to 0.33348, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.3335 - accuracy: 0.8615 - val_loss: 0.2824 - val_accuracy: 0.8999\n",
      " Learning rate:  0.0001\n",
      "Epoch 28/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3308 - accuracy: 0.8635\n",
      "Epoch 00028: loss improved from 0.33348 to 0.33080, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.3308 - accuracy: 0.8635 - val_loss: 0.3048 - val_accuracy: 0.8929\n",
      " Learning rate:  0.0001\n",
      "Epoch 29/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3259 - accuracy: 0.8662\n",
      "Epoch 00029: loss improved from 0.33080 to 0.32592, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 0.3259 - accuracy: 0.8662 - val_loss: 0.2880 - val_accuracy: 0.8967\n",
      " Learning rate:  0.0001\n",
      "Epoch 30/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3242 - accuracy: 0.8660\n",
      "Epoch 00030: loss improved from 0.32592 to 0.32424, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.3242 - accuracy: 0.8660 - val_loss: 0.3059 - val_accuracy: 0.8891\n",
      " Learning rate:  0.0001\n",
      "Epoch 31/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3195 - accuracy: 0.8687\n",
      "Epoch 00031: loss improved from 0.32424 to 0.31945, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.3195 - accuracy: 0.8687 - val_loss: 0.2696 - val_accuracy: 0.9021\n",
      " Learning rate:  0.0001\n",
      "Epoch 32/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3168 - accuracy: 0.8705\n",
      "Epoch 00032: loss improved from 0.31945 to 0.31675, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.3168 - accuracy: 0.8705 - val_loss: 0.2753 - val_accuracy: 0.9006\n",
      " Learning rate:  0.0001\n",
      "Epoch 33/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3142 - accuracy: 0.8720\n",
      "Epoch 00033: loss improved from 0.31675 to 0.31424, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.3142 - accuracy: 0.8720 - val_loss: 0.2742 - val_accuracy: 0.9025\n",
      " Learning rate:  0.0001\n",
      "Epoch 34/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3093 - accuracy: 0.8743\n",
      "Epoch 00034: loss improved from 0.31424 to 0.30928, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.3093 - accuracy: 0.8743 - val_loss: 0.2667 - val_accuracy: 0.9035\n",
      " Learning rate:  0.0001\n",
      "Epoch 35/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8769\n",
      "Epoch 00035: loss improved from 0.30928 to 0.30550, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 130s 201ms/step - loss: 0.3055 - accuracy: 0.8769 - val_loss: 0.2666 - val_accuracy: 0.8974\n",
      " Learning rate:  0.0001\n",
      "Epoch 36/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8767\n",
      "Epoch 00036: loss improved from 0.30550 to 0.30503, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 199ms/step - loss: 0.3050 - accuracy: 0.8767 - val_loss: 0.2811 - val_accuracy: 0.9017\n",
      " Learning rate:  0.0001\n",
      "Epoch 37/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3046 - accuracy: 0.8775\n",
      "Epoch 00037: loss improved from 0.30503 to 0.30466, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 199ms/step - loss: 0.3047 - accuracy: 0.8775 - val_loss: 0.2780 - val_accuracy: 0.8986\n",
      " Learning rate:  0.0001\n",
      "Epoch 38/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.3048 - accuracy: 0.8777\n",
      "Epoch 00038: loss did not improve from 0.30466\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.3048 - accuracy: 0.8777 - val_loss: 0.2675 - val_accuracy: 0.9025\n",
      " Learning rate:  0.0001\n",
      "Epoch 39/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2976 - accuracy: 0.8819\n",
      "Epoch 00039: loss improved from 0.30466 to 0.29755, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.2976 - accuracy: 0.8819 - val_loss: 0.2801 - val_accuracy: 0.8957\n",
      " Learning rate:  0.0001\n",
      "Epoch 40/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2957 - accuracy: 0.8832\n",
      "Epoch 00040: loss improved from 0.29755 to 0.29569, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 201ms/step - loss: 0.2957 - accuracy: 0.8832 - val_loss: 0.2662 - val_accuracy: 0.9038\n",
      " Learning rate:  0.0001\n",
      "Epoch 41/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.8838\n",
      "Epoch 00041: loss improved from 0.29569 to 0.29339, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 130s 201ms/step - loss: 0.2934 - accuracy: 0.8838 - val_loss: 0.2741 - val_accuracy: 0.9033\n",
      " Learning rate:  0.0001\n",
      "Epoch 42/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.8839\n",
      "Epoch 00042: loss improved from 0.29339 to 0.29028, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 130s 202ms/step - loss: 0.2903 - accuracy: 0.8839 - val_loss: 0.2638 - val_accuracy: 0.9052\n",
      " Learning rate:  0.0001\n",
      "Epoch 43/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2877 - accuracy: 0.8850\n",
      "Epoch 00043: loss improved from 0.29028 to 0.28767, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.2877 - accuracy: 0.8850 - val_loss: 0.2633 - val_accuracy: 0.9019\n",
      " Learning rate:  0.0001\n",
      "Epoch 44/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2828 - accuracy: 0.8874\n",
      "Epoch 00044: loss improved from 0.28767 to 0.28275, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 0.2827 - accuracy: 0.8874 - val_loss: 0.2651 - val_accuracy: 0.9025\n",
      " Learning rate:  0.0001\n",
      "Epoch 45/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2818 - accuracy: 0.8874\n",
      "Epoch 00045: loss improved from 0.28275 to 0.28178, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.2818 - accuracy: 0.8874 - val_loss: 0.2681 - val_accuracy: 0.8988\n",
      " Learning rate:  0.0001\n",
      "Epoch 46/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2780 - accuracy: 0.8900\n",
      "Epoch 00046: loss improved from 0.28178 to 0.27800, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 0.2780 - accuracy: 0.8900 - val_loss: 0.2715 - val_accuracy: 0.9022\n",
      " Learning rate:  0.0001\n",
      "Epoch 47/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2758 - accuracy: 0.8906\n",
      "Epoch 00047: loss improved from 0.27800 to 0.27578, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.2758 - accuracy: 0.8907 - val_loss: 0.2653 - val_accuracy: 0.9054\n",
      " Learning rate:  0.0001\n",
      "Epoch 48/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2671 - accuracy: 0.8915\n",
      "Epoch 00048: loss improved from 0.27578 to 0.26714, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 130s 201ms/step - loss: 0.2671 - accuracy: 0.8915 - val_loss: 0.2519 - val_accuracy: 0.9071\n",
      " Learning rate:  0.0001\n",
      "Epoch 49/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2640 - accuracy: 0.8928\n",
      "Epoch 00049: loss improved from 0.26714 to 0.26399, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.2640 - accuracy: 0.8928 - val_loss: 0.2622 - val_accuracy: 0.9028\n",
      " Learning rate:  0.0001\n",
      "Epoch 50/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2606 - accuracy: 0.8951\n",
      "Epoch 00050: loss improved from 0.26399 to 0.26059, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.2606 - accuracy: 0.8951 - val_loss: 0.2631 - val_accuracy: 0.9021\n",
      " Learning rate:  0.0001\n",
      "Epoch 51/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2538 - accuracy: 0.8987\n",
      "Epoch 00051: loss improved from 0.26059 to 0.25378, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 0.2538 - accuracy: 0.8987 - val_loss: 0.2575 - val_accuracy: 0.9067\n",
      " Learning rate:  3e-06\n",
      "Epoch 52/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2426 - accuracy: 0.9036\n",
      "Epoch 00052: loss improved from 0.25378 to 0.24263, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.2426 - accuracy: 0.9036 - val_loss: 0.2579 - val_accuracy: 0.9063\n",
      " Learning rate:  3e-06\n",
      "Epoch 53/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2408 - accuracy: 0.9049\n",
      "Epoch 00053: loss improved from 0.24263 to 0.24075, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.2408 - accuracy: 0.9049 - val_loss: 0.2538 - val_accuracy: 0.9072\n",
      " Learning rate:  3e-06\n",
      "Epoch 54/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2387 - accuracy: 0.9054\n",
      "Epoch 00054: loss improved from 0.24075 to 0.23866, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.2387 - accuracy: 0.9054 - val_loss: 0.2535 - val_accuracy: 0.9069\n",
      " Learning rate:  3e-06\n",
      "Epoch 55/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2380 - accuracy: 0.9059\n",
      "Epoch 00055: loss improved from 0.23866 to 0.23799, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.2380 - accuracy: 0.9059 - val_loss: 0.2521 - val_accuracy: 0.9066\n",
      " Learning rate:  3e-06\n",
      "Epoch 56/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.9059\n",
      "Epoch 00056: loss did not improve from 0.23799\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2390 - accuracy: 0.9059 - val_loss: 0.2527 - val_accuracy: 0.9063\n",
      " Learning rate:  3e-06\n",
      "Epoch 57/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2359 - accuracy: 0.9070\n",
      "Epoch 00057: loss improved from 0.23799 to 0.23589, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 199ms/step - loss: 0.2359 - accuracy: 0.9070 - val_loss: 0.2499 - val_accuracy: 0.9075\n",
      " Learning rate:  3e-06\n",
      "Epoch 58/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2365 - accuracy: 0.9062\n",
      "Epoch 00058: loss did not improve from 0.23589\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2365 - accuracy: 0.9062 - val_loss: 0.2508 - val_accuracy: 0.9074\n",
      " Learning rate:  3e-06\n",
      "Epoch 59/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2354 - accuracy: 0.9065\n",
      "Epoch 00059: loss improved from 0.23589 to 0.23542, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.2354 - accuracy: 0.9065 - val_loss: 0.2508 - val_accuracy: 0.9073\n",
      " Learning rate:  3e-06\n",
      "Epoch 60/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2347 - accuracy: 0.9079\n",
      "Epoch 00060: loss improved from 0.23542 to 0.23467, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 129s 200ms/step - loss: 0.2347 - accuracy: 0.9079 - val_loss: 0.2478 - val_accuracy: 0.9077\n",
      " Learning rate:  3e-06\n",
      "Epoch 61/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2341 - accuracy: 0.9074\n",
      "Epoch 00061: loss improved from 0.23467 to 0.23413, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.2341 - accuracy: 0.9074 - val_loss: 0.2489 - val_accuracy: 0.9069\n",
      " Learning rate:  3e-06\n",
      "Epoch 62/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2335 - accuracy: 0.9075\n",
      "Epoch 00062: loss improved from 0.23413 to 0.23352, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.2335 - accuracy: 0.9075 - val_loss: 0.2511 - val_accuracy: 0.9066\n",
      " Learning rate:  3e-06\n",
      "Epoch 63/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2306 - accuracy: 0.9101\n",
      "Epoch 00063: loss improved from 0.23352 to 0.23058, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.2306 - accuracy: 0.9101 - val_loss: 0.2489 - val_accuracy: 0.9070\n",
      " Learning rate:  3e-06\n",
      "Epoch 64/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2324 - accuracy: 0.9079\n",
      "Epoch 00064: loss did not improve from 0.23058\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2324 - accuracy: 0.9079 - val_loss: 0.2495 - val_accuracy: 0.9069\n",
      " Learning rate:  3e-06\n",
      "Epoch 65/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2331 - accuracy: 0.9083\n",
      "Epoch 00065: loss did not improve from 0.23058\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2331 - accuracy: 0.9083 - val_loss: 0.2492 - val_accuracy: 0.9069\n",
      " Learning rate:  3e-06\n",
      "Epoch 66/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2300 - accuracy: 0.9088\n",
      "Epoch 00066: loss improved from 0.23058 to 0.22997, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.2300 - accuracy: 0.9088 - val_loss: 0.2494 - val_accuracy: 0.9068\n",
      " Learning rate:  3e-06\n",
      "Epoch 67/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2307 - accuracy: 0.9087\n",
      "Epoch 00067: loss did not improve from 0.22997\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2307 - accuracy: 0.9087 - val_loss: 0.2487 - val_accuracy: 0.9069\n",
      " Learning rate:  3e-06\n",
      "Epoch 68/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2310 - accuracy: 0.9094\n",
      "Epoch 00068: loss did not improve from 0.22997\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2310 - accuracy: 0.9094 - val_loss: 0.2457 - val_accuracy: 0.9082\n",
      " Learning rate:  3e-06\n",
      "Epoch 69/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2298 - accuracy: 0.9100\n",
      "Epoch 00069: loss improved from 0.22997 to 0.22982, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 130s 202ms/step - loss: 0.2298 - accuracy: 0.9100 - val_loss: 0.2474 - val_accuracy: 0.9076\n",
      " Learning rate:  3e-06\n",
      "Epoch 70/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2288 - accuracy: 0.9102\n",
      "Epoch 00070: loss improved from 0.22982 to 0.22879, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.2288 - accuracy: 0.9102 - val_loss: 0.2471 - val_accuracy: 0.9080\n",
      " Learning rate:  3e-06\n",
      "Epoch 71/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2283 - accuracy: 0.9111\n",
      "Epoch 00071: loss improved from 0.22879 to 0.22834, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 198ms/step - loss: 0.2283 - accuracy: 0.9111 - val_loss: 0.2494 - val_accuracy: 0.9070\n",
      " Learning rate:  3e-06\n",
      "Epoch 72/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2290 - accuracy: 0.9106\n",
      "Epoch 00072: loss did not improve from 0.22834\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2291 - accuracy: 0.9106 - val_loss: 0.2467 - val_accuracy: 0.9077\n",
      " Learning rate:  3e-06\n",
      "Epoch 73/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2285 - accuracy: 0.9106\n",
      "Epoch 00073: loss did not improve from 0.22834\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2285 - accuracy: 0.9106 - val_loss: 0.2472 - val_accuracy: 0.9078\n",
      " Learning rate:  3e-06\n",
      "Epoch 74/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2275 - accuracy: 0.9103\n",
      "Epoch 00074: loss improved from 0.22834 to 0.22754, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.2275 - accuracy: 0.9103 - val_loss: 0.2455 - val_accuracy: 0.9082\n",
      " Learning rate:  3e-06\n",
      "Epoch 75/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2266 - accuracy: 0.9112\n",
      "Epoch 00075: loss improved from 0.22754 to 0.22661, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.2266 - accuracy: 0.9112 - val_loss: 0.2480 - val_accuracy: 0.9080\n",
      " Learning rate:  3e-06\n",
      "Epoch 76/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2276 - accuracy: 0.9103\n",
      "Epoch 00076: loss did not improve from 0.22661\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2276 - accuracy: 0.9103 - val_loss: 0.2465 - val_accuracy: 0.9082\n",
      " Learning rate:  3e-06\n",
      "Epoch 77/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2260 - accuracy: 0.9118\n",
      "Epoch 00077: loss improved from 0.22661 to 0.22597, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 130s 201ms/step - loss: 0.2260 - accuracy: 0.9118 - val_loss: 0.2449 - val_accuracy: 0.9079\n",
      " Learning rate:  3e-06\n",
      "Epoch 78/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2281 - accuracy: 0.9106\n",
      "Epoch 00078: loss did not improve from 0.22597\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2281 - accuracy: 0.9106 - val_loss: 0.2488 - val_accuracy: 0.9076\n",
      " Learning rate:  3e-06\n",
      "Epoch 79/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2263 - accuracy: 0.9110\n",
      "Epoch 00079: loss did not improve from 0.22597\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2263 - accuracy: 0.9110 - val_loss: 0.2458 - val_accuracy: 0.9089\n",
      " Learning rate:  3e-06\n",
      "Epoch 80/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2239 - accuracy: 0.9115\n",
      "Epoch 00080: loss improved from 0.22597 to 0.22393, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 128s 199ms/step - loss: 0.2239 - accuracy: 0.9115 - val_loss: 0.2465 - val_accuracy: 0.9079\n",
      " Learning rate:  3e-06\n",
      "Epoch 81/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2278 - accuracy: 0.9112\n",
      "Epoch 00081: loss did not improve from 0.22393\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2277 - accuracy: 0.9112 - val_loss: 0.2464 - val_accuracy: 0.9080\n",
      " Learning rate:  1e-07\n",
      "Epoch 82/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2242 - accuracy: 0.9126\n",
      "Epoch 00082: loss did not improve from 0.22393\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2242 - accuracy: 0.9126 - val_loss: 0.2462 - val_accuracy: 0.9081\n",
      " Learning rate:  1e-07\n",
      "Epoch 83/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2243 - accuracy: 0.9130\n",
      "Epoch 00083: loss did not improve from 0.22393\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2243 - accuracy: 0.9130 - val_loss: 0.2464 - val_accuracy: 0.9077\n",
      " Learning rate:  1e-07\n",
      "Epoch 84/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2263 - accuracy: 0.9108\n",
      "Epoch 00084: loss did not improve from 0.22393\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2263 - accuracy: 0.9108 - val_loss: 0.2460 - val_accuracy: 0.9081\n",
      " Learning rate:  1e-07\n",
      "Epoch 85/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2272 - accuracy: 0.9116\n",
      "Epoch 00085: loss did not improve from 0.22393\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2272 - accuracy: 0.9116 - val_loss: 0.2464 - val_accuracy: 0.9080\n",
      " Learning rate:  1e-07\n",
      "Epoch 86/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2262 - accuracy: 0.9111\n",
      "Epoch 00086: loss did not improve from 0.22393\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2262 - accuracy: 0.9111 - val_loss: 0.2463 - val_accuracy: 0.9081\n",
      " Learning rate:  1e-07\n",
      "Epoch 87/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2247 - accuracy: 0.9120\n",
      "Epoch 00087: loss did not improve from 0.22393\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2247 - accuracy: 0.9120 - val_loss: 0.2464 - val_accuracy: 0.9079\n",
      " Learning rate:  1e-07\n",
      "Epoch 88/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.9114\n",
      "Epoch 00088: loss did not improve from 0.22393\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2248 - accuracy: 0.9114 - val_loss: 0.2465 - val_accuracy: 0.9078\n",
      " Learning rate:  1e-07\n",
      "Epoch 89/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2240 - accuracy: 0.9117\n",
      "Epoch 00089: loss did not improve from 0.22393\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2240 - accuracy: 0.9117 - val_loss: 0.2463 - val_accuracy: 0.9083\n",
      " Learning rate:  1e-07\n",
      "Epoch 90/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2269 - accuracy: 0.9108\n",
      "Epoch 00090: loss did not improve from 0.22393\n",
      "645/645 [==============================] - 120s 187ms/step - loss: 0.2269 - accuracy: 0.9108 - val_loss: 0.2462 - val_accuracy: 0.9081\n",
      " Learning rate:  1e-07\n",
      "Epoch 91/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2269 - accuracy: 0.9111\n",
      "Epoch 00091: loss did not improve from 0.22393\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2269 - accuracy: 0.9111 - val_loss: 0.2462 - val_accuracy: 0.9080\n",
      " Learning rate:  1e-07\n",
      "Epoch 92/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2253 - accuracy: 0.9110\n",
      "Epoch 00092: loss did not improve from 0.22393\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2253 - accuracy: 0.9110 - val_loss: 0.2462 - val_accuracy: 0.9084\n",
      " Learning rate:  1e-07\n",
      "Epoch 93/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2238 - accuracy: 0.9120\n",
      "Epoch 00093: loss improved from 0.22393 to 0.22385, saving model to /mnt/vdb/thesis/ProtCNN.V5.h5\n",
      "645/645 [==============================] - 130s 201ms/step - loss: 0.2239 - accuracy: 0.9120 - val_loss: 0.2460 - val_accuracy: 0.9084\n",
      " Learning rate:  1e-07\n",
      "Epoch 94/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9116\n",
      "Epoch 00094: loss did not improve from 0.22385\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2259 - accuracy: 0.9116 - val_loss: 0.2460 - val_accuracy: 0.9083\n",
      " Learning rate:  1e-07\n",
      "Epoch 95/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2252 - accuracy: 0.9123\n",
      "Epoch 00095: loss did not improve from 0.22385\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2252 - accuracy: 0.9123 - val_loss: 0.2464 - val_accuracy: 0.9081\n",
      " Learning rate:  1e-07\n",
      "Epoch 96/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2245 - accuracy: 0.9122\n",
      "Epoch 00096: loss did not improve from 0.22385\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2245 - accuracy: 0.9122 - val_loss: 0.2462 - val_accuracy: 0.9084\n",
      " Learning rate:  1e-07\n",
      "Epoch 97/100\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2253 - accuracy: 0.9111\n",
      "Epoch 00097: loss did not improve from 0.22385\n",
      "645/645 [==============================] - 121s 187ms/step - loss: 0.2253 - accuracy: 0.9111 - val_loss: 0.2462 - val_accuracy: 0.9084\n",
      "Epoch 00097: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Early Stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "# val_loss\n",
    "# ProtCNN_256X2_NN1221X4.V2.h5\n",
    "checkpoint = ModelCheckpoint(\"/mnt/vdb/thesis/ProtCNN.V5.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "# val_accuracy\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "history2 = model_ProtCNN.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100, batch_size=128,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[checkpoint, es,lr_scheduler]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ProtCNN.save(\"/mnt/vdb/thesis/ProtCNN.V5.h5\")\n",
    "#model_ProtCNN= keras.models.load_model(\"/mnt/vdb/ProtCNN.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91     13856\n",
      "           1       0.92      0.88      0.90     13622\n",
      "\n",
      "    accuracy                           0.90     27478\n",
      "   macro avg       0.91      0.90      0.90     27478\n",
      "weighted avg       0.91      0.90      0.90     27478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_probas = model_ProtCNN.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_predict = np.where(y_probas > threshold, 1, 0)\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "kZO0R1iHSm4T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95     25564\n",
      "           1       0.94      0.95      0.95     25244\n",
      "\n",
      "    accuracy                           0.95     50808\n",
      "   macro avg       0.95      0.95      0.95     50808\n",
      "weighted avg       0.95      0.95      0.95     50808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_probas = model_ProtCNN.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_predict = np.where(y_probas > threshold, 1, 0)\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pMIGguFR4EHD"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_3iiV1sM4D6d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_ProtCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6ffcb3a07e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m display_model_score(model_ProtCNN,\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     [X_test, y_test])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_ProtCNN' is not defined"
     ]
    }
   ],
   "source": [
    "display_model_score(model_ProtCNN,\n",
    "    [X_train, y_train],\n",
    "    [X_val, y_val],\n",
    "    [X_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with PWM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = Input(shape=(1,20,window_sizes))\n",
    "\n",
    "#initial conv\n",
    "conv = Conv1D(128, 1, padding='same')(x_input) \n",
    "\n",
    "# per-residue representation\n",
    "res1 = residual_block(conv, 128, 2)\n",
    "res2 = residual_block(res1, 128, 3)\n",
    "\n",
    "x = MaxPooling1D(3)(res2)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# softmax classifier\n",
    "x = Flatten()(x)\n",
    "x_output = Dense(1000, activation='sigmoid', kernel_regularizer=l2(0.0001))(x)\n",
    "#x_output = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0001))(x)\n",
    "\n",
    "model_ProtCNN = Model(inputs=x_input, outputs=x_output)\n",
    "model_ProtCNN.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_ProtCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fALQEI9g4Ejn"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Gh0Zvl9j4E38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1900, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 1900, 256)    512         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1900, 256)    1024        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1900, 256)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1900, 256)    65792       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1900, 256)    1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1900, 256)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1900, 256)    196864      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 1900, 256)    0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1900, 256)    1024        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1900, 256)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1900, 256)    65792       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1900, 256)    1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1900, 256)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1900, 256)    196864      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1900, 256)    0           conv1d_4[0][0]                   \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 633, 256)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 633, 256)     0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 162048)       0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1000)         162049000   flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            1001        dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 162,579,921\n",
      "Trainable params: 162,577,873\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "with tf.device('/cpu:0'):\n",
    "        ProtCNN= load_model(\"/mnt/vdb/ProtCNN.bestmodel.h5\")\n",
    "ProtCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CustomDNNModel.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
