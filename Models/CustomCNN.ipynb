{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48641189/fitting-3d-data-as-input-into-keras-sequential-model-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qhoptim\n",
      "  Downloading qhoptim-1.1.0-py3-none-any.whl (20 kB)\n",
      "\u001b[33mWARNING: Error parsing requirements for matplotlib: [Errno 2] No such file or directory: '/home/ubuntu/miniconda3/envs/py3/lib/python3.7/site-packages/matplotlib-3.3.2.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: qhoptim\n",
      "Successfully installed qhoptim-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install qhoptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ubuntu/miniconda3/envs/py3\n",
      "\n",
      "  added / updated specs:\n",
      "    - keras=2.3.1\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2020.12.5          |   py37h06a4308_0         141 KB\n",
      "    keras-2.3.1                |                0          12 KB\n",
      "    keras-base-2.3.1           |           py37_0         495 KB\n",
      "    openssl-1.1.1i             |       h27cfd23_0         2.5 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  keras-base         pkgs/main/linux-64::keras-base-2.3.1-py37_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2020.12.~ --> pkgs/main::ca-certificates-2021.1.19-h06a4308_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge::certifi-2020.12.5-py37h8~ --> pkgs/main::certifi-2020.12.5-py37h06a4308_0\n",
      "  keras                conda-forge/noarch::keras-2.4.3-py_0 --> pkgs/main/linux-64::keras-2.3.1-0\n",
      "  openssl            conda-forge::openssl-1.1.1i-h7f98852_0 --> pkgs/main::openssl-1.1.1i-h27cfd23_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.1.1i       | 2.5 MB    | ##################################### | 100% \n",
      "keras-base-2.3.1     | 495 KB    | ##################################### | 100% \n",
      "keras-2.3.1          | 12 KB     | ##################################### | 100% \n",
      "certifi-2020.12.5    | 141 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install keras=2.3.1 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Use only CPU\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15360368458749329693\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16134426514430374056\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11756146409991432083\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 31595870336\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14256696921772055789\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:00:06.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4aBzk8QXHS9S"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "\n",
    "#keras\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv1D, Add, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GlobalMaxPooling1D\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# CuDNNLSTM error; The error was because from TensorFlow 2 you do not need to specify CuDNNLSTM. \n",
    "# You can just use LSTM with no activation function and it will automatically use the CuDNN version. You do have to install CuDNN first.\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# \n",
    "from qhoptim.tf import QHAdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Optimizer\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "class QHAdam(Optimizer):\n",
    "    \"\"\"QH-Adam optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta_1 < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta_2 < 1. Generally close to 1.\n",
    "        neu_1: float, 0 < neu_1 < 1. Default based on paper equals 0.7\n",
    "        neu_2: float, 0 < neu_2 < 1. Default based on paper equals 1\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "    # References\n",
    "        - [QUASI-HYPERBOLIC MOMENTUM AND ADAM FOR DEEP LEARNING](\n",
    "           https://openreview.net/pdf?id=S1fUpoR5FQ)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 lr=0.001,\n",
    "                 beta_1=0.999,\n",
    "                 beta_2=0.999,\n",
    "                 neu_1 = 0.7,\n",
    "                 neu_2 = 1.,\n",
    "                 epsilon=1e-3,\n",
    "                 decay=0.,\n",
    "                 amsgrad=False,\n",
    "                 **kwargs):\n",
    "        super(QHAdam, self).__init__(name=\"QHAdam\",**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.learning_rate = K.variable(lr, name='lr')\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.neu_1 = K.variable(neu_1, name='neu_1')\n",
    "            self.neu_2 = K.variable(neu_2, name='neu_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.learning_rate\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            m_t_adj = m_t/(1. - K.pow(self.beta_1, t))\n",
    "            v_t_adj = v_t/(1. - K.pow(self.beta_2, t))\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - lr * ((1.- self.neu_1)*g + self.neu_1*(m_t_adj)) / \\\n",
    "                       (K.sqrt((1.-self.neu_2) * K.square(g) + self.neu_2 * vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                p_t = p - lr * ((1.-self.neu_1)*g + self.neu_1*(m_t_adj)) / \\\n",
    "                       (K.sqrt((1.-self.neu_2)*K.square(g) + self.neu_2 * v_t_adj) + self.epsilon)\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.learning_rate)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'neu_1': float(K.get_value(self.neu_1)),\n",
    "                  'neu_2': float(K.get_value(self.neu_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad}\n",
    "        base_config = super(QHAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FNhtXHJrfEE4"
   },
   "outputs": [],
   "source": [
    "# small old ../datasets/AMPsNonAMPs_df.239.plk\n",
    "# /home/ubuntu/data/AMPsNonAMPs_df.plk old dataset\n",
    "# /mnt/vdb/thesis/jax/AMPNonAMP.final.reps new dataset\n",
    "import pickle5 as pickle\n",
    "with open( \"/mnt/vdb/thesis/jax/AMPNonAMP.final.reps\", 'rb') as file:\n",
    "    AMPs_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "mGiMSzzPfj2t",
    "outputId": "092eb611-b89c-4cac-c8c4-0557799510cf"
   },
   "outputs": [],
   "source": [
    "#AMPs_df.drop_duplicates(subset=['Sequence'],inplace=True)\n",
    "AMPs_df =AMPs_df[AMPs_df[\"length\"] <=30 ]\n",
    "AMPs_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2i7Tk41aDZ5"
   },
   "source": [
    "### Utility function: plot_history, display_model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7F7ykQsDVxHO"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  # dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  x = range(1, len(acc) + 1)\n",
    "\n",
    "  plt.figure(figsize=(12, 5))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(x, acc, 'b', label='Training acc')\n",
    "  plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(x, loss, 'b', label='Training loss')\n",
    "  plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.legend()\n",
    "\n",
    "# Display model score(Loss & Accuracy) across all sets.\n",
    "def display_model_score(model, train, val, test):\n",
    "  train_score = model.evaluate(train[0], train[1], verbose=1)\n",
    "  print('Train loss: ', train_score[0])\n",
    "  print('Train accuracy: ', train_score[1])\n",
    "  print('-'*70)\n",
    "  val_score = model.evaluate(val[0], val[1], verbose=1)\n",
    "  print('Val loss: ', val_score[0])\n",
    "  print('Val accuracy: ', val_score[1])\n",
    "  print('-'*70)\n",
    "  test_score = model.evaluate(test[0], test[1], verbose=1)\n",
    "  print('Test loss: ', test_score[0])\n",
    "  print('Test accuracy: ', test_score[1])\n",
    "\n",
    "def plot_history_CV(cv, estimator,x,y):\n",
    "  # plot arrows\n",
    "  fig1 = plt.figure(figsize=[12,12])\n",
    "  ax1 = fig1.add_subplot(111,aspect = 'equal')\n",
    "  ax1.add_patch(\n",
    "      patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
    "      )\n",
    "  ax1.add_patch(\n",
    "      patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
    "      )\n",
    "\n",
    "  tprs = []\n",
    "  aucs = []\n",
    "  mean_fpr = np.linspace(0,1,100)\n",
    "  i = 1\n",
    "  for train,test in cv.split(x,y):\n",
    "      model = create_Modelbaseline()\n",
    "      model.fit(x[train],y.iloc[train],\n",
    "            epochs=30,\n",
    "            shuffle=True,verbose=0)\n",
    "      prediction = model.predict(x[test])\n",
    "      fpr, tpr, t = roc_curve(y[test], prediction[:, 1])\n",
    "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "      roc_auc = auc(fpr, tpr)\n",
    "      aucs.append(roc_auc)\n",
    "      plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "      i= i+1\n",
    "\n",
    "  plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "  mean_tpr = np.mean(tprs, axis=0)\n",
    "  mean_auc = auc(mean_fpr, mean_tpr)\n",
    "  plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "          label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.title('ROC')\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "  plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI-_ZAvfIb5A"
   },
   "source": [
    "# Split Train/ Test / Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lAAQLx4UIptD"
   },
   "outputs": [],
   "source": [
    "X= np.array(AMPs_df['reps'].to_list())\n",
    "y= np.array(AMPs_df['class'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EkmqGqfUT0XR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88692, 1900)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape  = X.shape\n",
    "input_shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UWQ2IZWgIbST"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "IM7Scdevkwpp",
    "outputId": "55fa7ea6-a5f9-4e23-bf42-f1a4bf64f8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  53214\n",
      "Val size:  17739\n",
      "Test size:  17739\n"
     ]
    }
   ],
   "source": [
    "# Given data size\n",
    "print('Train size: ', len(X_train))\n",
    "print('Val size: ', len(X_val))\n",
    "print('Test size: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53214, 1900, 1)\n",
      "(17739, 1900, 1)\n",
      "(17739, 1900, 1)\n"
     ]
    }
   ],
   "source": [
    "# 3d dimension for LSTM\n",
    "# Batchs, n_timesteps, n_features\n",
    "\n",
    "# Images 3d dimension\n",
    "# width , heigth , channel\n",
    "\n",
    "# Conv1D with sequential data\n",
    "# batch, steps, channels\n",
    "\n",
    "# https://stackoverflow.com/questions/52803989/how-to-correct-shape-of-keras-input-into-a-3d-array/52804200\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
    "print(X_train.shape)\n",
    "X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\n",
    "print(X_test.shape)\n",
    "X_val = np.reshape(X_val,(X_val.shape[0],X_val.shape[1],1))\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_Modelbaseline():\n",
    "    x_input = Input(shape=(1900,1)) # n_timesteps, n_features\n",
    "    # Conv\n",
    "    conv = Conv1D(512, kernel_size=7, strides=1, padding='same', activation='relu')(x_input) \n",
    "    conv = MaxPooling1D(pool_size=3)(conv)\n",
    "    conv = Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu')(conv) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "\n",
    "    # Flatten NN\n",
    "    flat = Flatten()(conv)\n",
    "    \n",
    "    layer_3 = Dense(1211, activation='relu')(flat)\n",
    "    dropout_3 = Dropout(0.2)(layer_3)\n",
    "    layer_4 = Dense(1211, activation='relu')(dropout_3)\n",
    "    dropout_4 = Dropout(0.2)(layer_4)\n",
    "    x_output = Dense(1, activation='sigmoid', name='output_layer', kernel_regularizer=l2(0.0001))(dropout_4)\n",
    "\n",
    "    model = Model(inputs=x_input, outputs=x_output)\n",
    "    model.compile(optimizer=\"RMSprop\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1900, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 1900, 512)         4608      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 633, 512)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 633, 256)          524544    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 316, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 80896)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1211)              97966267  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1211)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1211)              1467732   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1211)              0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 1)                 1212      \n",
      "=================================================================\n",
      "Total params: 99,964,363\n",
      "Trainable params: 99,964,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# n_timesteps, n_features, n_outputs = X_train.shape[2], X_train.shape[1], 1\n",
    "# nn.Conv1d with a kernel size of 1 and nn.Linear   give exactly the same results.\n",
    "\n",
    "\n",
    "#SGD\n",
    "#RMSprop\n",
    "#Adam\n",
    "#Adadelta\n",
    "#Adagrad\n",
    "#Adamax\n",
    "#Nadam\n",
    "#Ftrl\n",
    "#\n",
    "#\n",
    "\n",
    "def create_Modelbaseline():\n",
    "    x_input = Input(shape=(1900,1)) # n_timesteps, n_features\n",
    "    # Conv\n",
    "    #conv = Conv1D(512, kernel_size=7, strides=1, padding='same', activation='relu')(x_input) \n",
    "    #conv = MaxPooling1D(pool_size=3)(conv)\n",
    "    conv = Conv1D(512, kernel_size=8, strides=1, padding='same', activation='relu')(x_input) \n",
    "    conv = MaxPooling1D(pool_size=3)(conv)\n",
    "    conv = Conv1D(256, kernel_size=4, strides=1, padding='same', activation='relu')(conv) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "   \n",
    "    # Flatten NN\n",
    "    flat = Flatten()(conv)\n",
    "    layer_3 = Dense(1211, activation='relu')(flat)\n",
    "    dropout_3 = Dropout(0.2)(layer_3)\n",
    "    layer_4 = Dense(1211, activation='relu')(dropout_3)\n",
    "    dropout_4 = Dropout(0.2)(layer_4)\n",
    "    x_output = Dense(1, activation='sigmoid', name='output_layer', kernel_regularizer=l2(0.0001))(dropout_4)\n",
    "\n",
    "    model = Model(inputs=x_input, outputs=x_output)\n",
    "    model.compile(optimizer=\"Adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_Modelbaseline()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import backend as K\n",
    "#K.set_value(model_ProtCNN.optimizer.learning_rate, 0.00001)\n",
    "def lr_schedule(epoch):\n",
    "    \n",
    "    lr = 1e-3\n",
    "    if epoch > 80:\n",
    "        lr = 0.1e-5\n",
    "    elif epoch > 50:    \n",
    "        lr = 0.3e-5\n",
    "    elif epoch > 20:\n",
    "        lr = 1e-4\n",
    "        \n",
    "    print(' Learning rate: ', lr)    \n",
    "    return lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      " Learning rate:  0.001\n",
      "Epoch 1/100\n",
      "  1/832 [..............................] - ETA: 0s - loss: 0.6938 - accuracy: 0.5000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0125s vs `on_train_batch_end` time: 0.0208s). Check your callbacks.\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.3590 - accuracy: 0.8367\n",
      "Epoch 00001: loss improved from inf to 0.35900, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 35s 42ms/step - loss: 0.3590 - accuracy: 0.8367 - val_loss: 0.3621 - val_accuracy: 0.8406\n",
      " Learning rate:  0.001\n",
      "Epoch 2/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.2812 - accuracy: 0.8838\n",
      "Epoch 00002: loss improved from 0.35900 to 0.28111, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.2811 - accuracy: 0.8839 - val_loss: 0.2770 - val_accuracy: 0.8858\n",
      " Learning rate:  0.001\n",
      "Epoch 3/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.2554 - accuracy: 0.8940\n",
      "Epoch 00003: loss improved from 0.28111 to 0.25544, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.2554 - accuracy: 0.8940 - val_loss: 0.2815 - val_accuracy: 0.8844\n",
      " Learning rate:  0.001\n",
      "Epoch 4/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.2388 - accuracy: 0.9032\n",
      "Epoch 00004: loss improved from 0.25544 to 0.23878, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.2388 - accuracy: 0.9033 - val_loss: 0.2596 - val_accuracy: 0.8928\n",
      " Learning rate:  0.001\n",
      "Epoch 5/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.2251 - accuracy: 0.9083\n",
      "Epoch 00005: loss improved from 0.23878 to 0.22506, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.2251 - accuracy: 0.9083 - val_loss: 0.2460 - val_accuracy: 0.8973\n",
      " Learning rate:  0.001\n",
      "Epoch 6/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.2087 - accuracy: 0.9152\n",
      "Epoch 00006: loss improved from 0.22506 to 0.20873, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.2087 - accuracy: 0.9152 - val_loss: 0.2414 - val_accuracy: 0.9003\n",
      " Learning rate:  0.001\n",
      "Epoch 7/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.1976 - accuracy: 0.9197\n",
      "Epoch 00007: loss improved from 0.20873 to 0.19772, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 33s 40ms/step - loss: 0.1977 - accuracy: 0.9196 - val_loss: 0.2421 - val_accuracy: 0.9046\n",
      " Learning rate:  0.001\n",
      "Epoch 8/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.1869 - accuracy: 0.9238\n",
      "Epoch 00008: loss improved from 0.19772 to 0.18686, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.1869 - accuracy: 0.9238 - val_loss: 0.2464 - val_accuracy: 0.9042\n",
      " Learning rate:  0.001\n",
      "Epoch 9/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.1767 - accuracy: 0.9285\n",
      "Epoch 00009: loss improved from 0.18686 to 0.17679, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.1768 - accuracy: 0.9285 - val_loss: 0.2422 - val_accuracy: 0.9063\n",
      " Learning rate:  0.001\n",
      "Epoch 10/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9329\n",
      "Epoch 00010: loss improved from 0.17679 to 0.16717, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.1672 - accuracy: 0.9329 - val_loss: 0.2431 - val_accuracy: 0.9069\n",
      " Learning rate:  0.001\n",
      "Epoch 11/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.1591 - accuracy: 0.9357\n",
      "Epoch 00011: loss improved from 0.16717 to 0.15910, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 33s 40ms/step - loss: 0.1591 - accuracy: 0.9357 - val_loss: 0.2283 - val_accuracy: 0.9109\n",
      " Learning rate:  0.001\n",
      "Epoch 12/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.1506 - accuracy: 0.9391\n",
      "Epoch 00012: loss improved from 0.15910 to 0.15059, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.1506 - accuracy: 0.9391 - val_loss: 0.2424 - val_accuracy: 0.9071\n",
      " Learning rate:  0.001\n",
      "Epoch 13/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.1418 - accuracy: 0.9419\n",
      "Epoch 00013: loss improved from 0.15059 to 0.14171, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.1417 - accuracy: 0.9419 - val_loss: 0.2254 - val_accuracy: 0.9165\n",
      " Learning rate:  0.001\n",
      "Epoch 14/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.1332 - accuracy: 0.9455\n",
      "Epoch 00014: loss improved from 0.14171 to 0.13319, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.1332 - accuracy: 0.9455 - val_loss: 0.2645 - val_accuracy: 0.9116\n",
      " Learning rate:  0.001\n",
      "Epoch 15/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9503\n",
      "Epoch 00015: loss improved from 0.13319 to 0.12447, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 33s 40ms/step - loss: 0.1245 - accuracy: 0.9503 - val_loss: 0.2716 - val_accuracy: 0.9131\n",
      " Learning rate:  0.001\n",
      "Epoch 16/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9508\n",
      "Epoch 00016: loss improved from 0.12447 to 0.12110, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.1211 - accuracy: 0.9508 - val_loss: 0.2642 - val_accuracy: 0.9136\n",
      " Learning rate:  0.001\n",
      "Epoch 17/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9562\n",
      "Epoch 00017: loss improved from 0.12110 to 0.11060, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.1106 - accuracy: 0.9562 - val_loss: 0.2823 - val_accuracy: 0.9138\n",
      " Learning rate:  0.001\n",
      "Epoch 18/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.1042 - accuracy: 0.9586\n",
      "Epoch 00018: loss improved from 0.11060 to 0.10415, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.1042 - accuracy: 0.9586 - val_loss: 0.2755 - val_accuracy: 0.9125\n",
      " Learning rate:  0.001\n",
      "Epoch 19/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0995 - accuracy: 0.9594\n",
      "Epoch 00019: loss improved from 0.10415 to 0.09943, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.0994 - accuracy: 0.9594 - val_loss: 0.2731 - val_accuracy: 0.9147\n",
      " Learning rate:  0.001\n",
      "Epoch 20/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0939 - accuracy: 0.9624\n",
      "Epoch 00020: loss improved from 0.09943 to 0.09391, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0939 - accuracy: 0.9624 - val_loss: 0.3032 - val_accuracy: 0.9134\n",
      " Learning rate:  0.001\n",
      "Epoch 21/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0900 - accuracy: 0.9641\n",
      "Epoch 00021: loss improved from 0.09391 to 0.08995, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0900 - accuracy: 0.9641 - val_loss: 0.3174 - val_accuracy: 0.9162\n",
      " Learning rate:  0.0001\n",
      "Epoch 22/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0581 - accuracy: 0.9775\n",
      "Epoch 00022: loss improved from 0.08995 to 0.05814, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.0581 - accuracy: 0.9774 - val_loss: 0.3223 - val_accuracy: 0.9180\n",
      " Learning rate:  0.0001\n",
      "Epoch 23/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0496 - accuracy: 0.9812\n",
      "Epoch 00023: loss improved from 0.05814 to 0.04957, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0496 - accuracy: 0.9812 - val_loss: 0.3453 - val_accuracy: 0.9181\n",
      " Learning rate:  0.0001\n",
      "Epoch 24/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0447 - accuracy: 0.9823\n",
      "Epoch 00024: loss improved from 0.04957 to 0.04465, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.0446 - accuracy: 0.9823 - val_loss: 0.3681 - val_accuracy: 0.9201\n",
      " Learning rate:  0.0001\n",
      "Epoch 25/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0437 - accuracy: 0.9835\n",
      "Epoch 00025: loss improved from 0.04465 to 0.04367, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 33s 39ms/step - loss: 0.0437 - accuracy: 0.9834 - val_loss: 0.3898 - val_accuracy: 0.9188\n",
      " Learning rate:  0.0001\n",
      "Epoch 26/100\n",
      "832/832 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9848\n",
      "Epoch 00026: loss improved from 0.04367 to 0.03995, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.0399 - accuracy: 0.9848 - val_loss: 0.3957 - val_accuracy: 0.9182\n",
      " Learning rate:  0.0001\n",
      "Epoch 27/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0396 - accuracy: 0.9852\n",
      "Epoch 00027: loss improved from 0.03995 to 0.03962, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 33s 40ms/step - loss: 0.0396 - accuracy: 0.9852 - val_loss: 0.4156 - val_accuracy: 0.9181\n",
      " Learning rate:  0.0001\n",
      "Epoch 28/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0358 - accuracy: 0.9865\n",
      "Epoch 00028: loss improved from 0.03962 to 0.03580, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0358 - accuracy: 0.9865 - val_loss: 0.4309 - val_accuracy: 0.9168\n",
      " Learning rate:  0.0001\n",
      "Epoch 29/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0350 - accuracy: 0.9873\n",
      "Epoch 00029: loss improved from 0.03580 to 0.03497, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0350 - accuracy: 0.9872 - val_loss: 0.4571 - val_accuracy: 0.9180\n",
      " Learning rate:  0.0001\n",
      "Epoch 30/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0344 - accuracy: 0.9875\n",
      "Epoch 00030: loss improved from 0.03497 to 0.03440, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0344 - accuracy: 0.9875 - val_loss: 0.4496 - val_accuracy: 0.9191\n",
      " Learning rate:  0.0001\n",
      "Epoch 31/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0314 - accuracy: 0.9885\n",
      "Epoch 00031: loss improved from 0.03440 to 0.03151, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0315 - accuracy: 0.9884 - val_loss: 0.4826 - val_accuracy: 0.9194\n",
      " Learning rate:  0.0001\n",
      "Epoch 32/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0303 - accuracy: 0.9888\n",
      "Epoch 00032: loss improved from 0.03151 to 0.03032, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0303 - accuracy: 0.9888 - val_loss: 0.4786 - val_accuracy: 0.9192\n",
      " Learning rate:  0.0001\n",
      "Epoch 33/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0308 - accuracy: 0.9889\n",
      "Epoch 00033: loss did not improve from 0.03032\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0308 - accuracy: 0.9889 - val_loss: 0.5002 - val_accuracy: 0.9197\n",
      " Learning rate:  0.0001\n",
      "Epoch 34/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0292 - accuracy: 0.9891\n",
      "Epoch 00034: loss improved from 0.03032 to 0.02917, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0292 - accuracy: 0.9891 - val_loss: 0.4931 - val_accuracy: 0.9180\n",
      " Learning rate:  0.0001\n",
      "Epoch 35/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0285 - accuracy: 0.9892\n",
      "Epoch 00035: loss improved from 0.02917 to 0.02860, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.0286 - accuracy: 0.9891 - val_loss: 0.5155 - val_accuracy: 0.9194\n",
      " Learning rate:  0.0001\n",
      "Epoch 36/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0274 - accuracy: 0.9900\n",
      "Epoch 00036: loss improved from 0.02860 to 0.02744, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.0274 - accuracy: 0.9900 - val_loss: 0.5394 - val_accuracy: 0.9163\n",
      " Learning rate:  0.0001\n",
      "Epoch 37/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9908\n",
      "Epoch 00037: loss improved from 0.02744 to 0.02613, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0261 - accuracy: 0.9908 - val_loss: 0.5543 - val_accuracy: 0.9188\n",
      " Learning rate:  0.0001\n",
      "Epoch 38/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0251 - accuracy: 0.9910\n",
      "Epoch 00038: loss improved from 0.02613 to 0.02514, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.0251 - accuracy: 0.9910 - val_loss: 0.5579 - val_accuracy: 0.9177\n",
      " Learning rate:  0.0001\n",
      "Epoch 39/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0241 - accuracy: 0.9918\n",
      "Epoch 00039: loss improved from 0.02514 to 0.02411, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0241 - accuracy: 0.9918 - val_loss: 0.5714 - val_accuracy: 0.9193\n",
      " Learning rate:  0.0001\n",
      "Epoch 40/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0236 - accuracy: 0.9912\n",
      "Epoch 00040: loss improved from 0.02411 to 0.02356, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0236 - accuracy: 0.9912 - val_loss: 0.5864 - val_accuracy: 0.9185\n",
      " Learning rate:  0.0001\n",
      "Epoch 41/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0237 - accuracy: 0.9912\n",
      "Epoch 00041: loss did not improve from 0.02356\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0237 - accuracy: 0.9912 - val_loss: 0.5897 - val_accuracy: 0.9169\n",
      " Learning rate:  0.0001\n",
      "Epoch 42/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0229 - accuracy: 0.9915\n",
      "Epoch 00042: loss improved from 0.02356 to 0.02286, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0229 - accuracy: 0.9915 - val_loss: 0.5912 - val_accuracy: 0.9171\n",
      " Learning rate:  0.0001\n",
      "Epoch 43/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9923\n",
      "Epoch 00043: loss improved from 0.02286 to 0.02216, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 33s 40ms/step - loss: 0.0222 - accuracy: 0.9923 - val_loss: 0.5922 - val_accuracy: 0.9183\n",
      " Learning rate:  0.0001\n",
      "Epoch 44/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0214 - accuracy: 0.9928\n",
      "Epoch 00044: loss improved from 0.02216 to 0.02135, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0214 - accuracy: 0.9928 - val_loss: 0.6100 - val_accuracy: 0.9158\n",
      " Learning rate:  0.0001\n",
      "Epoch 45/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0226 - accuracy: 0.9924\n",
      "Epoch 00045: loss did not improve from 0.02135\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0226 - accuracy: 0.9924 - val_loss: 0.5844 - val_accuracy: 0.9175\n",
      " Learning rate:  0.0001\n",
      "Epoch 46/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9927\n",
      "Epoch 00046: loss improved from 0.02135 to 0.02005, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.0201 - accuracy: 0.9927 - val_loss: 0.6184 - val_accuracy: 0.9193\n",
      " Learning rate:  0.0001\n",
      "Epoch 47/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0196 - accuracy: 0.9930\n",
      "Epoch 00047: loss improved from 0.02005 to 0.01963, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 33s 40ms/step - loss: 0.0196 - accuracy: 0.9930 - val_loss: 0.6247 - val_accuracy: 0.9182\n",
      " Learning rate:  0.0001\n",
      "Epoch 48/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0189 - accuracy: 0.9932\n",
      "Epoch 00048: loss improved from 0.01963 to 0.01886, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.0189 - accuracy: 0.9932 - val_loss: 0.6314 - val_accuracy: 0.9185\n",
      " Learning rate:  0.0001\n",
      "Epoch 49/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9934\n",
      "Epoch 00049: loss did not improve from 0.01886\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0195 - accuracy: 0.9934 - val_loss: 0.6283 - val_accuracy: 0.9172\n",
      " Learning rate:  0.0001\n",
      "Epoch 50/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9932\n",
      "Epoch 00050: loss did not improve from 0.01886\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0197 - accuracy: 0.9932 - val_loss: 0.6447 - val_accuracy: 0.9179\n",
      " Learning rate:  0.0001\n",
      "Epoch 51/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0186 - accuracy: 0.9939\n",
      "Epoch 00051: loss improved from 0.01886 to 0.01858, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.6374 - val_accuracy: 0.9177\n",
      " Learning rate:  3e-06\n",
      "Epoch 52/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9947\n",
      "Epoch 00052: loss improved from 0.01858 to 0.01702, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0170 - accuracy: 0.9947 - val_loss: 0.6452 - val_accuracy: 0.9180\n",
      " Learning rate:  3e-06\n",
      "Epoch 53/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9946\n",
      "Epoch 00053: loss improved from 0.01702 to 0.01692, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.0169 - accuracy: 0.9946 - val_loss: 0.6499 - val_accuracy: 0.9179\n",
      " Learning rate:  3e-06\n",
      "Epoch 54/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0168 - accuracy: 0.9942\n",
      "Epoch 00054: loss improved from 0.01692 to 0.01679, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0168 - accuracy: 0.9942 - val_loss: 0.6535 - val_accuracy: 0.9181\n",
      " Learning rate:  3e-06\n",
      "Epoch 55/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0173 - accuracy: 0.9943\n",
      "Epoch 00055: loss did not improve from 0.01679\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0173 - accuracy: 0.9942 - val_loss: 0.6562 - val_accuracy: 0.9180\n",
      " Learning rate:  3e-06\n",
      "Epoch 56/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9947\n",
      "Epoch 00056: loss improved from 0.01679 to 0.01635, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0163 - accuracy: 0.9947 - val_loss: 0.6605 - val_accuracy: 0.9181\n",
      " Learning rate:  3e-06\n",
      "Epoch 57/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9950\n",
      "Epoch 00057: loss improved from 0.01635 to 0.01539, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 33s 40ms/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.6635 - val_accuracy: 0.9178\n",
      " Learning rate:  3e-06\n",
      "Epoch 58/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9952\n",
      "Epoch 00058: loss improved from 0.01539 to 0.01514, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 40ms/step - loss: 0.0151 - accuracy: 0.9952 - val_loss: 0.6667 - val_accuracy: 0.9179\n",
      " Learning rate:  3e-06\n",
      "Epoch 59/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9950\n",
      "Epoch 00059: loss did not improve from 0.01514\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0164 - accuracy: 0.9950 - val_loss: 0.6665 - val_accuracy: 0.9184\n",
      " Learning rate:  3e-06\n",
      "Epoch 60/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0161 - accuracy: 0.9949\n",
      "Epoch 00060: loss did not improve from 0.01514\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.6680 - val_accuracy: 0.9180\n",
      " Learning rate:  3e-06\n",
      "Epoch 61/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9949\n",
      "Epoch 00061: loss did not improve from 0.01514\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0152 - accuracy: 0.9949 - val_loss: 0.6711 - val_accuracy: 0.9182\n",
      " Learning rate:  3e-06\n",
      "Epoch 62/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0160 - accuracy: 0.9947\n",
      "Epoch 00062: loss did not improve from 0.01514\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0160 - accuracy: 0.9947 - val_loss: 0.6727 - val_accuracy: 0.9184\n",
      " Learning rate:  3e-06\n",
      "Epoch 63/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9950\n",
      "Epoch 00063: loss improved from 0.01514 to 0.01475, saving model to /mnt/vdb/thesis/CustomCNN.512_1211.hdf5\n",
      "832/832 [==============================] - 34s 41ms/step - loss: 0.0147 - accuracy: 0.9950 - val_loss: 0.6755 - val_accuracy: 0.9180\n",
      " Learning rate:  3e-06\n",
      "Epoch 64/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9951\n",
      "Epoch 00064: loss did not improve from 0.01475\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0159 - accuracy: 0.9951 - val_loss: 0.6758 - val_accuracy: 0.9182\n",
      " Learning rate:  3e-06\n",
      "Epoch 65/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9950\n",
      "Epoch 00065: loss did not improve from 0.01475\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0152 - accuracy: 0.9950 - val_loss: 0.6752 - val_accuracy: 0.9183\n",
      " Learning rate:  3e-06\n",
      "Epoch 66/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0158 - accuracy: 0.9950\n",
      "Epoch 00066: loss did not improve from 0.01475\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.6758 - val_accuracy: 0.9183\n",
      " Learning rate:  3e-06\n",
      "Epoch 67/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0149 - accuracy: 0.9953\n",
      "Epoch 00067: loss did not improve from 0.01475\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0149 - accuracy: 0.9952 - val_loss: 0.6775 - val_accuracy: 0.9181\n",
      " Learning rate:  3e-06\n",
      "Epoch 68/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9952\n",
      "Epoch 00068: loss did not improve from 0.01475\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0156 - accuracy: 0.9952 - val_loss: 0.6761 - val_accuracy: 0.9184\n",
      " Learning rate:  3e-06\n",
      "Epoch 69/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9948\n",
      "Epoch 00069: loss did not improve from 0.01475\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0154 - accuracy: 0.9948 - val_loss: 0.6766 - val_accuracy: 0.9183\n",
      " Learning rate:  3e-06\n",
      "Epoch 70/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9951\n",
      "Epoch 00070: loss did not improve from 0.01475\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0151 - accuracy: 0.9951 - val_loss: 0.6789 - val_accuracy: 0.9180\n",
      " Learning rate:  3e-06\n",
      "Epoch 71/100\n",
      "831/832 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9951\n",
      "Epoch 00071: loss did not improve from 0.01475\n",
      "832/832 [==============================] - 29s 35ms/step - loss: 0.0150 - accuracy: 0.9951 - val_loss: 0.6788 - val_accuracy: 0.9184\n",
      "Epoch 00071: early stopping\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"/mnt/vdb/thesis/CustomCNN.512_1211.hdf5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "# Early Stopping\n",
    "es = EarlyStopping(monitor='loss', patience=8, verbose=1)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    batch_size=64, validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint, es,lr_scheduler], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1663/1663 [==============================] - 11s 7ms/step - loss: 0.0069 - accuracy: 0.9981\n",
      "Train loss:  0.006891955621540546\n",
      "Train accuracy:  0.9981020092964172\n",
      "----------------------------------------------------------------------\n",
      "555/555 [==============================] - 4s 7ms/step - loss: 0.6788 - accuracy: 0.9184\n",
      "Val loss:  0.6788290739059448\n",
      "Val accuracy:  0.9184283018112183\n",
      "----------------------------------------------------------------------\n",
      "555/555 [==============================] - 4s 7ms/step - loss: 0.7124 - accuracy: 0.9175\n",
      "Test loss:  0.7124297022819519\n",
      "Test accuracy:  0.9174699783325195\n"
     ]
    }
   ],
   "source": [
    "display_model_score(model,\n",
    "    [X_train, y_train],\n",
    "    [X_val, y_val],\n",
    "    [X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92      8928\n",
      "           1       0.92      0.92      0.92      8811\n",
      "\n",
      "    accuracy                           0.92     17739\n",
      "   macro avg       0.92      0.92      0.92     17739\n",
      "weighted avg       0.92      0.92      0.92     17739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_probas = model.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_predict = np.where(y_probas > threshold, 1, 0)\n",
    "\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fALQEI9g4Ejn"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gh0Zvl9j4E38"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CustomDNNModel.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
