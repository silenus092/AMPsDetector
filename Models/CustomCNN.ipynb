{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48641189/fitting-3d-data-as-input-into-keras-sequential-model-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qhoptim\n",
      "  Downloading qhoptim-1.1.0-py3-none-any.whl (20 kB)\n",
      "\u001b[33mWARNING: Error parsing requirements for matplotlib: [Errno 2] No such file or directory: '/home/ubuntu/miniconda3/envs/py3/lib/python3.7/site-packages/matplotlib-3.3.2.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: qhoptim\n",
      "Successfully installed qhoptim-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install qhoptim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 4.9.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ubuntu/miniconda3/envs/py3\n",
      "\n",
      "  added / updated specs:\n",
      "    - keras=2.3.1\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2020.12.5          |   py37h06a4308_0         141 KB\n",
      "    keras-2.3.1                |                0          12 KB\n",
      "    keras-base-2.3.1           |           py37_0         495 KB\n",
      "    openssl-1.1.1i             |       h27cfd23_0         2.5 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         3.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  keras-base         pkgs/main/linux-64::keras-base-2.3.1-py37_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2020.12.~ --> pkgs/main::ca-certificates-2021.1.19-h06a4308_0\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge::certifi-2020.12.5-py37h8~ --> pkgs/main::certifi-2020.12.5-py37h06a4308_0\n",
      "  keras                conda-forge/noarch::keras-2.4.3-py_0 --> pkgs/main/linux-64::keras-2.3.1-0\n",
      "  openssl            conda-forge::openssl-1.1.1i-h7f98852_0 --> pkgs/main::openssl-1.1.1i-h27cfd23_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.1.1i       | 2.5 MB    | ##################################### | 100% \n",
      "keras-base-2.3.1     | 495 KB    | ##################################### | 100% \n",
      "keras-2.3.1          | 12 KB     | ##################################### | 100% \n",
      "certifi-2020.12.5    | 141 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install keras=2.3.1 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Use only CPU\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17993022828398494510\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 3274618457875477408\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4232872575163048890\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 31595870336\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 5370779830014107259\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:00:06.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4aBzk8QXHS9S"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "\n",
    "#keras\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.layers import Conv1D, Add, MaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, GlobalMaxPooling1D\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "# CuDNNLSTM error; The error was because from TensorFlow 2 you do not need to specify CuDNNLSTM. \n",
    "# You can just use LSTM with no activation function and it will automatically use the CuDNN version. You do have to install CuDNN first.\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# \n",
    "from qhoptim.tf import QHAdamOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Optimizer\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "class QHAdam(Optimizer):\n",
    "    \"\"\"QH-Adam optimizer.\n",
    "    Default parameters follow those provided in the original paper.\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        beta_1: float, 0 < beta_1 < 1. Generally close to 1.\n",
    "        beta_2: float, 0 < beta_2 < 1. Generally close to 1.\n",
    "        neu_1: float, 0 < neu_1 < 1. Default based on paper equals 0.7\n",
    "        neu_2: float, 0 < neu_2 < 1. Default based on paper equals 1\n",
    "        epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
    "            algorithm from the paper \"On the Convergence of Adam and\n",
    "            Beyond\".\n",
    "    # References\n",
    "        - [QUASI-HYPERBOLIC MOMENTUM AND ADAM FOR DEEP LEARNING](\n",
    "           https://openreview.net/pdf?id=S1fUpoR5FQ)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 lr=0.001,\n",
    "                 beta_1=0.999,\n",
    "                 beta_2=0.999,\n",
    "                 neu_1 = 0.7,\n",
    "                 neu_2 = 1.,\n",
    "                 epsilon=1e-3,\n",
    "                 decay=0.,\n",
    "                 amsgrad=False,\n",
    "                 **kwargs):\n",
    "        super(QHAdam, self).__init__(name=\"QHAdam\",**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.learning_rate = K.variable(lr, name='lr')\n",
    "            self.iterations = K.variable(0, dtype='int64', name='iterations')\n",
    "            self.beta_1 = K.variable(beta_1, name='beta_1')\n",
    "            self.beta_2 = K.variable(beta_2, name='beta_2')\n",
    "            self.neu_1 = K.variable(neu_1, name='neu_1')\n",
    "            self.neu_2 = K.variable(neu_2, name='neu_2')\n",
    "            self.decay = K.variable(decay, name='decay')\n",
    "        if epsilon is None:\n",
    "            epsilon = K.epsilon()\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_decay = decay\n",
    "        self.amsgrad = amsgrad\n",
    "\n",
    "    @interfaces.legacy_get_updates_support\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.learning_rate\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        t = K.cast(self.iterations, K.floatx()) + 1\n",
    "\n",
    "\n",
    "        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        if self.amsgrad:\n",
    "            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n",
    "        else:\n",
    "            vhats = [K.zeros(1) for _ in params]\n",
    "        self.weights = [self.iterations] + ms + vs + vhats\n",
    "\n",
    "        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)\n",
    "            m_t_adj = m_t/(1. - K.pow(self.beta_1, t))\n",
    "            v_t_adj = v_t/(1. - K.pow(self.beta_2, t))\n",
    "            if self.amsgrad:\n",
    "                vhat_t = K.maximum(vhat, v_t)\n",
    "                p_t = p - lr * ((1.- self.neu_1)*g + self.neu_1*(m_t_adj)) / \\\n",
    "                       (K.sqrt((1.-self.neu_2) * K.square(g) + self.neu_2 * vhat_t) + self.epsilon)\n",
    "                self.updates.append(K.update(vhat, vhat_t))\n",
    "            else:\n",
    "                p_t = p - lr * ((1.-self.neu_1)*g + self.neu_1*(m_t_adj)) / \\\n",
    "                       (K.sqrt((1.-self.neu_2)*K.square(g) + self.neu_2 * v_t_adj) + self.epsilon)\n",
    "            self.updates.append(K.update(m, m_t))\n",
    "            self.updates.append(K.update(v, v_t))\n",
    "            new_p = p_t\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'lr': float(K.get_value(self.learning_rate)),\n",
    "                  'beta_1': float(K.get_value(self.beta_1)),\n",
    "                  'beta_2': float(K.get_value(self.beta_2)),\n",
    "                  'neu_1': float(K.get_value(self.neu_1)),\n",
    "                  'neu_2': float(K.get_value(self.neu_2)),\n",
    "                  'decay': float(K.get_value(self.decay)),\n",
    "                  'epsilon': self.epsilon,\n",
    "                  'amsgrad': self.amsgrad}\n",
    "        base_config = super(QHAdam, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FNhtXHJrfEE4"
   },
   "outputs": [],
   "source": [
    "# small old ../datasets/AMPsNonAMPs_df.239.plk\n",
    "# /home/ubuntu/data/AMPsNonAMPs_df.plk old dataset\n",
    "# /mnt/vdb/thesis/jax/AMPNonAMP.final.reps new dataset\n",
    "import pickle5 as pickle\n",
    "with open( \"/mnt/vdb/thesis/jax/AMPNonAMP.final.reps\", 'rb') as file:\n",
    "    AMPs_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "mGiMSzzPfj2t",
    "outputId": "092eb611-b89c-4cac-c8c4-0557799510cf"
   },
   "outputs": [],
   "source": [
    "#AMPs_df.drop_duplicates(subset=['Sequence'],inplace=True)\n",
    "AMPs_df =AMPs_df[AMPs_df[\"length\"] <=30 ]\n",
    "AMPs_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2i7Tk41aDZ5"
   },
   "source": [
    "### Utility function: plot_history, display_model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7F7ykQsDVxHO"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  # dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  x = range(1, len(acc) + 1)\n",
    "\n",
    "  plt.figure(figsize=(12, 5))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.plot(x, acc, 'b', label='Training acc')\n",
    "  plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  plt.legend()\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(x, loss, 'b', label='Training loss')\n",
    "  plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.legend()\n",
    "\n",
    "# Display model score(Loss & Accuracy) across all sets.\n",
    "def display_model_score(model, train, val, test):\n",
    "  train_score = model.evaluate(train[0], train[1], verbose=1)\n",
    "  print('Train loss: ', train_score[0])\n",
    "  print('Train accuracy: ', train_score[1])\n",
    "  print('-'*70)\n",
    "  val_score = model.evaluate(val[0], val[1], verbose=1)\n",
    "  print('Val loss: ', val_score[0])\n",
    "  print('Val accuracy: ', val_score[1])\n",
    "  print('-'*70)\n",
    "  test_score = model.evaluate(test[0], test[1], verbose=1)\n",
    "  print('Test loss: ', test_score[0])\n",
    "  print('Test accuracy: ', test_score[1])\n",
    "\n",
    "def plot_history_CV(cv, estimator,x,y):\n",
    "  # plot arrows\n",
    "  fig1 = plt.figure(figsize=[12,12])\n",
    "  ax1 = fig1.add_subplot(111,aspect = 'equal')\n",
    "  ax1.add_patch(\n",
    "      patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
    "      )\n",
    "  ax1.add_patch(\n",
    "      patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
    "      )\n",
    "\n",
    "  tprs = []\n",
    "  aucs = []\n",
    "  mean_fpr = np.linspace(0,1,100)\n",
    "  i = 1\n",
    "  for train,test in cv.split(x,y):\n",
    "      model = create_Modelbaseline()\n",
    "      model.fit(x[train],y.iloc[train],\n",
    "            epochs=30,\n",
    "            shuffle=True,verbose=0)\n",
    "      prediction = model.predict(x[test])\n",
    "      fpr, tpr, t = roc_curve(y[test], prediction[:, 1])\n",
    "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "      roc_auc = auc(fpr, tpr)\n",
    "      aucs.append(roc_auc)\n",
    "      plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
    "      i= i+1\n",
    "\n",
    "  plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
    "  mean_tpr = np.mean(tprs, axis=0)\n",
    "  mean_auc = auc(mean_fpr, mean_tpr)\n",
    "  plt.plot(mean_fpr, mean_tpr, color='blue',\n",
    "          label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
    "\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.title('ROC')\n",
    "  plt.legend(loc=\"lower right\")\n",
    "  plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
    "  plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI-_ZAvfIb5A"
   },
   "source": [
    "# Split Train/ Test / Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lAAQLx4UIptD"
   },
   "outputs": [],
   "source": [
    "#X= np.array(AMPs_df['reps'].to_list())\n",
    "#y= np.array(AMPs_df['class'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EkmqGqfUT0XR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254036, 1900)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape  = X.shape\n",
    "input_shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UWQ2IZWgIbST"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(AMPs_df['reps'].to_list()),  np.array(AMPs_df['class'].to_list()), test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "IM7Scdevkwpp",
    "outputId": "55fa7ea6-a5f9-4e23-bf42-f1a4bf64f8a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  152421\n",
      "Val size:  50807\n",
      "Test size:  50808\n"
     ]
    }
   ],
   "source": [
    "# Given data size\n",
    "print('Train size: ', len(X_train))\n",
    "print('Val size: ', len(X_val))\n",
    "print('Test size: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152421, 1900, 1)\n",
      "(50808, 1900, 1)\n",
      "(50807, 1900, 1)\n"
     ]
    }
   ],
   "source": [
    "# 3d dimension for LSTM\n",
    "# Batchs, n_timesteps, n_features\n",
    "\n",
    "# Images 3d dimension\n",
    "# width , heigth , channel\n",
    "\n",
    "# Conv1D with sequential data\n",
    "# batch, steps, channels\n",
    "\n",
    "# https://stackoverflow.com/questions/52803989/how-to-correct-shape-of-keras-input-into-a-3d-array/52804200\n",
    "X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))\n",
    "print(X_train.shape)\n",
    "X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],1))\n",
    "print(X_test.shape)\n",
    "X_val = np.reshape(X_val,(X_val.shape[0],X_val.shape[1],1))\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_Modelbaseline():\n",
    "    x_input = Input(shape=(1900,1)) # n_timesteps, n_features\n",
    "    # Conv\n",
    "    conv = Conv1D(512, kernel_size=7, strides=1, padding='same', activation='relu')(x_input) \n",
    "    conv = MaxPooling1D(pool_size=3)(conv)\n",
    "    conv = Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu')(conv) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "\n",
    "    # Flatten NN\n",
    "    flat = Flatten()(conv)\n",
    "    \n",
    "    layer_3 = Dense(1211, activation='relu')(flat)\n",
    "    dropout_3 = Dropout(0.2)(layer_3)\n",
    "    layer_4 = Dense(1211, activation='relu')(dropout_3)\n",
    "    dropout_4 = Dropout(0.2)(layer_4)\n",
    "    x_output = Dense(1, activation='sigmoid', name='output_layer', kernel_regularizer=l2(0.0001))(dropout_4)\n",
    "\n",
    "    model = Model(inputs=x_input, outputs=x_output)\n",
    "    model.compile(optimizer=\"RMSprop\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 1900, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 1900, 512)         4608      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 633, 512)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 633, 256)          655616    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 316, 256)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 80896)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1211)              97966267  \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 1211)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1211)              1467732   \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 1211)              0         \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 1)                 1212      \n",
      "=================================================================\n",
      "Total params: 100,095,435\n",
      "Trainable params: 100,095,435\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# n_timesteps, n_features, n_outputs = X_train.shape[2], X_train.shape[1], 1\n",
    "# nn.Conv1d with a kernel size of 1 and nn.Linear   give exactly the same results.\n",
    "\n",
    "\n",
    "#SGD\n",
    "#RMSprop\n",
    "#Adam\n",
    "#Adadelta\n",
    "#Adagrad\n",
    "#Adamax\n",
    "#Nadam\n",
    "#Ftrl\n",
    "#\n",
    "#\n",
    "\n",
    "def create_Modelbaseline():\n",
    "    x_input = Input(shape=(1900,1)) # n_timesteps, n_features\n",
    "    # Conv\n",
    "    #conv = Conv1D(512, kernel_size=7, strides=1, padding='same', activation='relu')(x_input) \n",
    "    #conv = MaxPooling1D(pool_size=3)(conv)\n",
    "    conv = Conv1D(512, kernel_size=8, strides=1, padding='same', activation='relu')(x_input) \n",
    "    conv = MaxPooling1D(pool_size=3)(conv)\n",
    "    conv = Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu')(conv) \n",
    "    conv = MaxPooling1D(pool_size=2)(conv)\n",
    "   \n",
    "    # Flatten NN\n",
    "    flat = Flatten()(conv)\n",
    "    layer_3 = Dense(1211, activation='relu')(flat)\n",
    "    dropout_3 = Dropout(0.2)(layer_3)\n",
    "    layer_4 = Dense(1211, activation='relu')(dropout_3)\n",
    "    dropout_4 = Dropout(0.2)(layer_4)\n",
    "    x_output = Dense(1, activation='sigmoid', name='output_layer', kernel_regularizer=l2(0.0001))(dropout_4)\n",
    "\n",
    "    model = Model(inputs=x_input, outputs=x_output)\n",
    "    model.compile(optimizer=\"Adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_Modelbaseline()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras import backend as K\n",
    "#K.set_value(model_ProtCNN.optimizer.learning_rate, 0.00001)\n",
    "def lr_schedule(epoch):\n",
    "    \n",
    "    lr = 1e-3\n",
    "    if epoch > 80:\n",
    "        lr = 0.1e-5\n",
    "    elif epoch > 50:    \n",
    "        lr = 0.3e-5\n",
    "    elif epoch > 20:\n",
    "        lr = 1e-4\n",
    "        \n",
    "    print(' Learning rate: ', lr)    \n",
    "    return lr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      " Learning rate:  0.001\n",
      "Epoch 1/100\n",
      "   2/1191 [..............................] - ETA: 34s - loss: 0.6906 - accuracy: 0.5078WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0206s vs `on_train_batch_end` time: 0.0366s). Check your callbacks.\n",
      "1191/1191 [==============================] - 72s 61ms/step - loss: 0.3221 - accuracy: 0.8587 - val_loss: 0.2449 - val_accuracy: 0.8992\n",
      " Learning rate:  0.001\n",
      "Epoch 2/100\n",
      "1191/1191 [==============================] - 72s 61ms/step - loss: 0.2233 - accuracy: 0.9114 - val_loss: 0.2008 - val_accuracy: 0.9228\n",
      " Learning rate:  0.001\n",
      "Epoch 3/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.1903 - accuracy: 0.9252 - val_loss: 0.1863 - val_accuracy: 0.9296\n",
      " Learning rate:  0.001\n",
      "Epoch 4/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.1705 - accuracy: 0.9339 - val_loss: 0.1720 - val_accuracy: 0.9341\n",
      " Learning rate:  0.001\n",
      "Epoch 5/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.1567 - accuracy: 0.9398 - val_loss: 0.1636 - val_accuracy: 0.9373\n",
      " Learning rate:  0.001\n",
      "Epoch 6/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.1442 - accuracy: 0.9445 - val_loss: 0.1570 - val_accuracy: 0.9415\n",
      " Learning rate:  0.001\n",
      "Epoch 7/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.1330 - accuracy: 0.9495 - val_loss: 0.1461 - val_accuracy: 0.9460\n",
      " Learning rate:  0.001\n",
      "Epoch 8/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.1240 - accuracy: 0.9531 - val_loss: 0.1458 - val_accuracy: 0.9456\n",
      " Learning rate:  0.001\n",
      "Epoch 9/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.1158 - accuracy: 0.9562 - val_loss: 0.1484 - val_accuracy: 0.9460\n",
      " Learning rate:  0.001\n",
      "Epoch 10/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.1069 - accuracy: 0.9594 - val_loss: 0.1454 - val_accuracy: 0.9495\n",
      " Learning rate:  0.001\n",
      "Epoch 11/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.1009 - accuracy: 0.9621 - val_loss: 0.1440 - val_accuracy: 0.9479\n",
      " Learning rate:  0.001\n",
      "Epoch 12/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0944 - accuracy: 0.9646 - val_loss: 0.1466 - val_accuracy: 0.9493\n",
      " Learning rate:  0.001\n",
      "Epoch 13/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0887 - accuracy: 0.9669 - val_loss: 0.1432 - val_accuracy: 0.9501\n",
      " Learning rate:  0.001\n",
      "Epoch 14/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0822 - accuracy: 0.9692 - val_loss: 0.1483 - val_accuracy: 0.9509\n",
      " Learning rate:  0.001\n",
      "Epoch 15/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0783 - accuracy: 0.9708 - val_loss: 0.1427 - val_accuracy: 0.9513\n",
      " Learning rate:  0.001\n",
      "Epoch 16/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0738 - accuracy: 0.9723 - val_loss: 0.1491 - val_accuracy: 0.9514\n",
      " Learning rate:  0.001\n",
      "Epoch 17/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0698 - accuracy: 0.9741 - val_loss: 0.1631 - val_accuracy: 0.9522\n",
      " Learning rate:  0.001\n",
      "Epoch 18/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0643 - accuracy: 0.9761 - val_loss: 0.1485 - val_accuracy: 0.9537\n",
      " Learning rate:  0.001\n",
      "Epoch 19/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0615 - accuracy: 0.9770 - val_loss: 0.1515 - val_accuracy: 0.9516\n",
      " Learning rate:  0.001\n",
      "Epoch 20/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0576 - accuracy: 0.9788 - val_loss: 0.1543 - val_accuracy: 0.9536\n",
      " Learning rate:  0.001\n",
      "Epoch 21/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0560 - accuracy: 0.9794 - val_loss: 0.1510 - val_accuracy: 0.9538\n",
      " Learning rate:  0.0001\n",
      "Epoch 22/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0312 - accuracy: 0.9887 - val_loss: 0.1789 - val_accuracy: 0.9575\n",
      " Learning rate:  0.0001\n",
      "Epoch 23/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0265 - accuracy: 0.9904 - val_loss: 0.1874 - val_accuracy: 0.9580\n",
      " Learning rate:  0.0001\n",
      "Epoch 24/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0233 - accuracy: 0.9918 - val_loss: 0.1949 - val_accuracy: 0.9579\n",
      " Learning rate:  0.0001\n",
      "Epoch 25/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0221 - accuracy: 0.9925 - val_loss: 0.2048 - val_accuracy: 0.9579\n",
      " Learning rate:  0.0001\n",
      "Epoch 26/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0208 - accuracy: 0.9929 - val_loss: 0.2159 - val_accuracy: 0.9582\n",
      " Learning rate:  0.0001\n",
      "Epoch 27/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0203 - accuracy: 0.9931 - val_loss: 0.2223 - val_accuracy: 0.9576\n",
      " Learning rate:  0.0001\n",
      "Epoch 28/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0186 - accuracy: 0.9936 - val_loss: 0.2297 - val_accuracy: 0.9585\n",
      " Learning rate:  0.0001\n",
      "Epoch 29/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 0.2309 - val_accuracy: 0.9586\n",
      " Learning rate:  0.0001\n",
      "Epoch 30/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0166 - accuracy: 0.9944 - val_loss: 0.2456 - val_accuracy: 0.9571\n",
      " Learning rate:  0.0001\n",
      "Epoch 31/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0173 - accuracy: 0.9943 - val_loss: 0.2449 - val_accuracy: 0.9577\n",
      " Learning rate:  0.0001\n",
      "Epoch 32/100\n",
      "1191/1191 [==============================] - 72s 61ms/step - loss: 0.0164 - accuracy: 0.9945 - val_loss: 0.2467 - val_accuracy: 0.9579\n",
      " Learning rate:  0.0001\n",
      "Epoch 33/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0156 - accuracy: 0.9946 - val_loss: 0.2542 - val_accuracy: 0.9573\n",
      " Learning rate:  0.0001\n",
      "Epoch 34/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0149 - accuracy: 0.9951 - val_loss: 0.2626 - val_accuracy: 0.9573\n",
      " Learning rate:  0.0001\n",
      "Epoch 35/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0146 - accuracy: 0.9953 - val_loss: 0.2619 - val_accuracy: 0.9574\n",
      " Learning rate:  0.0001\n",
      "Epoch 36/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0148 - accuracy: 0.9953 - val_loss: 0.2849 - val_accuracy: 0.9559\n",
      " Learning rate:  0.0001\n",
      "Epoch 37/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0135 - accuracy: 0.9957 - val_loss: 0.2828 - val_accuracy: 0.9571\n",
      " Learning rate:  0.0001\n",
      "Epoch 38/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0128 - accuracy: 0.9957 - val_loss: 0.2884 - val_accuracy: 0.9570\n",
      " Learning rate:  0.0001\n",
      "Epoch 39/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0127 - accuracy: 0.9959 - val_loss: 0.2954 - val_accuracy: 0.9566\n",
      " Learning rate:  0.0001\n",
      "Epoch 40/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0120 - accuracy: 0.9962 - val_loss: 0.3024 - val_accuracy: 0.9570\n",
      " Learning rate:  0.0001\n",
      "Epoch 41/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0124 - accuracy: 0.9960 - val_loss: 0.3047 - val_accuracy: 0.9570\n",
      " Learning rate:  0.0001\n",
      "Epoch 42/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0125 - accuracy: 0.9960 - val_loss: 0.2934 - val_accuracy: 0.9571\n",
      " Learning rate:  0.0001\n",
      "Epoch 43/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0118 - accuracy: 0.9962 - val_loss: 0.2890 - val_accuracy: 0.9572\n",
      " Learning rate:  0.0001\n",
      "Epoch 44/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0116 - accuracy: 0.9965 - val_loss: 0.3002 - val_accuracy: 0.9566\n",
      " Learning rate:  0.0001\n",
      "Epoch 45/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0113 - accuracy: 0.9962 - val_loss: 0.3008 - val_accuracy: 0.9572\n",
      " Learning rate:  0.0001\n",
      "Epoch 46/100\n",
      "1191/1191 [==============================] - 72s 61ms/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.2914 - val_accuracy: 0.9572\n",
      " Learning rate:  0.0001\n",
      "Epoch 47/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0107 - accuracy: 0.9965 - val_loss: 0.3086 - val_accuracy: 0.9565\n",
      " Learning rate:  0.0001\n",
      "Epoch 48/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.3040 - val_accuracy: 0.9565\n",
      " Learning rate:  0.0001\n",
      "Epoch 49/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0108 - accuracy: 0.9968 - val_loss: 0.3102 - val_accuracy: 0.9565\n",
      " Learning rate:  0.0001\n",
      "Epoch 50/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0102 - accuracy: 0.9968 - val_loss: 0.3162 - val_accuracy: 0.9565\n",
      " Learning rate:  0.0001\n",
      "Epoch 51/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0095 - accuracy: 0.9973 - val_loss: 0.3183 - val_accuracy: 0.9569\n",
      " Learning rate:  3e-06\n",
      "Epoch 52/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.3177 - val_accuracy: 0.9571\n",
      " Learning rate:  3e-06\n",
      "Epoch 53/100\n",
      "1191/1191 [==============================] - 72s 61ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.3174 - val_accuracy: 0.9572\n",
      " Learning rate:  3e-06\n",
      "Epoch 54/100\n",
      "1191/1191 [==============================] - 72s 61ms/step - loss: 0.0084 - accuracy: 0.9975 - val_loss: 0.3208 - val_accuracy: 0.9568\n",
      " Learning rate:  3e-06\n",
      "Epoch 55/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.3215 - val_accuracy: 0.9571\n",
      " Learning rate:  3e-06\n",
      "Epoch 56/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.3218 - val_accuracy: 0.9569\n",
      " Learning rate:  3e-06\n",
      "Epoch 57/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.3230 - val_accuracy: 0.9570\n",
      " Learning rate:  3e-06\n",
      "Epoch 58/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.3238 - val_accuracy: 0.9570\n",
      " Learning rate:  3e-06\n",
      "Epoch 59/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0084 - accuracy: 0.9974 - val_loss: 0.3247 - val_accuracy: 0.9572\n",
      " Learning rate:  3e-06\n",
      "Epoch 60/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.3242 - val_accuracy: 0.9570\n",
      " Learning rate:  3e-06\n",
      "Epoch 61/100\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.3239 - val_accuracy: 0.9571\n",
      " Learning rate:  3e-06\n",
      "Epoch 62/100\n",
      "1191/1191 [==============================] - 73s 61ms/step - loss: 0.0086 - accuracy: 0.9977 - val_loss: 0.3248 - val_accuracy: 0.9569\n",
      " Learning rate:  3e-06\n",
      "Epoch 63/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.3256 - val_accuracy: 0.9570\n",
      " Learning rate:  3e-06\n",
      "Epoch 64/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.3266 - val_accuracy: 0.9569\n",
      " Learning rate:  3e-06\n",
      "Epoch 65/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.3246 - val_accuracy: 0.9571\n",
      " Learning rate:  3e-06\n",
      "Epoch 66/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.3272 - val_accuracy: 0.9568\n",
      " Learning rate:  3e-06\n",
      "Epoch 67/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.3267 - val_accuracy: 0.9570\n",
      " Learning rate:  3e-06\n",
      "Epoch 68/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.3281 - val_accuracy: 0.9569\n",
      " Learning rate:  3e-06\n",
      "Epoch 69/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.3289 - val_accuracy: 0.9569\n",
      " Learning rate:  3e-06\n",
      "Epoch 70/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.3293 - val_accuracy: 0.9569\n",
      " Learning rate:  3e-06\n",
      "Epoch 71/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.3291 - val_accuracy: 0.9568\n",
      " Learning rate:  3e-06\n",
      "Epoch 72/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.3288 - val_accuracy: 0.9570\n",
      " Learning rate:  3e-06\n",
      "Epoch 73/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.3308 - val_accuracy: 0.9567\n",
      " Learning rate:  3e-06\n",
      "Epoch 74/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.3313 - val_accuracy: 0.9567\n",
      " Learning rate:  3e-06\n",
      "Epoch 75/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0076 - accuracy: 0.9979 - val_loss: 0.3298 - val_accuracy: 0.9567\n",
      " Learning rate:  3e-06\n",
      "Epoch 76/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.3319 - val_accuracy: 0.9565\n",
      " Learning rate:  3e-06\n",
      "Epoch 77/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.3329 - val_accuracy: 0.9567\n",
      " Learning rate:  3e-06\n",
      "Epoch 78/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0073 - accuracy: 0.9979 - val_loss: 0.3318 - val_accuracy: 0.9569\n",
      " Learning rate:  3e-06\n",
      "Epoch 79/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0072 - accuracy: 0.9977 - val_loss: 0.3320 - val_accuracy: 0.9569\n",
      " Learning rate:  3e-06\n",
      "Epoch 80/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.3321 - val_accuracy: 0.9569\n",
      " Learning rate:  3e-06\n",
      "Epoch 81/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0079 - accuracy: 0.9978 - val_loss: 0.3324 - val_accuracy: 0.9567\n",
      " Learning rate:  1e-06\n",
      "Epoch 82/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.3326 - val_accuracy: 0.9566\n",
      " Learning rate:  1e-06\n",
      "Epoch 83/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0075 - accuracy: 0.9977 - val_loss: 0.3332 - val_accuracy: 0.9567\n",
      " Learning rate:  1e-06\n",
      "Epoch 84/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.3328 - val_accuracy: 0.9566\n",
      " Learning rate:  1e-06\n",
      "Epoch 85/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0075 - accuracy: 0.9979 - val_loss: 0.3335 - val_accuracy: 0.9567\n",
      " Learning rate:  1e-06\n",
      "Epoch 86/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.3337 - val_accuracy: 0.9567\n",
      " Learning rate:  1e-06\n",
      "Epoch 87/100\n",
      "1191/1191 [==============================] - 72s 60ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.3338 - val_accuracy: 0.9566\n",
      "Epoch 00087: early stopping\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(\"/mnt/vdb/thesis/CustomCNN.Adam.512_1211.V2.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "# Early Stopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=8, verbose=1)\n",
    "\n",
    "#learning rate decay\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    batch_size=128, validation_data=(X_val, y_val),\n",
    "                    callbacks=[ es,lr_scheduler], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/mnt/vdb/thesis/CustomCNN.Adam.512_1211.V2.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4764/4764 [==============================] - 31s 7ms/step - loss: 0.0026 - accuracy: 0.9995\n",
      "Train loss:  0.0025876360014081\n",
      "Train accuracy:  0.9994816780090332\n",
      "----------------------------------------------------------------------\n",
      "1588/1588 [==============================] - 10s 6ms/step - loss: 0.3338 - accuracy: 0.9566\n",
      "Val loss:  0.33377593755722046\n",
      "Val accuracy:  0.9565807580947876\n",
      "----------------------------------------------------------------------\n",
      "1588/1588 [==============================] - 10s 7ms/step - loss: 0.3412 - accuracy: 0.9548\n",
      "Test loss:  0.34119874238967896\n",
      "Test accuracy:  0.9547905921936035\n"
     ]
    }
   ],
   "source": [
    "display_model_score(model,\n",
    "    [X_train, y_train],\n",
    "    [X_val, y_val],\n",
    "    [X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AMPs_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-13b0a12c6039>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mAMPs_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'AMPs_df' is not defined"
     ]
    }
   ],
   "source": [
    "del AMPs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96     25564\n",
      "           1       0.96      0.95      0.95     25244\n",
      "\n",
      "    accuracy                           0.95     50808\n",
      "   macro avg       0.95      0.95      0.95     50808\n",
      "weighted avg       0.95      0.95      0.95     50808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_probas = model.predict(X_test)\n",
    "threshold = 0.5\n",
    "y_predict = np.where(y_probas > threshold, 1, 0)\n",
    "\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fALQEI9g4Ejn"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gh0Zvl9j4E38"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CustomDNNModel.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
