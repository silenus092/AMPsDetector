{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CustomDNNModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sq1mJoMEXh73",
        "outputId": "e77ebffa-16df-451b-ae17-5e8500e4c361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "!pip install --upgrade keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nw5Dpk9fA1L",
        "outputId": "339d1cc2-30f9-4f1e-f4f6-4715c7a86a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!pip install pickle5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.6/dist-packages (0.0.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSOm_Dj_TZkP"
      },
      "source": [
        "!cp \"/content/drive/My Drive/Colab Notebooks/AMPsNonAMPs_df.plk\" ."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcnBwM_mmaDj",
        "outputId": "f6d9ee23-f636-4236-b6c0-5e1133f29053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "!pip install scikit-plot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-plot\n",
            "  Downloading https://files.pythonhosted.org/packages/7c/47/32520e259340c140a4ad27c1b97050dd3254fdc517b1d59974d47037510e/scikit_plot-0.3.7-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.6/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.18.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Installing collected packages: scikit-plot\n",
            "Successfully installed scikit-plot-0.3.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aBzk8QXHS9S"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import seaborn as sns\n",
        "\n",
        "from collections import Counter\n",
        "from prettytable import PrettyTable\n",
        "from IPython.display import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "from keras.constraints import max_norm\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Input, Dense, Dropout, Flatten, Activation\n",
        "from keras.layers import Conv1D, Add, MaxPooling1D, BatchNormalization\n",
        "from keras.layers import Embedding, Bidirectional, GlobalMaxPooling1D\n",
        "from tensorflow.python.keras.layers.recurrent import LSTM\n",
        "# CuDNNLSTM error; The error was because from TensorFlow 2 you do not need to specify CuDNNLSTM. \n",
        "# You can just use LSTM with no activation function and it will automatically use the CuDNN version. You do have to install CuDNN first.\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNhtXHJrfEE4"
      },
      "source": [
        "import pickle5 as pickle\n",
        "with open( \"AMPsNonAMPs_df.plk\", 'rb') as file:\n",
        "    AMPs_df = pickle.load(file)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz4gQNIeIaQh"
      },
      "source": [
        "# AMPs_df = pd.read_pickle('AMPsNonAMPs_df.plk')\n",
        "AMPs_df = pd.read_pickle('AMPsNonAMPs_df.239.plk')\n",
        "AMPs_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2i7Tk41aDZ5"
      },
      "source": [
        "### Utility function: plot_history, display_model_score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F7ykQsDVxHO"
      },
      "source": [
        "def plot_history(history):\n",
        "  # dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
        "  acc = history.history['accuracy']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  x = range(1, len(acc) + 1)\n",
        "\n",
        "  plt.figure(figsize=(12, 5))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(x, acc, 'b', label='Training acc')\n",
        "  plt.plot(x, val_acc, 'r', label='Validation acc')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.legend()\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(x, loss, 'b', label='Training loss')\n",
        "  plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.legend()\n",
        "\n",
        "# Display model score(Loss & Accuracy) across all sets.\n",
        "def display_model_score(model, train, val, test):\n",
        "  train_score = model.evaluate(train[0], train[1], verbose=1)\n",
        "  print('Train loss: ', train_score[0])\n",
        "  print('Train accuracy: ', train_score[1])\n",
        "  print('-'*70)\n",
        "  val_score = model.evaluate(val[0], val[1], verbose=1)\n",
        "  print('Val loss: ', val_score[0])\n",
        "  print('Val accuracy: ', val_score[1])\n",
        "  print('-'*70)\n",
        "  test_score = model.evaluate(test[0], test[1], verbose=1)\n",
        "  print('Test loss: ', test_score[0])\n",
        "  print('Test accuracy: ', test_score[1])\n",
        "\n",
        "def plot_history_CV(cv, estimator,x,y):\n",
        "  # plot arrows\n",
        "  fig1 = plt.figure(figsize=[12,12])\n",
        "  ax1 = fig1.add_subplot(111,aspect = 'equal')\n",
        "  ax1.add_patch(\n",
        "      patches.Arrow(0.45,0.5,-0.25,0.25,width=0.3,color='green',alpha = 0.5)\n",
        "      )\n",
        "  ax1.add_patch(\n",
        "      patches.Arrow(0.5,0.45,0.25,-0.25,width=0.3,color='red',alpha = 0.5)\n",
        "      )\n",
        "\n",
        "  tprs = []\n",
        "  aucs = []\n",
        "  mean_fpr = np.linspace(0,1,100)\n",
        "  i = 1\n",
        "  for train,test in cv.split(x,y):\n",
        "      model = create_Modelbaseline()\n",
        "      model.fit(x[train],y.iloc[train],\n",
        "            epochs=30,\n",
        "            shuffle=True,verbose=0)\n",
        "      prediction = model.predict(x[test])\n",
        "      fpr, tpr, t = roc_curve(y[test], prediction[:, 1])\n",
        "      tprs.append(interp(mean_fpr, fpr, tpr))\n",
        "      roc_auc = auc(fpr, tpr)\n",
        "      aucs.append(roc_auc)\n",
        "      plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n",
        "      i= i+1\n",
        "\n",
        "  plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n",
        "  mean_tpr = np.mean(tprs, axis=0)\n",
        "  mean_auc = auc(mean_fpr, mean_tpr)\n",
        "  plt.plot(mean_fpr, mean_tpr, color='blue',\n",
        "          label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n",
        "\n",
        "  plt.xlabel('False Positive Rate')\n",
        "  plt.ylabel('True Positive Rate')\n",
        "  plt.title('ROC')\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n",
        "  plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI-_ZAvfIb5A"
      },
      "source": [
        "# Split Train/ Test / Validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAAQLx4UIptD"
      },
      "source": [
        "X= np.array(AMPs_df['reps'].to_list())\n",
        "y= np.array(AMPs_df['class'].to_list())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkmqGqfUT0XR"
      },
      "source": [
        "input_shape  = X.shape"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWQ2IZWgIbST"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM7Scdevkwpp",
        "outputId": "5152ff7e-85ba-4680-9193-6a8a7f4a2c90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Given data size\n",
        "print('Train size: ', len(X_train))\n",
        "print('Val size: ', len(X_val))\n",
        "print('Test size: ', len(X_test))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size:  25467\n",
            "Val size:  8489\n",
            "Test size:  8489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dHnNnR7IAAs"
      },
      "source": [
        "# Model 1: OurModel "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tb99vdzJc6A"
      },
      "source": [
        "## Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnEsdsHWJf2B"
      },
      "source": [
        "def create_Modelbaseline():\n",
        "  x_input = Input(shape=(1900,))\n",
        "  layer_0 = Dense(896)(x_input)\n",
        "  dropout_0 = Dropout(0.5)(layer_0)\n",
        "  layer_1 = Dense(448)(dropout_0)\n",
        "  dropout_1 = Dropout(0.5)(layer_1)\n",
        "  layer_2 = Dense(224)(dropout_1)\n",
        "  dropout_2 = Dropout(0.5)(layer_2)\n",
        "  layer_3 = Dense(112)(dropout_2)\n",
        "  dropout_3 = Dropout(0.5)(layer_3)\n",
        "  layer_4 = Dense(56)(dropout_3)\n",
        "  dropout_4 = Dropout(0.5)(layer_4)\n",
        "  x_output = Dense(1, activation='sigmoid', name='output_layer')(dropout_4)\n",
        "\n",
        "  model = Model(inputs=x_input, outputs=x_output)\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BiM6J7tJf5u",
        "outputId": "0ccf33aa-fc59-4042-dd33-c4e946d0038d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        }
      },
      "source": [
        " model = create_Modelbaseline()\n",
        " model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 1900)]            0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 896)               1703296   \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 896)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 448)               401856    \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 448)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 224)               100576    \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 224)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 112)               25200     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 112)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 56)                6328      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 56)                0         \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 57        \n",
            "=================================================================\n",
            "Total params: 2,237,313\n",
            "Trainable params: 2,237,313\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn-5xjDBV0Fs",
        "outputId": "ebd47370-44fb-4f69-fd99-d34525fae8d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpoint = ModelCheckpoint(\"our_best_model.hdf5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val),\n",
        "                     callbacks=[checkpoint], verbose=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.93312, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.9331 - accuracy: 0.7697 - val_loss: 0.3559 - val_accuracy: 0.8530\n",
            "Epoch 2/100\n",
            "\n",
            "Epoch 00002: loss improved from 0.93312 to 0.36438, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.3644 - accuracy: 0.8467 - val_loss: 0.3197 - val_accuracy: 0.8584\n",
            "Epoch 3/100\n",
            "\n",
            "Epoch 00003: loss improved from 0.36438 to 0.33533, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.3353 - accuracy: 0.8598 - val_loss: 0.3158 - val_accuracy: 0.8662\n",
            "Epoch 4/100\n",
            "\n",
            "Epoch 00004: loss improved from 0.33533 to 0.33082, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.3308 - accuracy: 0.8624 - val_loss: 0.3141 - val_accuracy: 0.8649\n",
            "Epoch 5/100\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.33082\n",
            "796/796 - 3s - loss: 0.3322 - accuracy: 0.8617 - val_loss: 0.3192 - val_accuracy: 0.8631\n",
            "Epoch 6/100\n",
            "\n",
            "Epoch 00006: loss improved from 0.33082 to 0.32910, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.3291 - accuracy: 0.8643 - val_loss: 0.3162 - val_accuracy: 0.8634\n",
            "Epoch 7/100\n",
            "\n",
            "Epoch 00007: loss improved from 0.32910 to 0.32603, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.3260 - accuracy: 0.8646 - val_loss: 0.3046 - val_accuracy: 0.8691\n",
            "Epoch 8/100\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.32603\n",
            "796/796 - 3s - loss: 0.3329 - accuracy: 0.8651 - val_loss: 0.3395 - val_accuracy: 0.8674\n",
            "Epoch 9/100\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.32603\n",
            "796/796 - 3s - loss: 0.3411 - accuracy: 0.8648 - val_loss: 0.4391 - val_accuracy: 0.8338\n",
            "Epoch 10/100\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.32603\n",
            "796/796 - 3s - loss: 0.3344 - accuracy: 0.8649 - val_loss: 0.2955 - val_accuracy: 0.8735\n",
            "Epoch 11/100\n",
            "\n",
            "Epoch 00011: loss did not improve from 0.32603\n",
            "796/796 - 3s - loss: 0.3322 - accuracy: 0.8636 - val_loss: 0.3371 - val_accuracy: 0.8453\n",
            "Epoch 12/100\n",
            "\n",
            "Epoch 00012: loss improved from 0.32603 to 0.32282, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.3228 - accuracy: 0.8684 - val_loss: 0.2926 - val_accuracy: 0.8737\n",
            "Epoch 13/100\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.32282\n",
            "796/796 - 3s - loss: 0.3243 - accuracy: 0.8681 - val_loss: 0.3231 - val_accuracy: 0.8650\n",
            "Epoch 14/100\n",
            "\n",
            "Epoch 00014: loss improved from 0.32282 to 0.31968, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.3197 - accuracy: 0.8685 - val_loss: 0.2966 - val_accuracy: 0.8727\n",
            "Epoch 15/100\n",
            "\n",
            "Epoch 00015: loss improved from 0.31968 to 0.31874, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.3187 - accuracy: 0.8744 - val_loss: 0.2963 - val_accuracy: 0.8748\n",
            "Epoch 16/100\n",
            "\n",
            "Epoch 00016: loss did not improve from 0.31874\n",
            "796/796 - 3s - loss: 0.3223 - accuracy: 0.8698 - val_loss: 0.2988 - val_accuracy: 0.8754\n",
            "Epoch 17/100\n",
            "\n",
            "Epoch 00017: loss improved from 0.31874 to 0.31512, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.3151 - accuracy: 0.8754 - val_loss: 0.3706 - val_accuracy: 0.8464\n",
            "Epoch 18/100\n",
            "\n",
            "Epoch 00018: loss did not improve from 0.31512\n",
            "796/796 - 3s - loss: 0.3152 - accuracy: 0.8754 - val_loss: 0.3413 - val_accuracy: 0.8393\n",
            "Epoch 19/100\n",
            "\n",
            "Epoch 00019: loss did not improve from 0.31512\n",
            "796/796 - 3s - loss: 0.3152 - accuracy: 0.8755 - val_loss: 0.3024 - val_accuracy: 0.8753\n",
            "Epoch 20/100\n",
            "\n",
            "Epoch 00020: loss improved from 0.31512 to 0.30836, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.3084 - accuracy: 0.8782 - val_loss: 0.3127 - val_accuracy: 0.8736\n",
            "Epoch 21/100\n",
            "\n",
            "Epoch 00021: loss did not improve from 0.30836\n",
            "796/796 - 3s - loss: 0.3113 - accuracy: 0.8759 - val_loss: 0.3034 - val_accuracy: 0.8712\n",
            "Epoch 22/100\n",
            "\n",
            "Epoch 00022: loss did not improve from 0.30836\n",
            "796/796 - 3s - loss: 0.3089 - accuracy: 0.8771 - val_loss: 0.3707 - val_accuracy: 0.8677\n",
            "Epoch 23/100\n",
            "\n",
            "Epoch 00023: loss did not improve from 0.30836\n",
            "796/796 - 3s - loss: 0.3098 - accuracy: 0.8771 - val_loss: 0.2848 - val_accuracy: 0.8764\n",
            "Epoch 24/100\n",
            "\n",
            "Epoch 00024: loss improved from 0.30836 to 0.30552, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.3055 - accuracy: 0.8806 - val_loss: 0.3090 - val_accuracy: 0.8689\n",
            "Epoch 25/100\n",
            "\n",
            "Epoch 00025: loss improved from 0.30552 to 0.30132, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.3013 - accuracy: 0.8806 - val_loss: 0.2876 - val_accuracy: 0.8768\n",
            "Epoch 26/100\n",
            "\n",
            "Epoch 00026: loss did not improve from 0.30132\n",
            "796/796 - 3s - loss: 0.3079 - accuracy: 0.8759 - val_loss: 0.2945 - val_accuracy: 0.8773\n",
            "Epoch 27/100\n",
            "\n",
            "Epoch 00027: loss did not improve from 0.30132\n",
            "796/796 - 3s - loss: 0.3093 - accuracy: 0.8771 - val_loss: 0.3225 - val_accuracy: 0.8787\n",
            "Epoch 28/100\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.30132\n",
            "796/796 - 3s - loss: 0.3015 - accuracy: 0.8804 - val_loss: 0.2938 - val_accuracy: 0.8730\n",
            "Epoch 29/100\n",
            "\n",
            "Epoch 00029: loss did not improve from 0.30132\n",
            "796/796 - 3s - loss: 0.3089 - accuracy: 0.8796 - val_loss: 0.3370 - val_accuracy: 0.8612\n",
            "Epoch 30/100\n",
            "\n",
            "Epoch 00030: loss did not improve from 0.30132\n",
            "796/796 - 3s - loss: 0.3101 - accuracy: 0.8787 - val_loss: 0.2878 - val_accuracy: 0.8782\n",
            "Epoch 31/100\n",
            "\n",
            "Epoch 00031: loss did not improve from 0.30132\n",
            "796/796 - 3s - loss: 0.3050 - accuracy: 0.8826 - val_loss: 0.3148 - val_accuracy: 0.8761\n",
            "Epoch 32/100\n",
            "\n",
            "Epoch 00032: loss improved from 0.30132 to 0.29876, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.2988 - accuracy: 0.8828 - val_loss: 0.2897 - val_accuracy: 0.8758\n",
            "Epoch 33/100\n",
            "\n",
            "Epoch 00033: loss did not improve from 0.29876\n",
            "796/796 - 3s - loss: 0.3036 - accuracy: 0.8807 - val_loss: 0.3443 - val_accuracy: 0.8523\n",
            "Epoch 34/100\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.29876\n",
            "796/796 - 3s - loss: 0.3005 - accuracy: 0.8818 - val_loss: 0.2928 - val_accuracy: 0.8771\n",
            "Epoch 35/100\n",
            "\n",
            "Epoch 00035: loss improved from 0.29876 to 0.29594, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.2959 - accuracy: 0.8840 - val_loss: 0.3007 - val_accuracy: 0.8750\n",
            "Epoch 36/100\n",
            "\n",
            "Epoch 00036: loss did not improve from 0.29594\n",
            "796/796 - 3s - loss: 0.2995 - accuracy: 0.8831 - val_loss: 0.2979 - val_accuracy: 0.8767\n",
            "Epoch 37/100\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.29594\n",
            "796/796 - 3s - loss: 0.3012 - accuracy: 0.8826 - val_loss: 0.2967 - val_accuracy: 0.8741\n",
            "Epoch 38/100\n",
            "\n",
            "Epoch 00038: loss did not improve from 0.29594\n",
            "796/796 - 3s - loss: 0.3015 - accuracy: 0.8831 - val_loss: 0.3029 - val_accuracy: 0.8670\n",
            "Epoch 39/100\n",
            "\n",
            "Epoch 00039: loss improved from 0.29594 to 0.29442, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.2944 - accuracy: 0.8831 - val_loss: 0.3425 - val_accuracy: 0.8701\n",
            "Epoch 40/100\n",
            "\n",
            "Epoch 00040: loss did not improve from 0.29442\n",
            "796/796 - 3s - loss: 0.3041 - accuracy: 0.8811 - val_loss: 0.3369 - val_accuracy: 0.8707\n",
            "Epoch 41/100\n",
            "\n",
            "Epoch 00041: loss did not improve from 0.29442\n",
            "796/796 - 3s - loss: 0.2970 - accuracy: 0.8850 - val_loss: 0.2973 - val_accuracy: 0.8743\n",
            "Epoch 42/100\n",
            "\n",
            "Epoch 00042: loss improved from 0.29442 to 0.29233, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.2923 - accuracy: 0.8858 - val_loss: 0.3109 - val_accuracy: 0.8787\n",
            "Epoch 43/100\n",
            "\n",
            "Epoch 00043: loss did not improve from 0.29233\n",
            "796/796 - 3s - loss: 0.3016 - accuracy: 0.8794 - val_loss: 0.2875 - val_accuracy: 0.8827\n",
            "Epoch 44/100\n",
            "\n",
            "Epoch 00044: loss did not improve from 0.29233\n",
            "796/796 - 3s - loss: 0.2965 - accuracy: 0.8858 - val_loss: 0.3181 - val_accuracy: 0.8747\n",
            "Epoch 45/100\n",
            "\n",
            "Epoch 00045: loss did not improve from 0.29233\n",
            "796/796 - 3s - loss: 0.3031 - accuracy: 0.8815 - val_loss: 0.3450 - val_accuracy: 0.8738\n",
            "Epoch 46/100\n",
            "\n",
            "Epoch 00046: loss improved from 0.29233 to 0.29212, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.2921 - accuracy: 0.8844 - val_loss: 0.2849 - val_accuracy: 0.8807\n",
            "Epoch 47/100\n",
            "\n",
            "Epoch 00047: loss did not improve from 0.29212\n",
            "796/796 - 3s - loss: 0.2929 - accuracy: 0.8855 - val_loss: 0.2915 - val_accuracy: 0.8755\n",
            "Epoch 48/100\n",
            "\n",
            "Epoch 00048: loss did not improve from 0.29212\n",
            "796/796 - 3s - loss: 0.2979 - accuracy: 0.8839 - val_loss: 0.3072 - val_accuracy: 0.8789\n",
            "Epoch 49/100\n",
            "\n",
            "Epoch 00049: loss did not improve from 0.29212\n",
            "796/796 - 3s - loss: 0.3007 - accuracy: 0.8829 - val_loss: 0.3238 - val_accuracy: 0.8761\n",
            "Epoch 50/100\n",
            "\n",
            "Epoch 00050: loss did not improve from 0.29212\n",
            "796/796 - 3s - loss: 0.2969 - accuracy: 0.8839 - val_loss: 0.3027 - val_accuracy: 0.8782\n",
            "Epoch 51/100\n",
            "\n",
            "Epoch 00051: loss improved from 0.29212 to 0.28862, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.2886 - accuracy: 0.8846 - val_loss: 0.3458 - val_accuracy: 0.8790\n",
            "Epoch 52/100\n",
            "\n",
            "Epoch 00052: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2960 - accuracy: 0.8841 - val_loss: 0.2899 - val_accuracy: 0.8732\n",
            "Epoch 53/100\n",
            "\n",
            "Epoch 00053: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2934 - accuracy: 0.8845 - val_loss: 0.2897 - val_accuracy: 0.8756\n",
            "Epoch 54/100\n",
            "\n",
            "Epoch 00054: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2916 - accuracy: 0.8855 - val_loss: 0.2917 - val_accuracy: 0.8754\n",
            "Epoch 55/100\n",
            "\n",
            "Epoch 00055: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2956 - accuracy: 0.8838 - val_loss: 0.3053 - val_accuracy: 0.8721\n",
            "Epoch 56/100\n",
            "\n",
            "Epoch 00056: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2896 - accuracy: 0.8855 - val_loss: 0.3628 - val_accuracy: 0.8740\n",
            "Epoch 57/100\n",
            "\n",
            "Epoch 00057: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2975 - accuracy: 0.8841 - val_loss: 0.3736 - val_accuracy: 0.8623\n",
            "Epoch 58/100\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2994 - accuracy: 0.8833 - val_loss: 0.3095 - val_accuracy: 0.8768\n",
            "Epoch 59/100\n",
            "\n",
            "Epoch 00059: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2971 - accuracy: 0.8833 - val_loss: 0.3185 - val_accuracy: 0.8803\n",
            "Epoch 60/100\n",
            "\n",
            "Epoch 00060: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2922 - accuracy: 0.8855 - val_loss: 0.3460 - val_accuracy: 0.8626\n",
            "Epoch 61/100\n",
            "\n",
            "Epoch 00061: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2980 - accuracy: 0.8823 - val_loss: 0.3014 - val_accuracy: 0.8807\n",
            "Epoch 62/100\n",
            "\n",
            "Epoch 00062: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2905 - accuracy: 0.8868 - val_loss: 0.3174 - val_accuracy: 0.8723\n",
            "Epoch 63/100\n",
            "\n",
            "Epoch 00063: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2933 - accuracy: 0.8853 - val_loss: 0.3214 - val_accuracy: 0.8681\n",
            "Epoch 64/100\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2916 - accuracy: 0.8855 - val_loss: 0.2925 - val_accuracy: 0.8734\n",
            "Epoch 65/100\n",
            "\n",
            "Epoch 00065: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2896 - accuracy: 0.8855 - val_loss: 0.3237 - val_accuracy: 0.8803\n",
            "Epoch 66/100\n",
            "\n",
            "Epoch 00066: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2967 - accuracy: 0.8840 - val_loss: 0.2851 - val_accuracy: 0.8795\n",
            "Epoch 67/100\n",
            "\n",
            "Epoch 00067: loss did not improve from 0.28862\n",
            "796/796 - 3s - loss: 0.2957 - accuracy: 0.8848 - val_loss: 0.2984 - val_accuracy: 0.8738\n",
            "Epoch 68/100\n",
            "\n",
            "Epoch 00068: loss improved from 0.28862 to 0.28676, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.2868 - accuracy: 0.8857 - val_loss: 0.3025 - val_accuracy: 0.8734\n",
            "Epoch 69/100\n",
            "\n",
            "Epoch 00069: loss did not improve from 0.28676\n",
            "796/796 - 3s - loss: 0.2892 - accuracy: 0.8865 - val_loss: 0.3030 - val_accuracy: 0.8750\n",
            "Epoch 70/100\n",
            "\n",
            "Epoch 00070: loss did not improve from 0.28676\n",
            "796/796 - 3s - loss: 0.2938 - accuracy: 0.8851 - val_loss: 0.2902 - val_accuracy: 0.8768\n",
            "Epoch 71/100\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.28676\n",
            "796/796 - 3s - loss: 0.2955 - accuracy: 0.8847 - val_loss: 0.2838 - val_accuracy: 0.8811\n",
            "Epoch 72/100\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.28676\n",
            "796/796 - 3s - loss: 0.2898 - accuracy: 0.8868 - val_loss: 0.3021 - val_accuracy: 0.8826\n",
            "Epoch 73/100\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.28676\n",
            "796/796 - 3s - loss: 0.2891 - accuracy: 0.8862 - val_loss: 0.2877 - val_accuracy: 0.8798\n",
            "Epoch 74/100\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.28676\n",
            "796/796 - 3s - loss: 0.2928 - accuracy: 0.8854 - val_loss: 0.3972 - val_accuracy: 0.8765\n",
            "Epoch 75/100\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.28676\n",
            "796/796 - 3s - loss: 0.2919 - accuracy: 0.8867 - val_loss: 0.3482 - val_accuracy: 0.8575\n",
            "Epoch 76/100\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.28676\n",
            "796/796 - 3s - loss: 0.2905 - accuracy: 0.8872 - val_loss: 0.2919 - val_accuracy: 0.8754\n",
            "Epoch 77/100\n",
            "\n",
            "Epoch 00077: loss did not improve from 0.28676\n",
            "796/796 - 3s - loss: 0.2875 - accuracy: 0.8877 - val_loss: 0.2879 - val_accuracy: 0.8774\n",
            "Epoch 78/100\n",
            "\n",
            "Epoch 00078: loss did not improve from 0.28676\n",
            "796/796 - 3s - loss: 0.2871 - accuracy: 0.8876 - val_loss: 0.3569 - val_accuracy: 0.8787\n",
            "Epoch 79/100\n",
            "\n",
            "Epoch 00079: loss did not improve from 0.28676\n",
            "796/796 - 3s - loss: 0.2877 - accuracy: 0.8866 - val_loss: 0.3055 - val_accuracy: 0.8724\n",
            "Epoch 80/100\n",
            "\n",
            "Epoch 00080: loss did not improve from 0.28676\n",
            "796/796 - 3s - loss: 0.2972 - accuracy: 0.8840 - val_loss: 0.3254 - val_accuracy: 0.8806\n",
            "Epoch 81/100\n",
            "\n",
            "Epoch 00081: loss improved from 0.28676 to 0.28625, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.2862 - accuracy: 0.8880 - val_loss: 0.3280 - val_accuracy: 0.8809\n",
            "Epoch 82/100\n",
            "\n",
            "Epoch 00082: loss improved from 0.28625 to 0.28565, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.2857 - accuracy: 0.8876 - val_loss: 0.3302 - val_accuracy: 0.8761\n",
            "Epoch 83/100\n",
            "\n",
            "Epoch 00083: loss did not improve from 0.28565\n",
            "796/796 - 3s - loss: 0.2943 - accuracy: 0.8854 - val_loss: 0.3439 - val_accuracy: 0.8588\n",
            "Epoch 84/100\n",
            "\n",
            "Epoch 00084: loss did not improve from 0.28565\n",
            "796/796 - 3s - loss: 0.2864 - accuracy: 0.8900 - val_loss: 0.3202 - val_accuracy: 0.8784\n",
            "Epoch 85/100\n",
            "\n",
            "Epoch 00085: loss did not improve from 0.28565\n",
            "796/796 - 3s - loss: 0.2935 - accuracy: 0.8860 - val_loss: 0.3239 - val_accuracy: 0.8743\n",
            "Epoch 86/100\n",
            "\n",
            "Epoch 00086: loss improved from 0.28565 to 0.28394, saving model to our_best_model.hdf5\n",
            "796/796 - 3s - loss: 0.2839 - accuracy: 0.8912 - val_loss: 0.2990 - val_accuracy: 0.8773\n",
            "Epoch 87/100\n",
            "\n",
            "Epoch 00087: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2899 - accuracy: 0.8868 - val_loss: 0.3441 - val_accuracy: 0.8782\n",
            "Epoch 88/100\n",
            "\n",
            "Epoch 00088: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2881 - accuracy: 0.8878 - val_loss: 0.2941 - val_accuracy: 0.8802\n",
            "Epoch 89/100\n",
            "\n",
            "Epoch 00089: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2872 - accuracy: 0.8869 - val_loss: 0.2882 - val_accuracy: 0.8780\n",
            "Epoch 90/100\n",
            "\n",
            "Epoch 00090: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2866 - accuracy: 0.8874 - val_loss: 0.3313 - val_accuracy: 0.8773\n",
            "Epoch 91/100\n",
            "\n",
            "Epoch 00091: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2917 - accuracy: 0.8878 - val_loss: 0.3008 - val_accuracy: 0.8797\n",
            "Epoch 92/100\n",
            "\n",
            "Epoch 00092: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2858 - accuracy: 0.8893 - val_loss: 0.3141 - val_accuracy: 0.8717\n",
            "Epoch 93/100\n",
            "\n",
            "Epoch 00093: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2874 - accuracy: 0.8894 - val_loss: 0.3675 - val_accuracy: 0.8665\n",
            "Epoch 94/100\n",
            "\n",
            "Epoch 00094: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2885 - accuracy: 0.8883 - val_loss: 0.2922 - val_accuracy: 0.8761\n",
            "Epoch 95/100\n",
            "\n",
            "Epoch 00095: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2918 - accuracy: 0.8874 - val_loss: 0.3190 - val_accuracy: 0.8684\n",
            "Epoch 96/100\n",
            "\n",
            "Epoch 00096: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2953 - accuracy: 0.8841 - val_loss: 0.3065 - val_accuracy: 0.8776\n",
            "Epoch 97/100\n",
            "\n",
            "Epoch 00097: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2889 - accuracy: 0.8870 - val_loss: 0.3334 - val_accuracy: 0.8760\n",
            "Epoch 98/100\n",
            "\n",
            "Epoch 00098: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2888 - accuracy: 0.8874 - val_loss: 0.2954 - val_accuracy: 0.8776\n",
            "Epoch 99/100\n",
            "\n",
            "Epoch 00099: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2844 - accuracy: 0.8906 - val_loss: 0.3300 - val_accuracy: 0.8814\n",
            "Epoch 100/100\n",
            "\n",
            "Epoch 00100: loss did not improve from 0.28394\n",
            "796/796 - 3s - loss: 0.2882 - accuracy: 0.8880 - val_loss: 0.3437 - val_accuracy: 0.8715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78rubOsMatf7",
        "outputId": "b8b377a4-218c-4a7e-8796-c2a57132b029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsxtS0l3esdH"
      },
      "source": [
        "Overfitting if: training loss >> validation loss, training loss much greater than validation loss. That is underfitting.\n",
        "\n",
        "Underfitting if: training loss << validation loss, training loss much less than validation loss. That is overfitting.\n",
        "\n",
        "1. The training loss keeps decreasing after every epoch. Our model is learning to recognize the specific sequences in the training set. (Overfitting)\n",
        "2. The validation loss keeps .....\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMW5lf7fcX6h",
        "outputId": "c2c9a25d-fb3d-44d8-b831-9d5df2ec1b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "display_model_score(model,\n",
        "    [X_train, y_train],\n",
        "    [X_val, y_val],\n",
        "    [X_test, y_test])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "796/796 [==============================] - 1s 2ms/step - loss: 0.2443 - accuracy: 0.9010\n",
            "Train loss:  0.24427326023578644\n",
            "Train accuracy:  0.9010484218597412\n",
            "----------------------------------------------------------------------\n",
            "266/266 [==============================] - 0s 2ms/step - loss: 0.3437 - accuracy: 0.8715\n",
            "Val loss:  0.3436543345451355\n",
            "Val accuracy:  0.8714807629585266\n",
            "----------------------------------------------------------------------\n",
            "266/266 [==============================] - 0s 2ms/step - loss: 0.3325 - accuracy: 0.8777\n",
            "Test loss:  0.33247143030166626\n",
            "Test accuracy:  0.8777241110801697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N7O3u_dm1FL",
        "outputId": "5dda7411-cfc5-466a-bb10-0061b3f580fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "y_probas = model.predict(X_test)\n",
        "y_probas"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.5158687e-03],\n",
              "       [3.8974282e-01],\n",
              "       [9.5025122e-01],\n",
              "       ...,\n",
              "       [1.7285377e-01],\n",
              "       [1.2119177e-06],\n",
              "       [3.4751445e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9PISRBnj0xr",
        "outputId": "9b0b8d60-278a-44c0-8d82-2c8914434fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "source": [
        "threshold = 0.5\n",
        "y_predict = np.where(y_probas > threshold, 1, 0)\n",
        "\n",
        "print(classification_report(y_test, y_predict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.92      0.88      4300\n",
            "           1       0.91      0.83      0.87      4189\n",
            "\n",
            "    accuracy                           0.88      8489\n",
            "   macro avg       0.88      0.88      0.88      8489\n",
            "weighted avg       0.88      0.88      0.88      8489\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBBbFeGaXM7O"
      },
      "source": [
        "# Cross validation with whole dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfOdoWiqJfmO",
        "outputId": "62540ce0-881d-492b-a725-58e0e6977939",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "estimator = KerasClassifier(build_fn=create_Modelbaseline, epochs=100,verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "results = cross_val_score(estimator, X, y, cv=kfold)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_2:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_2:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "1062/1062 - 3s - loss: 0.3516 - accuracy: 0.8460\n",
            "Epoch 2/100\n",
            "1062/1062 - 3s - loss: 0.3116 - accuracy: 0.8654\n",
            "Epoch 3/100\n",
            "1062/1062 - 3s - loss: 0.2976 - accuracy: 0.8732\n",
            "Epoch 4/100\n",
            "1062/1062 - 3s - loss: 0.2912 - accuracy: 0.8760\n",
            "Epoch 5/100\n",
            "1062/1062 - 3s - loss: 0.2830 - accuracy: 0.8798\n",
            "Epoch 6/100\n",
            "1062/1062 - 3s - loss: 0.2793 - accuracy: 0.8810\n",
            "Epoch 7/100\n",
            "1062/1062 - 2s - loss: 0.2784 - accuracy: 0.8817\n",
            "Epoch 8/100\n",
            "1062/1062 - 3s - loss: 0.2786 - accuracy: 0.8830\n",
            "Epoch 9/100\n",
            "1062/1062 - 2s - loss: 0.2714 - accuracy: 0.8844\n",
            "Epoch 10/100\n",
            "1062/1062 - 3s - loss: 0.2694 - accuracy: 0.8867\n",
            "Epoch 11/100\n",
            "1062/1062 - 3s - loss: 0.2655 - accuracy: 0.8879\n",
            "Epoch 12/100\n",
            "1062/1062 - 2s - loss: 0.2666 - accuracy: 0.8878\n",
            "Epoch 13/100\n",
            "1062/1062 - 2s - loss: 0.2648 - accuracy: 0.8875\n",
            "Epoch 14/100\n",
            "1062/1062 - 2s - loss: 0.2625 - accuracy: 0.8895\n",
            "Epoch 15/100\n",
            "1062/1062 - 3s - loss: 0.2590 - accuracy: 0.8917\n",
            "Epoch 16/100\n",
            "1062/1062 - 3s - loss: 0.2610 - accuracy: 0.8921\n",
            "Epoch 17/100\n",
            "1062/1062 - 3s - loss: 0.2600 - accuracy: 0.8910\n",
            "Epoch 18/100\n",
            "1062/1062 - 3s - loss: 0.2584 - accuracy: 0.8926\n",
            "Epoch 19/100\n",
            "1062/1062 - 3s - loss: 0.2563 - accuracy: 0.8932\n",
            "Epoch 20/100\n",
            "1062/1062 - 2s - loss: 0.2562 - accuracy: 0.8935\n",
            "Epoch 21/100\n",
            "1062/1062 - 2s - loss: 0.2564 - accuracy: 0.8915\n",
            "Epoch 22/100\n",
            "1062/1062 - 2s - loss: 0.2561 - accuracy: 0.8939\n",
            "Epoch 23/100\n",
            "1062/1062 - 3s - loss: 0.2536 - accuracy: 0.8946\n",
            "Epoch 24/100\n",
            "1062/1062 - 2s - loss: 0.2533 - accuracy: 0.8932\n",
            "Epoch 25/100\n",
            "1062/1062 - 2s - loss: 0.2533 - accuracy: 0.8951\n",
            "Epoch 26/100\n",
            "1062/1062 - 2s - loss: 0.2520 - accuracy: 0.8942\n",
            "Epoch 27/100\n",
            "1062/1062 - 2s - loss: 0.2529 - accuracy: 0.8939\n",
            "Epoch 28/100\n",
            "1062/1062 - 2s - loss: 0.2520 - accuracy: 0.8953\n",
            "Epoch 29/100\n",
            "1062/1062 - 3s - loss: 0.2533 - accuracy: 0.8934\n",
            "Epoch 30/100\n",
            "1062/1062 - 2s - loss: 0.2514 - accuracy: 0.8939\n",
            "Epoch 31/100\n",
            "1062/1062 - 3s - loss: 0.2522 - accuracy: 0.8958\n",
            "Epoch 32/100\n",
            "1062/1062 - 2s - loss: 0.2495 - accuracy: 0.8963\n",
            "Epoch 33/100\n",
            "1062/1062 - 3s - loss: 0.2468 - accuracy: 0.8975\n",
            "Epoch 34/100\n",
            "1062/1062 - 3s - loss: 0.2521 - accuracy: 0.8957\n",
            "Epoch 35/100\n",
            "1062/1062 - 3s - loss: 0.2493 - accuracy: 0.8965\n",
            "Epoch 36/100\n",
            "1062/1062 - 3s - loss: 0.2483 - accuracy: 0.8964\n",
            "Epoch 37/100\n",
            "1062/1062 - 3s - loss: 0.2491 - accuracy: 0.8960\n",
            "Epoch 38/100\n",
            "1062/1062 - 2s - loss: 0.2499 - accuracy: 0.8961\n",
            "Epoch 39/100\n",
            "1062/1062 - 3s - loss: 0.2457 - accuracy: 0.8988\n",
            "Epoch 40/100\n",
            "1062/1062 - 2s - loss: 0.2462 - accuracy: 0.8977\n",
            "Epoch 41/100\n",
            "1062/1062 - 2s - loss: 0.2459 - accuracy: 0.8977\n",
            "Epoch 42/100\n",
            "1062/1062 - 2s - loss: 0.2457 - accuracy: 0.8987\n",
            "Epoch 43/100\n",
            "1062/1062 - 3s - loss: 0.2459 - accuracy: 0.8977\n",
            "Epoch 44/100\n",
            "1062/1062 - 2s - loss: 0.2480 - accuracy: 0.8972\n",
            "Epoch 45/100\n",
            "1062/1062 - 2s - loss: 0.2440 - accuracy: 0.8989\n",
            "Epoch 46/100\n",
            "1062/1062 - 3s - loss: 0.2433 - accuracy: 0.8994\n",
            "Epoch 47/100\n",
            "1062/1062 - 2s - loss: 0.2454 - accuracy: 0.8988\n",
            "Epoch 48/100\n",
            "1062/1062 - 3s - loss: 0.2447 - accuracy: 0.8972\n",
            "Epoch 49/100\n",
            "1062/1062 - 2s - loss: 0.2443 - accuracy: 0.8979\n",
            "Epoch 50/100\n",
            "1062/1062 - 3s - loss: 0.2464 - accuracy: 0.8974\n",
            "Epoch 51/100\n",
            "1062/1062 - 3s - loss: 0.2446 - accuracy: 0.8981\n",
            "Epoch 52/100\n",
            "1062/1062 - 3s - loss: 0.2438 - accuracy: 0.8981\n",
            "Epoch 53/100\n",
            "1062/1062 - 3s - loss: 0.2444 - accuracy: 0.8988\n",
            "Epoch 54/100\n",
            "1062/1062 - 3s - loss: 0.2425 - accuracy: 0.8992\n",
            "Epoch 55/100\n",
            "1062/1062 - 2s - loss: 0.2431 - accuracy: 0.8989\n",
            "Epoch 56/100\n",
            "1062/1062 - 2s - loss: 0.2431 - accuracy: 0.9002\n",
            "Epoch 57/100\n",
            "1062/1062 - 2s - loss: 0.2443 - accuracy: 0.8992\n",
            "Epoch 58/100\n",
            "1062/1062 - 3s - loss: 0.2411 - accuracy: 0.8997\n",
            "Epoch 59/100\n",
            "1062/1062 - 3s - loss: 0.2425 - accuracy: 0.8999\n",
            "Epoch 60/100\n",
            "1062/1062 - 3s - loss: 0.2432 - accuracy: 0.8994\n",
            "Epoch 61/100\n",
            "1062/1062 - 3s - loss: 0.2416 - accuracy: 0.8995\n",
            "Epoch 62/100\n",
            "1062/1062 - 3s - loss: 0.2421 - accuracy: 0.9002\n",
            "Epoch 63/100\n",
            "1062/1062 - 3s - loss: 0.2410 - accuracy: 0.9003\n",
            "Epoch 64/100\n",
            "1062/1062 - 3s - loss: 0.2406 - accuracy: 0.9000\n",
            "Epoch 65/100\n",
            "1062/1062 - 3s - loss: 0.2399 - accuracy: 0.9005\n",
            "Epoch 66/100\n",
            "1062/1062 - 3s - loss: 0.2422 - accuracy: 0.8997\n",
            "Epoch 67/100\n",
            "1062/1062 - 3s - loss: 0.2429 - accuracy: 0.8995\n",
            "Epoch 68/100\n",
            "1062/1062 - 3s - loss: 0.2413 - accuracy: 0.9006\n",
            "Epoch 69/100\n",
            "1062/1062 - 2s - loss: 0.2415 - accuracy: 0.8989\n",
            "Epoch 70/100\n",
            "1062/1062 - 2s - loss: 0.2391 - accuracy: 0.9009\n",
            "Epoch 71/100\n",
            "1062/1062 - 2s - loss: 0.2416 - accuracy: 0.8997\n",
            "Epoch 72/100\n",
            "1062/1062 - 2s - loss: 0.2410 - accuracy: 0.9004\n",
            "Epoch 73/100\n",
            "1062/1062 - 3s - loss: 0.2414 - accuracy: 0.9000\n",
            "Epoch 74/100\n",
            "1062/1062 - 2s - loss: 0.2393 - accuracy: 0.9027\n",
            "Epoch 75/100\n",
            "1062/1062 - 2s - loss: 0.2400 - accuracy: 0.9013\n",
            "Epoch 76/100\n",
            "1062/1062 - 3s - loss: 0.2397 - accuracy: 0.9003\n",
            "Epoch 77/100\n",
            "1062/1062 - 2s - loss: 0.2395 - accuracy: 0.9020\n",
            "Epoch 78/100\n",
            "1062/1062 - 2s - loss: 0.2397 - accuracy: 0.9018\n",
            "Epoch 79/100\n",
            "1062/1062 - 2s - loss: 0.2386 - accuracy: 0.9011\n",
            "Epoch 80/100\n",
            "1062/1062 - 2s - loss: 0.2399 - accuracy: 0.9000\n",
            "Epoch 81/100\n",
            "1062/1062 - 2s - loss: 0.2388 - accuracy: 0.9013\n",
            "Epoch 82/100\n",
            "1062/1062 - 2s - loss: 0.2393 - accuracy: 0.9010\n",
            "Epoch 83/100\n",
            "1062/1062 - 2s - loss: 0.2390 - accuracy: 0.9008\n",
            "Epoch 84/100\n",
            "1062/1062 - 2s - loss: 0.2369 - accuracy: 0.9030\n",
            "Epoch 85/100\n",
            "1062/1062 - 3s - loss: 0.2400 - accuracy: 0.9007\n",
            "Epoch 86/100\n",
            "1062/1062 - 2s - loss: 0.2393 - accuracy: 0.9010\n",
            "Epoch 87/100\n",
            "1062/1062 - 3s - loss: 0.2390 - accuracy: 0.8998\n",
            "Epoch 88/100\n",
            "1062/1062 - 3s - loss: 0.2387 - accuracy: 0.9010\n",
            "Epoch 89/100\n",
            "1062/1062 - 2s - loss: 0.2380 - accuracy: 0.9005\n",
            "Epoch 90/100\n",
            "1062/1062 - 3s - loss: 0.2383 - accuracy: 0.9016\n",
            "Epoch 91/100\n",
            "1062/1062 - 3s - loss: 0.2379 - accuracy: 0.9014\n",
            "Epoch 92/100\n",
            "1062/1062 - 3s - loss: 0.2378 - accuracy: 0.9008\n",
            "Epoch 93/100\n",
            "1062/1062 - 3s - loss: 0.2361 - accuracy: 0.9033\n",
            "Epoch 94/100\n",
            "1062/1062 - 2s - loss: 0.2384 - accuracy: 0.9021\n",
            "Epoch 95/100\n",
            "1062/1062 - 2s - loss: 0.2380 - accuracy: 0.9032\n",
            "Epoch 96/100\n",
            "1062/1062 - 2s - loss: 0.2367 - accuracy: 0.9027\n",
            "Epoch 97/100\n",
            "1062/1062 - 3s - loss: 0.2352 - accuracy: 0.9031\n",
            "Epoch 98/100\n",
            "1062/1062 - 3s - loss: 0.2382 - accuracy: 0.9013\n",
            "Epoch 99/100\n",
            "1062/1062 - 3s - loss: 0.2365 - accuracy: 0.9015\n",
            "Epoch 100/100\n",
            "1062/1062 - 3s - loss: 0.2375 - accuracy: 0.9012\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_2:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "266/266 - 0s - loss: 0.2659 - accuracy: 0.8904\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_3:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_3:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "1062/1062 - 2s - loss: 0.3532 - accuracy: 0.8476\n",
            "Epoch 2/100\n",
            "1062/1062 - 2s - loss: 0.3107 - accuracy: 0.8673\n",
            "Epoch 3/100\n",
            "1062/1062 - 2s - loss: 0.2996 - accuracy: 0.8720\n",
            "Epoch 4/100\n",
            "1062/1062 - 2s - loss: 0.2883 - accuracy: 0.8770\n",
            "Epoch 5/100\n",
            "1062/1062 - 2s - loss: 0.2822 - accuracy: 0.8798\n",
            "Epoch 6/100\n",
            "1062/1062 - 2s - loss: 0.2823 - accuracy: 0.8796\n",
            "Epoch 7/100\n",
            "1062/1062 - 2s - loss: 0.2739 - accuracy: 0.8830\n",
            "Epoch 8/100\n",
            "1062/1062 - 2s - loss: 0.2762 - accuracy: 0.8826\n",
            "Epoch 9/100\n",
            "1062/1062 - 3s - loss: 0.2704 - accuracy: 0.8866\n",
            "Epoch 10/100\n",
            "1062/1062 - 3s - loss: 0.2689 - accuracy: 0.8874\n",
            "Epoch 11/100\n",
            "1062/1062 - 2s - loss: 0.2668 - accuracy: 0.8856\n",
            "Epoch 12/100\n",
            "1062/1062 - 3s - loss: 0.2639 - accuracy: 0.8890\n",
            "Epoch 13/100\n",
            "1062/1062 - 2s - loss: 0.2653 - accuracy: 0.8879\n",
            "Epoch 14/100\n",
            "1062/1062 - 2s - loss: 0.2618 - accuracy: 0.8889\n",
            "Epoch 15/100\n",
            "1062/1062 - 2s - loss: 0.2609 - accuracy: 0.8899\n",
            "Epoch 16/100\n",
            "1062/1062 - 2s - loss: 0.2601 - accuracy: 0.8899\n",
            "Epoch 17/100\n",
            "1062/1062 - 2s - loss: 0.2620 - accuracy: 0.8893\n",
            "Epoch 18/100\n",
            "1062/1062 - 2s - loss: 0.2586 - accuracy: 0.8908\n",
            "Epoch 19/100\n",
            "1062/1062 - 3s - loss: 0.2580 - accuracy: 0.8910\n",
            "Epoch 20/100\n",
            "1062/1062 - 2s - loss: 0.2551 - accuracy: 0.8919\n",
            "Epoch 21/100\n",
            "1062/1062 - 2s - loss: 0.2564 - accuracy: 0.8936\n",
            "Epoch 22/100\n",
            "1062/1062 - 3s - loss: 0.2538 - accuracy: 0.8937\n",
            "Epoch 23/100\n",
            "1062/1062 - 2s - loss: 0.2550 - accuracy: 0.8938\n",
            "Epoch 24/100\n",
            "1062/1062 - 3s - loss: 0.2538 - accuracy: 0.8936\n",
            "Epoch 25/100\n",
            "1062/1062 - 3s - loss: 0.2537 - accuracy: 0.8924\n",
            "Epoch 26/100\n",
            "1062/1062 - 2s - loss: 0.2510 - accuracy: 0.8962\n",
            "Epoch 27/100\n",
            "1062/1062 - 3s - loss: 0.2504 - accuracy: 0.8943\n",
            "Epoch 28/100\n",
            "1062/1062 - 3s - loss: 0.2505 - accuracy: 0.8946\n",
            "Epoch 29/100\n",
            "1062/1062 - 3s - loss: 0.2503 - accuracy: 0.8953\n",
            "Epoch 30/100\n",
            "1062/1062 - 3s - loss: 0.2498 - accuracy: 0.8942\n",
            "Epoch 31/100\n",
            "1062/1062 - 3s - loss: 0.2499 - accuracy: 0.8957\n",
            "Epoch 32/100\n",
            "1062/1062 - 3s - loss: 0.2487 - accuracy: 0.8954\n",
            "Epoch 33/100\n",
            "1062/1062 - 2s - loss: 0.2482 - accuracy: 0.8961\n",
            "Epoch 34/100\n",
            "1062/1062 - 2s - loss: 0.2491 - accuracy: 0.8946\n",
            "Epoch 35/100\n",
            "1062/1062 - 3s - loss: 0.2465 - accuracy: 0.8954\n",
            "Epoch 36/100\n",
            "1062/1062 - 3s - loss: 0.2475 - accuracy: 0.8947\n",
            "Epoch 37/100\n",
            "1062/1062 - 2s - loss: 0.2487 - accuracy: 0.8943\n",
            "Epoch 38/100\n",
            "1062/1062 - 2s - loss: 0.2463 - accuracy: 0.8971\n",
            "Epoch 39/100\n",
            "1062/1062 - 2s - loss: 0.2455 - accuracy: 0.8972\n",
            "Epoch 40/100\n",
            "1062/1062 - 3s - loss: 0.2468 - accuracy: 0.8966\n",
            "Epoch 41/100\n",
            "1062/1062 - 2s - loss: 0.2465 - accuracy: 0.8970\n",
            "Epoch 42/100\n",
            "1062/1062 - 2s - loss: 0.2461 - accuracy: 0.8974\n",
            "Epoch 43/100\n",
            "1062/1062 - 2s - loss: 0.2451 - accuracy: 0.8974\n",
            "Epoch 44/100\n",
            "1062/1062 - 2s - loss: 0.2464 - accuracy: 0.8956\n",
            "Epoch 45/100\n",
            "1062/1062 - 2s - loss: 0.2422 - accuracy: 0.8984\n",
            "Epoch 46/100\n",
            "1062/1062 - 2s - loss: 0.2428 - accuracy: 0.8985\n",
            "Epoch 47/100\n",
            "1062/1062 - 2s - loss: 0.2453 - accuracy: 0.8969\n",
            "Epoch 48/100\n",
            "1062/1062 - 3s - loss: 0.2451 - accuracy: 0.8972\n",
            "Epoch 49/100\n",
            "1062/1062 - 2s - loss: 0.2445 - accuracy: 0.8981\n",
            "Epoch 50/100\n",
            "1062/1062 - 2s - loss: 0.2441 - accuracy: 0.8980\n",
            "Epoch 51/100\n",
            "1062/1062 - 2s - loss: 0.2438 - accuracy: 0.8979\n",
            "Epoch 52/100\n",
            "1062/1062 - 2s - loss: 0.2432 - accuracy: 0.8971\n",
            "Epoch 53/100\n",
            "1062/1062 - 2s - loss: 0.2430 - accuracy: 0.8980\n",
            "Epoch 54/100\n",
            "1062/1062 - 3s - loss: 0.2434 - accuracy: 0.8988\n",
            "Epoch 55/100\n",
            "1062/1062 - 3s - loss: 0.2431 - accuracy: 0.8979\n",
            "Epoch 56/100\n",
            "1062/1062 - 2s - loss: 0.2430 - accuracy: 0.8981\n",
            "Epoch 57/100\n",
            "1062/1062 - 2s - loss: 0.2418 - accuracy: 0.8973\n",
            "Epoch 58/100\n",
            "1062/1062 - 2s - loss: 0.2439 - accuracy: 0.8986\n",
            "Epoch 59/100\n",
            "1062/1062 - 2s - loss: 0.2426 - accuracy: 0.8977\n",
            "Epoch 60/100\n",
            "1062/1062 - 3s - loss: 0.2417 - accuracy: 0.8993\n",
            "Epoch 61/100\n",
            "1062/1062 - 3s - loss: 0.2410 - accuracy: 0.8993\n",
            "Epoch 62/100\n",
            "1062/1062 - 3s - loss: 0.2396 - accuracy: 0.8998\n",
            "Epoch 63/100\n",
            "1062/1062 - 2s - loss: 0.2424 - accuracy: 0.8993\n",
            "Epoch 64/100\n",
            "1062/1062 - 2s - loss: 0.2392 - accuracy: 0.9005\n",
            "Epoch 65/100\n",
            "1062/1062 - 2s - loss: 0.2418 - accuracy: 0.8985\n",
            "Epoch 66/100\n",
            "1062/1062 - 2s - loss: 0.2417 - accuracy: 0.8987\n",
            "Epoch 67/100\n",
            "1062/1062 - 2s - loss: 0.2394 - accuracy: 0.8998\n",
            "Epoch 68/100\n",
            "1062/1062 - 2s - loss: 0.2391 - accuracy: 0.8991\n",
            "Epoch 69/100\n",
            "1062/1062 - 3s - loss: 0.2392 - accuracy: 0.9007\n",
            "Epoch 70/100\n",
            "1062/1062 - 3s - loss: 0.2410 - accuracy: 0.8994\n",
            "Epoch 71/100\n",
            "1062/1062 - 2s - loss: 0.2403 - accuracy: 0.8980\n",
            "Epoch 72/100\n",
            "1062/1062 - 3s - loss: 0.2401 - accuracy: 0.9001\n",
            "Epoch 73/100\n",
            "1062/1062 - 2s - loss: 0.2386 - accuracy: 0.9011\n",
            "Epoch 74/100\n",
            "1062/1062 - 3s - loss: 0.2394 - accuracy: 0.9009\n",
            "Epoch 75/100\n",
            "1062/1062 - 2s - loss: 0.2390 - accuracy: 0.9008\n",
            "Epoch 76/100\n",
            "1062/1062 - 2s - loss: 0.2392 - accuracy: 0.9006\n",
            "Epoch 77/100\n",
            "1062/1062 - 2s - loss: 0.2380 - accuracy: 0.9013\n",
            "Epoch 78/100\n",
            "1062/1062 - 2s - loss: 0.2390 - accuracy: 0.9003\n",
            "Epoch 79/100\n",
            "1062/1062 - 3s - loss: 0.2407 - accuracy: 0.8997\n",
            "Epoch 80/100\n",
            "1062/1062 - 2s - loss: 0.2393 - accuracy: 0.9007\n",
            "Epoch 81/100\n",
            "1062/1062 - 2s - loss: 0.2369 - accuracy: 0.9025\n",
            "Epoch 82/100\n",
            "1062/1062 - 2s - loss: 0.2389 - accuracy: 0.9005\n",
            "Epoch 83/100\n",
            "1062/1062 - 3s - loss: 0.2375 - accuracy: 0.9006\n",
            "Epoch 84/100\n",
            "1062/1062 - 2s - loss: 0.2363 - accuracy: 0.9012\n",
            "Epoch 85/100\n",
            "1062/1062 - 3s - loss: 0.2370 - accuracy: 0.9009\n",
            "Epoch 86/100\n",
            "1062/1062 - 2s - loss: 0.2379 - accuracy: 0.9008\n",
            "Epoch 87/100\n",
            "1062/1062 - 3s - loss: 0.2361 - accuracy: 0.9015\n",
            "Epoch 88/100\n",
            "1062/1062 - 2s - loss: 0.2357 - accuracy: 0.9015\n",
            "Epoch 89/100\n",
            "1062/1062 - 3s - loss: 0.2375 - accuracy: 0.9012\n",
            "Epoch 90/100\n",
            "1062/1062 - 3s - loss: 0.2392 - accuracy: 0.8991\n",
            "Epoch 91/100\n",
            "1062/1062 - 3s - loss: 0.2351 - accuracy: 0.9014\n",
            "Epoch 92/100\n",
            "1062/1062 - 3s - loss: 0.2358 - accuracy: 0.9008\n",
            "Epoch 93/100\n",
            "1062/1062 - 3s - loss: 0.2374 - accuracy: 0.9013\n",
            "Epoch 94/100\n",
            "1062/1062 - 3s - loss: 0.2360 - accuracy: 0.9020\n",
            "Epoch 95/100\n",
            "1062/1062 - 3s - loss: 0.2372 - accuracy: 0.9005\n",
            "Epoch 96/100\n",
            "1062/1062 - 2s - loss: 0.2362 - accuracy: 0.8999\n",
            "Epoch 97/100\n",
            "1062/1062 - 2s - loss: 0.2354 - accuracy: 0.9014\n",
            "Epoch 98/100\n",
            "1062/1062 - 2s - loss: 0.2348 - accuracy: 0.9020\n",
            "Epoch 99/100\n",
            "1062/1062 - 2s - loss: 0.2369 - accuracy: 0.9003\n",
            "Epoch 100/100\n",
            "1062/1062 - 2s - loss: 0.2354 - accuracy: 0.9013\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_3:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "266/266 - 0s - loss: 0.2632 - accuracy: 0.8887\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_4:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_4:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "1062/1062 - 2s - loss: 0.3584 - accuracy: 0.8424\n",
            "Epoch 2/100\n",
            "1062/1062 - 2s - loss: 0.3146 - accuracy: 0.8644\n",
            "Epoch 3/100\n",
            "1062/1062 - 3s - loss: 0.3008 - accuracy: 0.8705\n",
            "Epoch 4/100\n",
            "1062/1062 - 2s - loss: 0.2916 - accuracy: 0.8762\n",
            "Epoch 5/100\n",
            "1062/1062 - 3s - loss: 0.2891 - accuracy: 0.8775\n",
            "Epoch 6/100\n",
            "1062/1062 - 3s - loss: 0.2802 - accuracy: 0.8814\n",
            "Epoch 7/100\n",
            "1062/1062 - 3s - loss: 0.2782 - accuracy: 0.8815\n",
            "Epoch 8/100\n",
            "1062/1062 - 2s - loss: 0.2755 - accuracy: 0.8836\n",
            "Epoch 9/100\n",
            "1062/1062 - 2s - loss: 0.2712 - accuracy: 0.8858\n",
            "Epoch 10/100\n",
            "1062/1062 - 3s - loss: 0.2695 - accuracy: 0.8864\n",
            "Epoch 11/100\n",
            "1062/1062 - 2s - loss: 0.2672 - accuracy: 0.8875\n",
            "Epoch 12/100\n",
            "1062/1062 - 3s - loss: 0.2668 - accuracy: 0.8890\n",
            "Epoch 13/100\n",
            "1062/1062 - 3s - loss: 0.2650 - accuracy: 0.8883\n",
            "Epoch 14/100\n",
            "1062/1062 - 3s - loss: 0.2628 - accuracy: 0.8891\n",
            "Epoch 15/100\n",
            "1062/1062 - 3s - loss: 0.2634 - accuracy: 0.8891\n",
            "Epoch 16/100\n",
            "1062/1062 - 2s - loss: 0.2637 - accuracy: 0.8892\n",
            "Epoch 17/100\n",
            "1062/1062 - 2s - loss: 0.2590 - accuracy: 0.8906\n",
            "Epoch 18/100\n",
            "1062/1062 - 2s - loss: 0.2596 - accuracy: 0.8906\n",
            "Epoch 19/100\n",
            "1062/1062 - 2s - loss: 0.2571 - accuracy: 0.8926\n",
            "Epoch 20/100\n",
            "1062/1062 - 2s - loss: 0.2584 - accuracy: 0.8913\n",
            "Epoch 21/100\n",
            "1062/1062 - 2s - loss: 0.2577 - accuracy: 0.8922\n",
            "Epoch 22/100\n",
            "1062/1062 - 3s - loss: 0.2541 - accuracy: 0.8936\n",
            "Epoch 23/100\n",
            "1062/1062 - 3s - loss: 0.2553 - accuracy: 0.8920\n",
            "Epoch 24/100\n",
            "1062/1062 - 3s - loss: 0.2569 - accuracy: 0.8922\n",
            "Epoch 25/100\n",
            "1062/1062 - 3s - loss: 0.2536 - accuracy: 0.8931\n",
            "Epoch 26/100\n",
            "1062/1062 - 3s - loss: 0.2522 - accuracy: 0.8950\n",
            "Epoch 27/100\n",
            "1062/1062 - 2s - loss: 0.2517 - accuracy: 0.8951\n",
            "Epoch 28/100\n",
            "1062/1062 - 3s - loss: 0.2528 - accuracy: 0.8939\n",
            "Epoch 29/100\n",
            "1062/1062 - 2s - loss: 0.2518 - accuracy: 0.8948\n",
            "Epoch 30/100\n",
            "1062/1062 - 2s - loss: 0.2518 - accuracy: 0.8937\n",
            "Epoch 31/100\n",
            "1062/1062 - 2s - loss: 0.2500 - accuracy: 0.8957\n",
            "Epoch 32/100\n",
            "1062/1062 - 2s - loss: 0.2518 - accuracy: 0.8940\n",
            "Epoch 33/100\n",
            "1062/1062 - 2s - loss: 0.2511 - accuracy: 0.8945\n",
            "Epoch 34/100\n",
            "1062/1062 - 2s - loss: 0.2493 - accuracy: 0.8957\n",
            "Epoch 35/100\n",
            "1062/1062 - 3s - loss: 0.2489 - accuracy: 0.8944\n",
            "Epoch 36/100\n",
            "1062/1062 - 2s - loss: 0.2476 - accuracy: 0.8971\n",
            "Epoch 37/100\n",
            "1062/1062 - 3s - loss: 0.2487 - accuracy: 0.8961\n",
            "Epoch 38/100\n",
            "1062/1062 - 2s - loss: 0.2485 - accuracy: 0.8958\n",
            "Epoch 39/100\n",
            "1062/1062 - 3s - loss: 0.2477 - accuracy: 0.8968\n",
            "Epoch 40/100\n",
            "1062/1062 - 2s - loss: 0.2467 - accuracy: 0.8974\n",
            "Epoch 41/100\n",
            "1062/1062 - 3s - loss: 0.2469 - accuracy: 0.8966\n",
            "Epoch 42/100\n",
            "1062/1062 - 2s - loss: 0.2474 - accuracy: 0.8968\n",
            "Epoch 43/100\n",
            "1062/1062 - 2s - loss: 0.2463 - accuracy: 0.8980\n",
            "Epoch 44/100\n",
            "1062/1062 - 2s - loss: 0.2451 - accuracy: 0.8966\n",
            "Epoch 45/100\n",
            "1062/1062 - 2s - loss: 0.2465 - accuracy: 0.8969\n",
            "Epoch 46/100\n",
            "1062/1062 - 2s - loss: 0.2464 - accuracy: 0.8976\n",
            "Epoch 47/100\n",
            "1062/1062 - 2s - loss: 0.2455 - accuracy: 0.8968\n",
            "Epoch 48/100\n",
            "1062/1062 - 3s - loss: 0.2449 - accuracy: 0.8985\n",
            "Epoch 49/100\n",
            "1062/1062 - 2s - loss: 0.2457 - accuracy: 0.8969\n",
            "Epoch 50/100\n",
            "1062/1062 - 3s - loss: 0.2437 - accuracy: 0.8983\n",
            "Epoch 51/100\n",
            "1062/1062 - 2s - loss: 0.2433 - accuracy: 0.8989\n",
            "Epoch 52/100\n",
            "1062/1062 - 3s - loss: 0.2431 - accuracy: 0.8977\n",
            "Epoch 53/100\n",
            "1062/1062 - 2s - loss: 0.2432 - accuracy: 0.8974\n",
            "Epoch 54/100\n",
            "1062/1062 - 2s - loss: 0.2436 - accuracy: 0.8985\n",
            "Epoch 55/100\n",
            "1062/1062 - 3s - loss: 0.2433 - accuracy: 0.8988\n",
            "Epoch 56/100\n",
            "1062/1062 - 3s - loss: 0.2431 - accuracy: 0.8988\n",
            "Epoch 57/100\n",
            "1062/1062 - 3s - loss: 0.2426 - accuracy: 0.9000\n",
            "Epoch 58/100\n",
            "1062/1062 - 3s - loss: 0.2429 - accuracy: 0.8976\n",
            "Epoch 59/100\n",
            "1062/1062 - 2s - loss: 0.2422 - accuracy: 0.8991\n",
            "Epoch 60/100\n",
            "1062/1062 - 2s - loss: 0.2417 - accuracy: 0.8995\n",
            "Epoch 61/100\n",
            "1062/1062 - 2s - loss: 0.2429 - accuracy: 0.8992\n",
            "Epoch 62/100\n",
            "1062/1062 - 3s - loss: 0.2419 - accuracy: 0.8991\n",
            "Epoch 63/100\n",
            "1062/1062 - 2s - loss: 0.2421 - accuracy: 0.8989\n",
            "Epoch 64/100\n",
            "1062/1062 - 2s - loss: 0.2422 - accuracy: 0.9000\n",
            "Epoch 65/100\n",
            "1062/1062 - 3s - loss: 0.2409 - accuracy: 0.9007\n",
            "Epoch 66/100\n",
            "1062/1062 - 2s - loss: 0.2399 - accuracy: 0.8996\n",
            "Epoch 67/100\n",
            "1062/1062 - 2s - loss: 0.2413 - accuracy: 0.8988\n",
            "Epoch 68/100\n",
            "1062/1062 - 2s - loss: 0.2403 - accuracy: 0.8990\n",
            "Epoch 69/100\n",
            "1062/1062 - 2s - loss: 0.2385 - accuracy: 0.9001\n",
            "Epoch 70/100\n",
            "1062/1062 - 2s - loss: 0.2419 - accuracy: 0.8989\n",
            "Epoch 71/100\n",
            "1062/1062 - 2s - loss: 0.2398 - accuracy: 0.8998\n",
            "Epoch 72/100\n",
            "1062/1062 - 2s - loss: 0.2397 - accuracy: 0.9008\n",
            "Epoch 73/100\n",
            "1062/1062 - 2s - loss: 0.2407 - accuracy: 0.9013\n",
            "Epoch 74/100\n",
            "1062/1062 - 3s - loss: 0.2381 - accuracy: 0.9014\n",
            "Epoch 75/100\n",
            "1062/1062 - 3s - loss: 0.2407 - accuracy: 0.9007\n",
            "Epoch 76/100\n",
            "1062/1062 - 3s - loss: 0.2393 - accuracy: 0.8998\n",
            "Epoch 77/100\n",
            "1062/1062 - 2s - loss: 0.2390 - accuracy: 0.9012\n",
            "Epoch 78/100\n",
            "1062/1062 - 2s - loss: 0.2386 - accuracy: 0.9008\n",
            "Epoch 79/100\n",
            "1062/1062 - 2s - loss: 0.2405 - accuracy: 0.8995\n",
            "Epoch 80/100\n",
            "1062/1062 - 2s - loss: 0.2380 - accuracy: 0.9024\n",
            "Epoch 81/100\n",
            "1062/1062 - 2s - loss: 0.2391 - accuracy: 0.9006\n",
            "Epoch 82/100\n",
            "1062/1062 - 3s - loss: 0.2398 - accuracy: 0.9001\n",
            "Epoch 83/100\n",
            "1062/1062 - 2s - loss: 0.2393 - accuracy: 0.9007\n",
            "Epoch 84/100\n",
            "1062/1062 - 3s - loss: 0.2390 - accuracy: 0.9017\n",
            "Epoch 85/100\n",
            "1062/1062 - 3s - loss: 0.2386 - accuracy: 0.9008\n",
            "Epoch 86/100\n",
            "1062/1062 - 2s - loss: 0.2380 - accuracy: 0.9018\n",
            "Epoch 87/100\n",
            "1062/1062 - 3s - loss: 0.2380 - accuracy: 0.9020\n",
            "Epoch 88/100\n",
            "1062/1062 - 3s - loss: 0.2379 - accuracy: 0.9023\n",
            "Epoch 89/100\n",
            "1062/1062 - 3s - loss: 0.2382 - accuracy: 0.9013\n",
            "Epoch 90/100\n",
            "1062/1062 - 3s - loss: 0.2396 - accuracy: 0.9009\n",
            "Epoch 91/100\n",
            "1062/1062 - 3s - loss: 0.2367 - accuracy: 0.9015\n",
            "Epoch 92/100\n",
            "1062/1062 - 2s - loss: 0.2376 - accuracy: 0.9013\n",
            "Epoch 93/100\n",
            "1062/1062 - 3s - loss: 0.2372 - accuracy: 0.9014\n",
            "Epoch 94/100\n",
            "1062/1062 - 2s - loss: 0.2381 - accuracy: 0.9006\n",
            "Epoch 95/100\n",
            "1062/1062 - 2s - loss: 0.2368 - accuracy: 0.9003\n",
            "Epoch 96/100\n",
            "1062/1062 - 2s - loss: 0.2375 - accuracy: 0.9015\n",
            "Epoch 97/100\n",
            "1062/1062 - 2s - loss: 0.2385 - accuracy: 0.9001\n",
            "Epoch 98/100\n",
            "1062/1062 - 3s - loss: 0.2384 - accuracy: 0.9011\n",
            "Epoch 99/100\n",
            "1062/1062 - 2s - loss: 0.2380 - accuracy: 0.9019\n",
            "Epoch 100/100\n",
            "1062/1062 - 2s - loss: 0.2380 - accuracy: 0.9026\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_4:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "266/266 - 0s - loss: 0.2899 - accuracy: 0.8807\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_5:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_5:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "1062/1062 - 2s - loss: 0.3509 - accuracy: 0.8468\n",
            "Epoch 2/100\n",
            "1062/1062 - 2s - loss: 0.3085 - accuracy: 0.8676\n",
            "Epoch 3/100\n",
            "1062/1062 - 2s - loss: 0.2923 - accuracy: 0.8749\n",
            "Epoch 4/100\n",
            "1062/1062 - 2s - loss: 0.2875 - accuracy: 0.8790\n",
            "Epoch 5/100\n",
            "1062/1062 - 2s - loss: 0.2802 - accuracy: 0.8801\n",
            "Epoch 6/100\n",
            "1062/1062 - 2s - loss: 0.2786 - accuracy: 0.8823\n",
            "Epoch 7/100\n",
            "1062/1062 - 2s - loss: 0.2702 - accuracy: 0.8846\n",
            "Epoch 8/100\n",
            "1062/1062 - 2s - loss: 0.2730 - accuracy: 0.8852\n",
            "Epoch 9/100\n",
            "1062/1062 - 2s - loss: 0.2690 - accuracy: 0.8842\n",
            "Epoch 10/100\n",
            "1062/1062 - 2s - loss: 0.2665 - accuracy: 0.8869\n",
            "Epoch 11/100\n",
            "1062/1062 - 3s - loss: 0.2638 - accuracy: 0.8884\n",
            "Epoch 12/100\n",
            "1062/1062 - 2s - loss: 0.2595 - accuracy: 0.8894\n",
            "Epoch 13/100\n",
            "1062/1062 - 2s - loss: 0.2599 - accuracy: 0.8915\n",
            "Epoch 14/100\n",
            "1062/1062 - 2s - loss: 0.2584 - accuracy: 0.8906\n",
            "Epoch 15/100\n",
            "1062/1062 - 2s - loss: 0.2593 - accuracy: 0.8916\n",
            "Epoch 16/100\n",
            "1062/1062 - 2s - loss: 0.2533 - accuracy: 0.8926\n",
            "Epoch 17/100\n",
            "1062/1062 - 2s - loss: 0.2561 - accuracy: 0.8915\n",
            "Epoch 18/100\n",
            "1062/1062 - 2s - loss: 0.2522 - accuracy: 0.8949\n",
            "Epoch 19/100\n",
            "1062/1062 - 3s - loss: 0.2552 - accuracy: 0.8924\n",
            "Epoch 20/100\n",
            "1062/1062 - 3s - loss: 0.2533 - accuracy: 0.8932\n",
            "Epoch 21/100\n",
            "1062/1062 - 3s - loss: 0.2533 - accuracy: 0.8927\n",
            "Epoch 22/100\n",
            "1062/1062 - 3s - loss: 0.2489 - accuracy: 0.8952\n",
            "Epoch 23/100\n",
            "1062/1062 - 3s - loss: 0.2525 - accuracy: 0.8936\n",
            "Epoch 24/100\n",
            "1062/1062 - 3s - loss: 0.2503 - accuracy: 0.8934\n",
            "Epoch 25/100\n",
            "1062/1062 - 2s - loss: 0.2485 - accuracy: 0.8952\n",
            "Epoch 26/100\n",
            "1062/1062 - 2s - loss: 0.2474 - accuracy: 0.8966\n",
            "Epoch 27/100\n",
            "1062/1062 - 2s - loss: 0.2489 - accuracy: 0.8961\n",
            "Epoch 28/100\n",
            "1062/1062 - 2s - loss: 0.2463 - accuracy: 0.8965\n",
            "Epoch 29/100\n",
            "1062/1062 - 3s - loss: 0.2455 - accuracy: 0.8981\n",
            "Epoch 30/100\n",
            "1062/1062 - 2s - loss: 0.2459 - accuracy: 0.8974\n",
            "Epoch 31/100\n",
            "1062/1062 - 3s - loss: 0.2445 - accuracy: 0.8983\n",
            "Epoch 32/100\n",
            "1062/1062 - 2s - loss: 0.2458 - accuracy: 0.8968\n",
            "Epoch 33/100\n",
            "1062/1062 - 2s - loss: 0.2456 - accuracy: 0.8971\n",
            "Epoch 34/100\n",
            "1062/1062 - 2s - loss: 0.2458 - accuracy: 0.8969\n",
            "Epoch 35/100\n",
            "1062/1062 - 3s - loss: 0.2436 - accuracy: 0.8988\n",
            "Epoch 36/100\n",
            "1062/1062 - 3s - loss: 0.2425 - accuracy: 0.8986\n",
            "Epoch 37/100\n",
            "1062/1062 - 3s - loss: 0.2426 - accuracy: 0.8980\n",
            "Epoch 38/100\n",
            "1062/1062 - 3s - loss: 0.2419 - accuracy: 0.8995\n",
            "Epoch 39/100\n",
            "1062/1062 - 2s - loss: 0.2434 - accuracy: 0.8990\n",
            "Epoch 40/100\n",
            "1062/1062 - 2s - loss: 0.2432 - accuracy: 0.8983\n",
            "Epoch 41/100\n",
            "1062/1062 - 2s - loss: 0.2416 - accuracy: 0.8989\n",
            "Epoch 42/100\n",
            "1062/1062 - 3s - loss: 0.2417 - accuracy: 0.8987\n",
            "Epoch 43/100\n",
            "1062/1062 - 2s - loss: 0.2403 - accuracy: 0.8983\n",
            "Epoch 44/100\n",
            "1062/1062 - 2s - loss: 0.2409 - accuracy: 0.8994\n",
            "Epoch 45/100\n",
            "1062/1062 - 2s - loss: 0.2411 - accuracy: 0.8988\n",
            "Epoch 46/100\n",
            "1062/1062 - 2s - loss: 0.2397 - accuracy: 0.9001\n",
            "Epoch 47/100\n",
            "1062/1062 - 2s - loss: 0.2406 - accuracy: 0.9009\n",
            "Epoch 48/100\n",
            "1062/1062 - 3s - loss: 0.2379 - accuracy: 0.9004\n",
            "Epoch 49/100\n",
            "1062/1062 - 2s - loss: 0.2401 - accuracy: 0.8990\n",
            "Epoch 50/100\n",
            "1062/1062 - 2s - loss: 0.2383 - accuracy: 0.8994\n",
            "Epoch 51/100\n",
            "1062/1062 - 3s - loss: 0.2382 - accuracy: 0.8994\n",
            "Epoch 52/100\n",
            "1062/1062 - 3s - loss: 0.2393 - accuracy: 0.8995\n",
            "Epoch 53/100\n",
            "1062/1062 - 3s - loss: 0.2373 - accuracy: 0.9006\n",
            "Epoch 54/100\n",
            "1062/1062 - 2s - loss: 0.2379 - accuracy: 0.9006\n",
            "Epoch 55/100\n",
            "1062/1062 - 2s - loss: 0.2390 - accuracy: 0.9000\n",
            "Epoch 56/100\n",
            "1062/1062 - 2s - loss: 0.2385 - accuracy: 0.9011\n",
            "Epoch 57/100\n",
            "1062/1062 - 2s - loss: 0.2386 - accuracy: 0.9004\n",
            "Epoch 58/100\n",
            "1062/1062 - 2s - loss: 0.2369 - accuracy: 0.9016\n",
            "Epoch 59/100\n",
            "1062/1062 - 2s - loss: 0.2374 - accuracy: 0.9017\n",
            "Epoch 60/100\n",
            "1062/1062 - 2s - loss: 0.2379 - accuracy: 0.9012\n",
            "Epoch 61/100\n",
            "1062/1062 - 2s - loss: 0.2363 - accuracy: 0.9015\n",
            "Epoch 62/100\n",
            "1062/1062 - 2s - loss: 0.2356 - accuracy: 0.9016\n",
            "Epoch 63/100\n",
            "1062/1062 - 2s - loss: 0.2380 - accuracy: 0.9010\n",
            "Epoch 64/100\n",
            "1062/1062 - 2s - loss: 0.2357 - accuracy: 0.9033\n",
            "Epoch 65/100\n",
            "1062/1062 - 2s - loss: 0.2369 - accuracy: 0.9005\n",
            "Epoch 66/100\n",
            "1062/1062 - 2s - loss: 0.2362 - accuracy: 0.9017\n",
            "Epoch 67/100\n",
            "1062/1062 - 2s - loss: 0.2357 - accuracy: 0.9028\n",
            "Epoch 68/100\n",
            "1062/1062 - 2s - loss: 0.2344 - accuracy: 0.9027\n",
            "Epoch 69/100\n",
            "1062/1062 - 3s - loss: 0.2352 - accuracy: 0.9028\n",
            "Epoch 70/100\n",
            "1062/1062 - 2s - loss: 0.2339 - accuracy: 0.9037\n",
            "Epoch 71/100\n",
            "1062/1062 - 2s - loss: 0.2359 - accuracy: 0.9031\n",
            "Epoch 72/100\n",
            "1062/1062 - 2s - loss: 0.2345 - accuracy: 0.9030\n",
            "Epoch 73/100\n",
            "1062/1062 - 2s - loss: 0.2342 - accuracy: 0.9014\n",
            "Epoch 74/100\n",
            "1062/1062 - 3s - loss: 0.2346 - accuracy: 0.9033\n",
            "Epoch 75/100\n",
            "1062/1062 - 2s - loss: 0.2338 - accuracy: 0.9018\n",
            "Epoch 76/100\n",
            "1062/1062 - 2s - loss: 0.2345 - accuracy: 0.9022\n",
            "Epoch 77/100\n",
            "1062/1062 - 2s - loss: 0.2343 - accuracy: 0.9030\n",
            "Epoch 78/100\n",
            "1062/1062 - 2s - loss: 0.2359 - accuracy: 0.9020\n",
            "Epoch 79/100\n",
            "1062/1062 - 3s - loss: 0.2346 - accuracy: 0.9040\n",
            "Epoch 80/100\n",
            "1062/1062 - 2s - loss: 0.2335 - accuracy: 0.9033\n",
            "Epoch 81/100\n",
            "1062/1062 - 2s - loss: 0.2338 - accuracy: 0.9029\n",
            "Epoch 82/100\n",
            "1062/1062 - 2s - loss: 0.2357 - accuracy: 0.9003\n",
            "Epoch 83/100\n",
            "1062/1062 - 3s - loss: 0.2334 - accuracy: 0.9030\n",
            "Epoch 84/100\n",
            "1062/1062 - 3s - loss: 0.2312 - accuracy: 0.9036\n",
            "Epoch 85/100\n",
            "1062/1062 - 3s - loss: 0.2334 - accuracy: 0.9020\n",
            "Epoch 86/100\n",
            "1062/1062 - 3s - loss: 0.2323 - accuracy: 0.9042\n",
            "Epoch 87/100\n",
            "1062/1062 - 2s - loss: 0.2338 - accuracy: 0.9018\n",
            "Epoch 88/100\n",
            "1062/1062 - 2s - loss: 0.2326 - accuracy: 0.9036\n",
            "Epoch 89/100\n",
            "1062/1062 - 2s - loss: 0.2331 - accuracy: 0.9038\n",
            "Epoch 90/100\n",
            "1062/1062 - 2s - loss: 0.2325 - accuracy: 0.9045\n",
            "Epoch 91/100\n",
            "1062/1062 - 2s - loss: 0.2341 - accuracy: 0.9027\n",
            "Epoch 92/100\n",
            "1062/1062 - 2s - loss: 0.2334 - accuracy: 0.9031\n",
            "Epoch 93/100\n",
            "1062/1062 - 2s - loss: 0.2342 - accuracy: 0.9030\n",
            "Epoch 94/100\n",
            "1062/1062 - 2s - loss: 0.2330 - accuracy: 0.9024\n",
            "Epoch 95/100\n",
            "1062/1062 - 3s - loss: 0.2325 - accuracy: 0.9032\n",
            "Epoch 96/100\n",
            "1062/1062 - 3s - loss: 0.2322 - accuracy: 0.9031\n",
            "Epoch 97/100\n",
            "1062/1062 - 3s - loss: 0.2322 - accuracy: 0.9030\n",
            "Epoch 98/100\n",
            "1062/1062 - 2s - loss: 0.2314 - accuracy: 0.9047\n",
            "Epoch 99/100\n",
            "1062/1062 - 3s - loss: 0.2313 - accuracy: 0.9045\n",
            "Epoch 100/100\n",
            "1062/1062 - 2s - loss: 0.2318 - accuracy: 0.9043\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_5:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "266/266 - 0s - loss: 0.3006 - accuracy: 0.8775\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_6:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_6:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "1062/1062 - 2s - loss: 0.3500 - accuracy: 0.8479\n",
            "Epoch 2/100\n",
            "1062/1062 - 2s - loss: 0.3081 - accuracy: 0.8673\n",
            "Epoch 3/100\n",
            "1062/1062 - 2s - loss: 0.2943 - accuracy: 0.8736\n",
            "Epoch 4/100\n",
            "1062/1062 - 2s - loss: 0.2849 - accuracy: 0.8782\n",
            "Epoch 5/100\n",
            "1062/1062 - 2s - loss: 0.2807 - accuracy: 0.8802\n",
            "Epoch 6/100\n",
            "1062/1062 - 2s - loss: 0.2750 - accuracy: 0.8832\n",
            "Epoch 7/100\n",
            "1062/1062 - 2s - loss: 0.2753 - accuracy: 0.8838\n",
            "Epoch 8/100\n",
            "1062/1062 - 3s - loss: 0.2720 - accuracy: 0.8853\n",
            "Epoch 9/100\n",
            "1062/1062 - 2s - loss: 0.2673 - accuracy: 0.8867\n",
            "Epoch 10/100\n",
            "1062/1062 - 2s - loss: 0.2657 - accuracy: 0.8882\n",
            "Epoch 11/100\n",
            "1062/1062 - 2s - loss: 0.2626 - accuracy: 0.8894\n",
            "Epoch 12/100\n",
            "1062/1062 - 3s - loss: 0.2624 - accuracy: 0.8900\n",
            "Epoch 13/100\n",
            "1062/1062 - 2s - loss: 0.2602 - accuracy: 0.8907\n",
            "Epoch 14/100\n",
            "1062/1062 - 2s - loss: 0.2593 - accuracy: 0.8909\n",
            "Epoch 15/100\n",
            "1062/1062 - 2s - loss: 0.2574 - accuracy: 0.8914\n",
            "Epoch 16/100\n",
            "1062/1062 - 3s - loss: 0.2574 - accuracy: 0.8920\n",
            "Epoch 17/100\n",
            "1062/1062 - 3s - loss: 0.2567 - accuracy: 0.8930\n",
            "Epoch 18/100\n",
            "1062/1062 - 3s - loss: 0.2549 - accuracy: 0.8948\n",
            "Epoch 19/100\n",
            "1062/1062 - 2s - loss: 0.2565 - accuracy: 0.8922\n",
            "Epoch 20/100\n",
            "1062/1062 - 2s - loss: 0.2535 - accuracy: 0.8943\n",
            "Epoch 21/100\n",
            "1062/1062 - 2s - loss: 0.2519 - accuracy: 0.8934\n",
            "Epoch 22/100\n",
            "1062/1062 - 2s - loss: 0.2527 - accuracy: 0.8930\n",
            "Epoch 23/100\n",
            "1062/1062 - 2s - loss: 0.2504 - accuracy: 0.8960\n",
            "Epoch 24/100\n",
            "1062/1062 - 2s - loss: 0.2505 - accuracy: 0.8944\n",
            "Epoch 25/100\n",
            "1062/1062 - 2s - loss: 0.2497 - accuracy: 0.8954\n",
            "Epoch 26/100\n",
            "1062/1062 - 2s - loss: 0.2492 - accuracy: 0.8968\n",
            "Epoch 27/100\n",
            "1062/1062 - 3s - loss: 0.2489 - accuracy: 0.8941\n",
            "Epoch 28/100\n",
            "1062/1062 - 2s - loss: 0.2493 - accuracy: 0.8964\n",
            "Epoch 29/100\n",
            "1062/1062 - 2s - loss: 0.2472 - accuracy: 0.8967\n",
            "Epoch 30/100\n",
            "1062/1062 - 2s - loss: 0.2470 - accuracy: 0.8976\n",
            "Epoch 31/100\n",
            "1062/1062 - 2s - loss: 0.2459 - accuracy: 0.8969\n",
            "Epoch 32/100\n",
            "1062/1062 - 2s - loss: 0.2471 - accuracy: 0.8970\n",
            "Epoch 33/100\n",
            "1062/1062 - 2s - loss: 0.2443 - accuracy: 0.8988\n",
            "Epoch 34/100\n",
            "1062/1062 - 2s - loss: 0.2448 - accuracy: 0.8980\n",
            "Epoch 35/100\n",
            "1062/1062 - 2s - loss: 0.2442 - accuracy: 0.8984\n",
            "Epoch 36/100\n",
            "1062/1062 - 2s - loss: 0.2418 - accuracy: 0.8992\n",
            "Epoch 37/100\n",
            "1062/1062 - 2s - loss: 0.2447 - accuracy: 0.8970\n",
            "Epoch 38/100\n",
            "1062/1062 - 2s - loss: 0.2451 - accuracy: 0.8974\n",
            "Epoch 39/100\n",
            "1062/1062 - 2s - loss: 0.2443 - accuracy: 0.8975\n",
            "Epoch 40/100\n",
            "1062/1062 - 2s - loss: 0.2424 - accuracy: 0.8995\n",
            "Epoch 41/100\n",
            "1062/1062 - 2s - loss: 0.2414 - accuracy: 0.8999\n",
            "Epoch 42/100\n",
            "1062/1062 - 2s - loss: 0.2440 - accuracy: 0.8994\n",
            "Epoch 43/100\n",
            "1062/1062 - 2s - loss: 0.2427 - accuracy: 0.8987\n",
            "Epoch 44/100\n",
            "1062/1062 - 2s - loss: 0.2416 - accuracy: 0.8990\n",
            "Epoch 45/100\n",
            "1062/1062 - 2s - loss: 0.2413 - accuracy: 0.9003\n",
            "Epoch 46/100\n",
            "1062/1062 - 2s - loss: 0.2404 - accuracy: 0.9003\n",
            "Epoch 47/100\n",
            "1062/1062 - 3s - loss: 0.2406 - accuracy: 0.9003\n",
            "Epoch 48/100\n",
            "1062/1062 - 3s - loss: 0.2414 - accuracy: 0.8993\n",
            "Epoch 49/100\n",
            "1062/1062 - 3s - loss: 0.2403 - accuracy: 0.8999\n",
            "Epoch 50/100\n",
            "1062/1062 - 3s - loss: 0.2400 - accuracy: 0.9005\n",
            "Epoch 51/100\n",
            "1062/1062 - 2s - loss: 0.2404 - accuracy: 0.8998\n",
            "Epoch 52/100\n",
            "1062/1062 - 2s - loss: 0.2392 - accuracy: 0.9007\n",
            "Epoch 53/100\n",
            "1062/1062 - 2s - loss: 0.2400 - accuracy: 0.9012\n",
            "Epoch 54/100\n",
            "1062/1062 - 2s - loss: 0.2400 - accuracy: 0.8997\n",
            "Epoch 55/100\n",
            "1062/1062 - 2s - loss: 0.2382 - accuracy: 0.9024\n",
            "Epoch 56/100\n",
            "1062/1062 - 2s - loss: 0.2397 - accuracy: 0.8992\n",
            "Epoch 57/100\n",
            "1062/1062 - 2s - loss: 0.2390 - accuracy: 0.9010\n",
            "Epoch 58/100\n",
            "1062/1062 - 3s - loss: 0.2388 - accuracy: 0.9019\n",
            "Epoch 59/100\n",
            "1062/1062 - 3s - loss: 0.2396 - accuracy: 0.9004\n",
            "Epoch 60/100\n",
            "1062/1062 - 3s - loss: 0.2382 - accuracy: 0.9003\n",
            "Epoch 61/100\n",
            "1062/1062 - 3s - loss: 0.2386 - accuracy: 0.9006\n",
            "Epoch 62/100\n",
            "1062/1062 - 3s - loss: 0.2386 - accuracy: 0.9013\n",
            "Epoch 63/100\n",
            "1062/1062 - 2s - loss: 0.2387 - accuracy: 0.8999\n",
            "Epoch 64/100\n",
            "1062/1062 - 3s - loss: 0.2383 - accuracy: 0.9019\n",
            "Epoch 65/100\n",
            "1062/1062 - 2s - loss: 0.2375 - accuracy: 0.9020\n",
            "Epoch 66/100\n",
            "1062/1062 - 2s - loss: 0.2356 - accuracy: 0.9015\n",
            "Epoch 67/100\n",
            "1062/1062 - 2s - loss: 0.2386 - accuracy: 0.9022\n",
            "Epoch 68/100\n",
            "1062/1062 - 2s - loss: 0.2369 - accuracy: 0.9022\n",
            "Epoch 69/100\n",
            "1062/1062 - 2s - loss: 0.2359 - accuracy: 0.9014\n",
            "Epoch 70/100\n",
            "1062/1062 - 2s - loss: 0.2381 - accuracy: 0.9018\n",
            "Epoch 71/100\n",
            "1062/1062 - 2s - loss: 0.2367 - accuracy: 0.9031\n",
            "Epoch 72/100\n",
            "1062/1062 - 2s - loss: 0.2369 - accuracy: 0.9011\n",
            "Epoch 73/100\n",
            "1062/1062 - 2s - loss: 0.2370 - accuracy: 0.9020\n",
            "Epoch 74/100\n",
            "1062/1062 - 2s - loss: 0.2368 - accuracy: 0.9019\n",
            "Epoch 75/100\n",
            "1062/1062 - 3s - loss: 0.2354 - accuracy: 0.9028\n",
            "Epoch 76/100\n",
            "1062/1062 - 2s - loss: 0.2373 - accuracy: 0.9015\n",
            "Epoch 77/100\n",
            "1062/1062 - 2s - loss: 0.2368 - accuracy: 0.9021\n",
            "Epoch 78/100\n",
            "1062/1062 - 3s - loss: 0.2358 - accuracy: 0.9023\n",
            "Epoch 79/100\n",
            "1062/1062 - 3s - loss: 0.2360 - accuracy: 0.9035\n",
            "Epoch 80/100\n",
            "1062/1062 - 3s - loss: 0.2345 - accuracy: 0.9033\n",
            "Epoch 81/100\n",
            "1062/1062 - 3s - loss: 0.2343 - accuracy: 0.9038\n",
            "Epoch 82/100\n",
            "1062/1062 - 3s - loss: 0.2357 - accuracy: 0.9029\n",
            "Epoch 83/100\n",
            "1062/1062 - 2s - loss: 0.2352 - accuracy: 0.9027\n",
            "Epoch 84/100\n",
            "1062/1062 - 2s - loss: 0.2362 - accuracy: 0.9033\n",
            "Epoch 85/100\n",
            "1062/1062 - 3s - loss: 0.2356 - accuracy: 0.9022\n",
            "Epoch 86/100\n",
            "1062/1062 - 2s - loss: 0.2347 - accuracy: 0.9024\n",
            "Epoch 87/100\n",
            "1062/1062 - 2s - loss: 0.2351 - accuracy: 0.9029\n",
            "Epoch 88/100\n",
            "1062/1062 - 2s - loss: 0.2330 - accuracy: 0.9028\n",
            "Epoch 89/100\n",
            "1062/1062 - 2s - loss: 0.2340 - accuracy: 0.9045\n",
            "Epoch 90/100\n",
            "1062/1062 - 2s - loss: 0.2352 - accuracy: 0.9023\n",
            "Epoch 91/100\n",
            "1062/1062 - 2s - loss: 0.2333 - accuracy: 0.9046\n",
            "Epoch 92/100\n",
            "1062/1062 - 2s - loss: 0.2339 - accuracy: 0.9034\n",
            "Epoch 93/100\n",
            "1062/1062 - 2s - loss: 0.2358 - accuracy: 0.9033\n",
            "Epoch 94/100\n",
            "1062/1062 - 2s - loss: 0.2335 - accuracy: 0.9048\n",
            "Epoch 95/100\n",
            "1062/1062 - 2s - loss: 0.2344 - accuracy: 0.9027\n",
            "Epoch 96/100\n",
            "1062/1062 - 2s - loss: 0.2348 - accuracy: 0.9026\n",
            "Epoch 97/100\n",
            "1062/1062 - 2s - loss: 0.2327 - accuracy: 0.9042\n",
            "Epoch 98/100\n",
            "1062/1062 - 2s - loss: 0.2350 - accuracy: 0.9021\n",
            "Epoch 99/100\n",
            "1062/1062 - 2s - loss: 0.2322 - accuracy: 0.9045\n",
            "Epoch 100/100\n",
            "1062/1062 - 2s - loss: 0.2326 - accuracy: 0.9038\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 42445, 1900) for input Tensor(\"input_6:0\", shape=(None, 42445, 1900), dtype=float32), but it was called on an input with incompatible shape (None, 1900).\n",
            "266/266 - 0s - loss: 0.2821 - accuracy: 0.8813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbGCYLBqVAB0",
        "outputId": "2500a254-766d-4df0-83ce-471e4f490dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline: 88.37% (0.50%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNONrOjcxCKz"
      },
      "source": [
        "The below models are from https://towardsdatascience.com/protein-sequence-classification-99c80d0ad2df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO18oGqEIAf5"
      },
      "source": [
        "# Model 2: Bidirectional LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax797xXuwsAJ"
      },
      "source": [
        "Embedding\n",
        "* https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\n",
        "\n",
        "must specify 3 arguments:\n",
        "\n",
        "    input_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
        "    output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
        "    input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Neqnr6Hhv1QC",
        "outputId": "b709cbfb-3df9-4c2f-b5c9-f7c20e01b713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "input_shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(42445, 1900)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYBM8HsNx9PF",
        "outputId": "791ff847-01e4-4f71-f0f8-d029181cc3bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "len(input_shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrc6-83n0Pb2",
        "outputId": "6e6e4e82-db7b-46bc-d492-15c54f6177f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "x_input = Input( ( 1900,) )\n",
        "emb = Embedding(21, 128, input_length=1900)(x_input)\n",
        "emb"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'embedding/embedding_lookup/Identity_1:0' shape=(None, 1900, 128) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLR399sQ2-Ke"
      },
      "source": [
        ""
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4_WG1mW3cP_"
      },
      "source": [
        "Keras - ImportError: cannot import name 'CuDNNLSTM'\n",
        "\n",
        " To slove : for Tensorflow-2: You can just use LSTM with no activation function and it will automatically use the CuDNN version\n",
        "\n",
        "* https://stackoverflow.com/questions/48086014/keras-model-with-cudnnlstm-layers-doesnt-work-on-production-server\n",
        "* https://forums.developer.nvidia.com/t/importerror-cannot-import-name-cudnnlstm-from-tensorflow-keras-layers/82778\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3K_HqPYhfu_r",
        "outputId": "a3576aa3-aaa9-474e-c74e-dee4d0a1504f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "\n",
        "bi_rnn = Bidirectional( LSTM(64, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01)\n",
        "                        , bias_regularizer=l2(0.01)))(emb)\n",
        "x = Dropout(0.3)(bi_rnn)\n",
        "\n",
        "# softmax classifier\n",
        "x_output = Dense(1, activation='sigmoid', name='output_layer')(x)\n",
        "\n",
        "model_BLSTM = Model(inputs=x_input, outputs=x_output)\n",
        "model_BLSTM.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_BLSTM.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1900)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 1900, 128)         2688      \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               98816     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 101,633\n",
            "Trainable params: 101,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT66n3ZxfvD4",
        "outputId": "5fdc2e13-bf89-4b9f-8557-89736c1f4cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "checkpoint_BLSTM = ModelCheckpoint(\"/content/drive/My Drive/Colab Notebooks/best_model_BLSTM.hdf5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "history_BLSTM = model_BLSTM.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val),\n",
        "                     callbacks=[checkpoint_BLSTM], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/100\n",
            " 29/796 [>.............................] - ETA: 2:33:06 - loss: 4.4196 - accuracy: 0.5032"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyvMdMU1UqD0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtViR8TYUp4i"
      },
      "source": [
        "plot_history(history_BLSTM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR4uE8Gzfuw0"
      },
      "source": [
        "display_model_score(model_BLSTM,\n",
        "    [X_train, y_train],\n",
        "    [X_val, y_val],\n",
        "    [X_test, y_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Atr10FO4fuuE"
      },
      "source": [
        "y_probas = model_BLSTM.predict(X_test)\n",
        "threshold = 0.5\n",
        "y_predict = np.where(y_probas > threshold, 1, 0)\n",
        "print(classification_report(y_test, y_predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1yvj_MHIA6-"
      },
      "source": [
        "\n",
        "# Model 3: ProtCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPnhJuGfSDzP"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIIiKYP8IRYE"
      },
      "source": [
        "def residual_block(data, filters, d_rate):\n",
        "  \"\"\"\n",
        "  _data: input\n",
        "  _filters: convolution filters\n",
        "  _d_rate: dilation rate\n",
        "  \"\"\"\n",
        "\n",
        "  shortcut = data\n",
        "\n",
        "  bn1 = BatchNormalization()(data)\n",
        "  act1 = Activation('relu')(bn1)\n",
        "  conv1 = Conv1D(filters, 1, dilation_rate=d_rate, padding='same', kernel_regularizer=l2(0.001))(act1)\n",
        "\n",
        "  #bottleneck convolution\n",
        "  bn2 = BatchNormalization()(conv1)\n",
        "  act2 = Activation('relu')(bn2)\n",
        "  conv2 = Conv1D(filters, 3, padding='same', kernel_regularizer=l2(0.001))(act2)\n",
        "\n",
        "  #skip connection\n",
        "  x = Add()([conv2, shortcut])\n",
        "\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFyHyapJ4EKF"
      },
      "source": [
        "# model\n",
        "\n",
        "x_input = Input(shape=(1900, 1))\n",
        "\n",
        "#initial conv\n",
        "conv = Conv1D(128, 1, padding='same')(x_input) \n",
        "\n",
        "# per-residue representation\n",
        "res1 = residual_block(conv, 128, 2)\n",
        "res2 = residual_block(res1, 128, 3)\n",
        "\n",
        "x = MaxPooling1D(3)(res2)\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# softmax classifier\n",
        "x = Flatten()(x)\n",
        "x_output = Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0001))(x)\n",
        "\n",
        "model_ProtCNN = Model(inputs=x_input, outputs=x_output)\n",
        "model_ProtCNN.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_ProtCNN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMIGguFR4EHD"
      },
      "source": [
        "plot_history(history_ProtCNN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3iiV1sM4D6d"
      },
      "source": [
        "display_model_score(model_ProtCNN,\n",
        "    [X_train, y_train],\n",
        "    [X_val, y_val],\n",
        "    [X_test, y_test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZO0R1iHSm4T"
      },
      "source": [
        "y_probas = model_ProtCNN.predict(X_test)\n",
        "threshold = 0.5\n",
        "y_predict = np.where(y_probas > threshold, 1, 0)\n",
        "print(classification_report(y_test, y_predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fALQEI9g4Ejn"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh0Zvl9j4E38"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}