{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlktI-RZmX5h"
   },
   "outputs": [],
   "source": [
    "!pip install -Uqq fastbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r_WHUIJomX5u",
    "outputId": "968d231a-48ea-48f5-ffa9-9ed6ccbae282"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kongkitimanonk/.conda/envs/jupyter_NB/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "from fastai.basics import *\n",
    "from fastai import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "of2yXs6amX56"
   },
   "outputs": [],
   "source": [
    "reps_columns =['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15'\n",
    ",'16','17','18','19','20','21','22','23','24','25','26','27','28','29'\n",
    ",'30','31','32','33','34','35','36','37','38','39','40','41','42','43'\n",
    ",'44','45','46','47','48','49','50','51','52','53','54','55','56','57'\n",
    ",'58','59','60','61','62','63','64','65','66','67','68','69','70','71'\n",
    ",'72','73','74','75','76','77','78','79','80','81','82','83','84','85'\n",
    ",'86','87','88','89','90','91','92','93','94','95','96','97','98','99'\n",
    ",'100','101','102','103','104','105','106','107','108','109','110','111'\n",
    ",'112','113','114','115','116','117','118','119','120','121','122','123'\n",
    ",'124','125','126','127','128','129','130','131','132','133','134','135'\n",
    ",'136','137','138','139','140','141','142','143','144','145','146','147'\n",
    ",'148','149','150','151','152','153','154','155','156','157','158','159'\n",
    ",'160','161','162','163','164','165','166','167','168','169','170','171'\n",
    ",'172','173','174','175','176','177','178','179','180','181','182','183'\n",
    ",'184','185','186','187','188','189','190','191','192','193','194','195'\n",
    ",'196','197','198','199','200','201','202','203','204','205','206','207'\n",
    ",'208','209','210','211','212','213','214','215','216','217','218','219'\n",
    ",'220','221','222','223','224','225','226','227','228','229','230','231'\n",
    ",'232','233','234','235','236','237','238','239','240','241','242','243'\n",
    ",'244','245','246','247','248','249','250','251','252','253','254','255'\n",
    ",'256','257','258','259','260','261','262','263','264','265','266','267'\n",
    ",'268','269','270','271','272','273','274','275','276','277','278','279'\n",
    ",'280','281','282','283','284','285','286','287','288','289','290','291'\n",
    ",'292','293','294','295','296','297','298','299','300','301','302','303'\n",
    ",'304','305','306','307','308','309','310','311','312','313','314','315'\n",
    ",'316','317','318','319','320','321','322','323','324','325','326','327'\n",
    ",'328','329','330','331','332','333','334','335','336','337','338','339'\n",
    ",'340','341','342','343','344','345','346','347','348','349','350','351'\n",
    ",'352','353','354','355','356','357','358','359','360','361','362','363'\n",
    ",'364','365','366','367','368','369','370','371','372','373','374','375'\n",
    ",'376','377','378','379','380','381','382','383','384','385','386','387'\n",
    ",'388','389','390','391','392','393','394','395','396','397','398','399'\n",
    ",'400','401','402','403','404','405','406','407','408','409','410','411'\n",
    ",'412','413','414','415','416','417','418','419','420','421','422','423'\n",
    ",'424','425','426','427','428','429','430','431','432','433','434','435'\n",
    ",'436','437','438','439','440','441','442','443','444','445','446','447'\n",
    ",'448','449','450','451','452','453','454','455','456','457','458','459'\n",
    ",'460','461','462','463','464','465','466','467','468','469','470','471'\n",
    ",'472','473','474','475','476','477','478','479','480','481','482','483'\n",
    ",'484','485','486','487','488','489','490','491','492','493','494','495'\n",
    ",'496','497','498','499','500','501','502','503','504','505','506','507'\n",
    ",'508','509','510','511','512','513','514','515','516','517','518','519'\n",
    ",'520','521','522','523','524','525','526','527','528','529','530','531'\n",
    ",'532','533','534','535','536','537','538','539','540','541','542','543'\n",
    ",'544','545','546','547','548','549','550','551','552','553','554','555'\n",
    ",'556','557','558','559','560','561','562','563','564','565','566','567'\n",
    ",'568','569','570','571','572','573','574','575','576','577','578','579'\n",
    ",'580','581','582','583','584','585','586','587','588','589','590','591'\n",
    ",'592','593','594','595','596','597','598','599','600','601','602','603'\n",
    ",'604','605','606','607','608','609','610','611','612','613','614','615'\n",
    ",'616','617','618','619','620','621','622','623','624','625','626','627'\n",
    ",'628','629','630','631','632','633','634','635','636','637','638','639'\n",
    ",'640','641','642','643','644','645','646','647','648','649','650','651'\n",
    ",'652','653','654','655','656','657','658','659','660','661','662','663'\n",
    ",'664','665','666','667','668','669','670','671','672','673','674','675'\n",
    ",'676','677','678','679','680','681','682','683','684','685','686','687'\n",
    ",'688','689','690','691','692','693','694','695','696','697','698','699'\n",
    ",'700','701','702','703','704','705','706','707','708','709','710','711'\n",
    ",'712','713','714','715','716','717','718','719','720','721','722','723'\n",
    ",'724','725','726','727','728','729','730','731','732','733','734','735'\n",
    ",'736','737','738','739','740','741','742','743','744','745','746','747'\n",
    ",'748','749','750','751','752','753','754','755','756','757','758','759'\n",
    ",'760','761','762','763','764','765','766','767','768','769','770','771'\n",
    ",'772','773','774','775','776','777','778','779','780','781','782','783'\n",
    ",'784','785','786','787','788','789','790','791','792','793','794','795'\n",
    ",'796','797','798','799','800','801','802','803','804','805','806','807'\n",
    ",'808','809','810','811','812','813','814','815','816','817','818','819'\n",
    ",'820','821','822','823','824','825','826','827','828','829','830','831'\n",
    ",'832','833','834','835','836','837','838','839','840','841','842','843'\n",
    ",'844','845','846','847','848','849','850','851','852','853','854','855'\n",
    ",'856','857','858','859','860','861','862','863','864','865','866','867'\n",
    ",'868','869','870','871','872','873','874','875','876','877','878','879'\n",
    ",'880','881','882','883','884','885','886','887','888','889','890','891'\n",
    ",'892','893','894','895','896','897','898','899','900','901','902','903'\n",
    ",'904','905','906','907','908','909','910','911','912','913','914','915'\n",
    ",'916','917','918','919','920','921','922','923','924','925','926','927'\n",
    ",'928','929','930','931','932','933','934','935','936','937','938','939'\n",
    ",'940','941','942','943','944','945','946','947','948','949','950','951'\n",
    ",'952','953','954','955','956','957','958','959','960','961','962','963'\n",
    ",'964','965','966','967','968','969','970','971','972','973','974','975'\n",
    ",'976','977','978','979','980','981','982','983','984','985','986','987'\n",
    ",'988','989','990','991','992','993','994','995','996','997','998','999'\n",
    ",'1000','1001','1002','1003','1004','1005','1006','1007','1008','1009'\n",
    ",'1010','1011','1012','1013','1014','1015','1016','1017','1018','1019'\n",
    ",'1020','1021','1022','1023','1024','1025','1026','1027','1028','1029'\n",
    ",'1030','1031','1032','1033','1034','1035','1036','1037','1038','1039'\n",
    ",'1040','1041','1042','1043','1044','1045','1046','1047','1048','1049'\n",
    ",'1050','1051','1052','1053','1054','1055','1056','1057','1058','1059'\n",
    ",'1060','1061','1062','1063','1064','1065','1066','1067','1068','1069'\n",
    ",'1070','1071','1072','1073','1074','1075','1076','1077','1078','1079'\n",
    ",'1080','1081','1082','1083','1084','1085','1086','1087','1088','1089'\n",
    ",'1090','1091','1092','1093','1094','1095','1096','1097','1098','1099'\n",
    ",'1100','1101','1102','1103','1104','1105','1106','1107','1108','1109'\n",
    ",'1110','1111','1112','1113','1114','1115','1116','1117','1118','1119'\n",
    ",'1120','1121','1122','1123','1124','1125','1126','1127','1128','1129'\n",
    ",'1130','1131','1132','1133','1134','1135','1136','1137','1138','1139'\n",
    ",'1140','1141','1142','1143','1144','1145','1146','1147','1148','1149'\n",
    ",'1150','1151','1152','1153','1154','1155','1156','1157','1158','1159'\n",
    ",'1160','1161','1162','1163','1164','1165','1166','1167','1168','1169'\n",
    ",'1170','1171','1172','1173','1174','1175','1176','1177','1178','1179'\n",
    ",'1180','1181','1182','1183','1184','1185','1186','1187','1188','1189'\n",
    ",'1190','1191','1192','1193','1194','1195','1196','1197','1198','1199'\n",
    ",'1200','1201','1202','1203','1204','1205','1206','1207','1208','1209'\n",
    ",'1210','1211','1212','1213','1214','1215','1216','1217','1218','1219'\n",
    ",'1220','1221','1222','1223','1224','1225','1226','1227','1228','1229'\n",
    ",'1230','1231','1232','1233','1234','1235','1236','1237','1238','1239'\n",
    ",'1240','1241','1242','1243','1244','1245','1246','1247','1248','1249'\n",
    ",'1250','1251','1252','1253','1254','1255','1256','1257','1258','1259'\n",
    ",'1260','1261','1262','1263','1264','1265','1266','1267','1268','1269'\n",
    ",'1270','1271','1272','1273','1274','1275','1276','1277','1278','1279'\n",
    ",'1280','1281','1282','1283','1284','1285','1286','1287','1288','1289'\n",
    ",'1290','1291','1292','1293','1294','1295','1296','1297','1298','1299'\n",
    ",'1300','1301','1302','1303','1304','1305','1306','1307','1308','1309'\n",
    ",'1310','1311','1312','1313','1314','1315','1316','1317','1318','1319'\n",
    ",'1320','1321','1322','1323','1324','1325','1326','1327','1328','1329'\n",
    ",'1330','1331','1332','1333','1334','1335','1336','1337','1338','1339'\n",
    ",'1340','1341','1342','1343','1344','1345','1346','1347','1348','1349'\n",
    ",'1350','1351','1352','1353','1354','1355','1356','1357','1358','1359'\n",
    ",'1360','1361','1362','1363','1364','1365','1366','1367','1368','1369'\n",
    ",'1370','1371','1372','1373','1374','1375','1376','1377','1378','1379'\n",
    ",'1380','1381','1382','1383','1384','1385','1386','1387','1388','1389'\n",
    ",'1390','1391','1392','1393','1394','1395','1396','1397','1398','1399'\n",
    ",'1400','1401','1402','1403','1404','1405','1406','1407','1408','1409'\n",
    ",'1410','1411','1412','1413','1414','1415','1416','1417','1418','1419'\n",
    ",'1420','1421','1422','1423','1424','1425','1426','1427','1428','1429'\n",
    ",'1430','1431','1432','1433','1434','1435','1436','1437','1438','1439'\n",
    ",'1440','1441','1442','1443','1444','1445','1446','1447','1448','1449'\n",
    ",'1450','1451','1452','1453','1454','1455','1456','1457','1458','1459'\n",
    ",'1460','1461','1462','1463','1464','1465','1466','1467','1468','1469'\n",
    ",'1470','1471','1472','1473','1474','1475','1476','1477','1478','1479'\n",
    ",'1480','1481','1482','1483','1484','1485','1486','1487','1488','1489'\n",
    ",'1490','1491','1492','1493','1494','1495','1496','1497','1498','1499'\n",
    ",'1500','1501','1502','1503','1504','1505','1506','1507','1508','1509'\n",
    ",'1510','1511','1512','1513','1514','1515','1516','1517','1518','1519'\n",
    ",'1520','1521','1522','1523','1524','1525','1526','1527','1528','1529'\n",
    ",'1530','1531','1532','1533','1534','1535','1536','1537','1538','1539'\n",
    ",'1540','1541','1542','1543','1544','1545','1546','1547','1548','1549'\n",
    ",'1550','1551','1552','1553','1554','1555','1556','1557','1558','1559'\n",
    ",'1560','1561','1562','1563','1564','1565','1566','1567','1568','1569'\n",
    ",'1570','1571','1572','1573','1574','1575','1576','1577','1578','1579'\n",
    ",'1580','1581','1582','1583','1584','1585','1586','1587','1588','1589'\n",
    ",'1590','1591','1592','1593','1594','1595','1596','1597','1598','1599'\n",
    ",'1600','1601','1602','1603','1604','1605','1606','1607','1608','1609'\n",
    ",'1610','1611','1612','1613','1614','1615','1616','1617','1618','1619'\n",
    ",'1620','1621','1622','1623','1624','1625','1626','1627','1628','1629'\n",
    ",'1630','1631','1632','1633','1634','1635','1636','1637','1638','1639'\n",
    ",'1640','1641','1642','1643','1644','1645','1646','1647','1648','1649'\n",
    ",'1650','1651','1652','1653','1654','1655','1656','1657','1658','1659'\n",
    ",'1660','1661','1662','1663','1664','1665','1666','1667','1668','1669'\n",
    ",'1670','1671','1672','1673','1674','1675','1676','1677','1678','1679'\n",
    ",'1680','1681','1682','1683','1684','1685','1686','1687','1688','1689'\n",
    ",'1690','1691','1692','1693','1694','1695','1696','1697','1698','1699'\n",
    ",'1700','1701','1702','1703','1704','1705','1706','1707','1708','1709'\n",
    ",'1710','1711','1712','1713','1714','1715','1716','1717','1718','1719'\n",
    ",'1720','1721','1722','1723','1724','1725','1726','1727','1728','1729'\n",
    ",'1730','1731','1732','1733','1734','1735','1736','1737','1738','1739'\n",
    ",'1740','1741','1742','1743','1744','1745','1746','1747','1748','1749'\n",
    ",'1750','1751','1752','1753','1754','1755','1756','1757','1758','1759'\n",
    ",'1760','1761','1762','1763','1764','1765','1766','1767','1768','1769'\n",
    ",'1770','1771','1772','1773','1774','1775','1776','1777','1778','1779'\n",
    ",'1780','1781','1782','1783','1784','1785','1786','1787','1788','1789'\n",
    ",'1790','1791','1792','1793','1794','1795','1796','1797','1798','1799'\n",
    ",'1800','1801','1802','1803','1804','1805','1806','1807','1808','1809'\n",
    ",'1810','1811','1812','1813','1814','1815','1816','1817','1818','1819'\n",
    ",'1820','1821','1822','1823','1824','1825','1826','1827','1828','1829'\n",
    ",'1830','1831','1832','1833','1834','1835','1836','1837','1838','1839'\n",
    ",'1840','1841','1842','1843','1844','1845','1846','1847','1848','1849'\n",
    ",'1850','1851','1852','1853','1854','1855','1856','1857','1858','1859'\n",
    ",'1860','1861','1862','1863','1864','1865','1866','1867','1868','1869'\n",
    ",'1870','1871','1872','1873','1874','1875','1876','1877','1878','1879'\n",
    ",'1880','1881','1882','1883','1884','1885','1886','1887','1888','1889'\n",
    ",'1890','1891','1892','1893','1894','1895','1896','1897','1898','1899']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZascbWaBmX6K"
   },
   "outputs": [],
   "source": [
    "# load the baseline model\n",
    "#learn.load('TubularLearner.fastAI._stage1')\n",
    "deployed_path = \"/home/kongkitimanonk/SCRATCH_NOBAK/tools/TabularLearner.AMPS_model.fastAI\"\n",
    "#deployed_path = \"TubularLearner.fastAI._stage2.pth\"\n",
    "learner = load_learner(deployed_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularModel(\n",
       "  (embeds): ModuleList()\n",
       "  (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(1900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): LinBnDrop(\n",
       "      (0): BatchNorm1d(1900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=1900, out_features=200, bias=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): LinBnDrop(\n",
       "      (0): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Linear(in_features=200, out_features=100, bias=False)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): LinBnDrop(\n",
       "      (0): Linear(in_features=100, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoTrGO6HmX6E"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_pickle(\"/home/kongkitimanonk/SCRATCH_NOBAK/phase3/DECockroach.len10.pkl\")\n",
    "#import pickle5 as pickle\n",
    "#with open( \"/home/kongkitimanonk/SCRATCH_NOBAK/pahase3/DECockroach.len10.pkl\", 'rb') as file:\n",
    "#    df = pickle.load(file)\n",
    "ready_df =df[[\"reps\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.01895383559167385, -0.054594799876213074, -0.0176044050604105, -0.00035888608545064926, -0.016080688685178757, 0.05952286347746849, -0.5018223524093628, -0.06243113428354263, -0.006974542047828436, 0.08264549821615219, -0.03869000822305679, -0.013411765918135643, 0.01708482764661312, 0.039064135402441025, 0.005318742711097002, 0.004067797679454088, 0.0297564547508955, -0.007240318227559328, -0.31639137864112854, -0.012492993846535683, 0.0019268222386017442, 0.009401950053870678, 0.14766307175159454, -0.3194642961025238, -0.010848816484212875, -0.2075124830007553, 0.031620122492313385, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.012356346473097801, 0.019348515197634697, 0.035544611513614655, -0.011909482069313526, -0.5936421751976013, 0.04223024472594261, -0.3983590006828308, -0.004493099171668291, -0.008300136774778366, 0.20358985662460327, 0.0011988065671175718, 0.0011746485251933336, 0.0995621606707573, -0.10297778248786926, 0.004824614617973566, 0.002194415545091033, 0.02810785360634327, -0.05018666014075279, 0.08977299183607101, -0.021542629227042198, -0.013711037114262581, 0.0824570506811142, 0.08610523492097855, -0.1218748688697815, -0.015778860077261925, -0.21678613126277924, -0.09146621823310852, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.039828237146139145, -0.024318987503647804, -0.008493148721754551, 0.004779402632266283, -0.06590026617050171, 0.02754816971719265, -0.4465098977088928, 0.0030789088923484087, -0.009968811646103859, 0.10137029737234116, 0.0167732834815979, -0.013892472721636295, 0.017564253881573677, -0.2908075451850891, 0.008216136135160923, 0.006583749782294035, 0.05314081534743309, -0.008199073374271393, -0.5812379121780396, -0.05488056689500809, 0.005561999510973692, 0.013892403803765774, 0.1132737323641777, -0.37167686223983765, -0.0015110609820112586, -0.2027391791343689, 0.04256262630224228, 0.020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0.010476253926753998, -0.03700648248195648, 0.0020506682340055704, 0.0008362423977814615, -0.11321403831243515, -0.004195282235741615, -0.4309786260128021, -0.0017584094312041998, -0.0065959542989730835, 0.1788569986820221, -0.22210964560508728, 0.0013327669585123658, -0.004801499657332897, 0.19922678172588348, 0.004026360344141722, 0.002824342343956232, 0.021703999489545822, -0.009478079155087471, -0.5242171883583069, -0.01411170233041048, -0.003689449978992343, 0.029495203867554665, 0.03415496274828911, -0.23880107700824738, -0.26177066564559937, 0.0029924949631094933, -0.00069670582888...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.0023850733414292336, -0.018008306622505188, 0.03165193274617195, -0.02701396681368351, 0.28968802094459534, 0.01543043740093708, -0.0447518415749073, -0.01088088471442461, -0.004008559510111809, -0.01747647486627102, -0.004662697669118643, 0.0003848073538392782, 0.03316676244139671, -0.046583183109760284, 0.008240465074777603, 0.002088269218802452, 0.0005078897229395807, -0.13827595114707947, 0.1839771866798401, -0.0036198708694428205, -0.02150820568203926, 0.005153958220034838, 0.14349135756492615, 0.04282233864068985, 0.023120597004890442, 0.09971153736114502, 0.2094339281320572, 0.01...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26356</th>\n",
       "      <td>[0.01887628249824047, -0.1597774177789688, 0.01881096139550209, 0.01751003786921501, -0.200810045003891, 0.04461025074124336, -0.37307584285736084, 0.008507090620696545, -0.022263677790760994, 0.2648080885410309, 0.22576771676540375, 0.011620165780186653, 0.04384046420454979, 0.04857516661286354, 0.021235767751932144, 0.017245551571249962, 0.03522114083170891, -0.15098680555820465, 0.13829192519187927, -0.07057751715183258, 0.0035949628800153732, 0.03866252303123474, 0.042004287242889404, 0.010325318202376366, -0.007851218804717064, -0.20717376470565796, 0.09105156362056732, 0.032682090997...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26357</th>\n",
       "      <td>[0.023860616609454155, -0.11182335019111633, -0.011121172457933426, 0.02152729220688343, -0.12321177870035172, 0.016969189047813416, -0.3578570485115051, 0.004920703358948231, -0.03336606174707413, 0.05210229754447937, 0.2930184602737427, 0.018214378505945206, 0.04739605635404587, 0.26733675599098206, 0.021457700058817863, 0.017539402469992638, 0.045082125812768936, -0.04124514013528824, -0.10002901405096054, -0.0502447672188282, 0.01409282349050045, 0.02582470141351223, 0.05066731572151184, -0.13248524069786072, 0.06069105491042137, 0.04038538038730621, 0.09897836297750473, 0.029832459986...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26358</th>\n",
       "      <td>[0.0220795851200819, -0.0032089976593852043, -0.003715106751769781, 0.02193770557641983, -0.02561749890446663, 0.031697485595941544, -0.46136191487312317, -0.005910424515604973, -0.034239400178194046, 0.19234508275985718, 0.08227146416902542, 0.023954378440976143, 0.0428282767534256, 0.22740855813026428, 0.02359062246978283, 0.01734965294599533, 0.04509333148598671, 0.00021410451154224575, -0.03067576140165329, -0.021782230585813522, 0.0157422237098217, 0.04272513836622238, 0.0713859274983406, -0.25268757343292236, 0.025774510577321053, 0.06818567961454391, 0.13879913091659546, 0.057338073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26359</th>\n",
       "      <td>[0.024670349434018135, -0.04348437860608101, -0.0043207816779613495, 0.021305741742253304, -0.07873472571372986, 0.05777618661522865, -0.5186329483985901, -0.009307102300226688, -0.02659173496067524, 0.26067882776260376, 0.27467864751815796, 0.015328658744692802, 0.044852737337350845, 0.14176872372627258, 0.02107737585902214, 0.016657933592796326, 0.027819132432341576, -0.0062135858461260796, 0.07013368606567383, -0.02216959185898304, 0.016140151768922806, 0.03250237554311752, 0.07744377106428146, -0.09691926836967468, 0.04939504340291023, -0.015300760976970196, 0.07653413712978363, 0.0427...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26360</th>\n",
       "      <td>[0.01750006154179573, -0.30035871267318726, 0.021210096776485443, 0.015011887066066265, -0.469089537858963, 0.01999136246740818, -0.30817773938179016, 0.04393145814538002, -0.017987117171287537, 0.22049850225448608, 0.6349100470542908, -0.051537755876779556, 0.043352991342544556, 0.055509280413389206, 0.015454576350748539, 0.009915020316839218, 0.05263889580965042, -0.09686477482318878, 0.03124208189547062, -0.14327391982078552, -0.01236592885106802, 0.06620960682630539, 0.03337473422288895, -0.055267177522182465, 0.006277650129050016, -0.15837351977825165, 0.05801704153418541, 0.123163536...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26055 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          reps\n",
       "0      [0.01895383559167385, -0.054594799876213074, -0.0176044050604105, -0.00035888608545064926, -0.016080688685178757, 0.05952286347746849, -0.5018223524093628, -0.06243113428354263, -0.006974542047828436, 0.08264549821615219, -0.03869000822305679, -0.013411765918135643, 0.01708482764661312, 0.039064135402441025, 0.005318742711097002, 0.004067797679454088, 0.0297564547508955, -0.007240318227559328, -0.31639137864112854, -0.012492993846535683, 0.0019268222386017442, 0.009401950053870678, 0.14766307175159454, -0.3194642961025238, -0.010848816484212875, -0.2075124830007553, 0.031620122492313385, 0...\n",
       "1      [0.012356346473097801, 0.019348515197634697, 0.035544611513614655, -0.011909482069313526, -0.5936421751976013, 0.04223024472594261, -0.3983590006828308, -0.004493099171668291, -0.008300136774778366, 0.20358985662460327, 0.0011988065671175718, 0.0011746485251933336, 0.0995621606707573, -0.10297778248786926, 0.004824614617973566, 0.002194415545091033, 0.02810785360634327, -0.05018666014075279, 0.08977299183607101, -0.021542629227042198, -0.013711037114262581, 0.0824570506811142, 0.08610523492097855, -0.1218748688697815, -0.015778860077261925, -0.21678613126277924, -0.09146621823310852, -0.02...\n",
       "2      [0.039828237146139145, -0.024318987503647804, -0.008493148721754551, 0.004779402632266283, -0.06590026617050171, 0.02754816971719265, -0.4465098977088928, 0.0030789088923484087, -0.009968811646103859, 0.10137029737234116, 0.0167732834815979, -0.013892472721636295, 0.017564253881573677, -0.2908075451850891, 0.008216136135160923, 0.006583749782294035, 0.05314081534743309, -0.008199073374271393, -0.5812379121780396, -0.05488056689500809, 0.005561999510973692, 0.013892403803765774, 0.1132737323641777, -0.37167686223983765, -0.0015110609820112586, -0.2027391791343689, 0.04256262630224228, 0.020...\n",
       "3      [0.010476253926753998, -0.03700648248195648, 0.0020506682340055704, 0.0008362423977814615, -0.11321403831243515, -0.004195282235741615, -0.4309786260128021, -0.0017584094312041998, -0.0065959542989730835, 0.1788569986820221, -0.22210964560508728, 0.0013327669585123658, -0.004801499657332897, 0.19922678172588348, 0.004026360344141722, 0.002824342343956232, 0.021703999489545822, -0.009478079155087471, -0.5242171883583069, -0.01411170233041048, -0.003689449978992343, 0.029495203867554665, 0.03415496274828911, -0.23880107700824738, -0.26177066564559937, 0.0029924949631094933, -0.00069670582888...\n",
       "4      [0.0023850733414292336, -0.018008306622505188, 0.03165193274617195, -0.02701396681368351, 0.28968802094459534, 0.01543043740093708, -0.0447518415749073, -0.01088088471442461, -0.004008559510111809, -0.01747647486627102, -0.004662697669118643, 0.0003848073538392782, 0.03316676244139671, -0.046583183109760284, 0.008240465074777603, 0.002088269218802452, 0.0005078897229395807, -0.13827595114707947, 0.1839771866798401, -0.0036198708694428205, -0.02150820568203926, 0.005153958220034838, 0.14349135756492615, 0.04282233864068985, 0.023120597004890442, 0.09971153736114502, 0.2094339281320572, 0.01...\n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ...\n",
       "26356  [0.01887628249824047, -0.1597774177789688, 0.01881096139550209, 0.01751003786921501, -0.200810045003891, 0.04461025074124336, -0.37307584285736084, 0.008507090620696545, -0.022263677790760994, 0.2648080885410309, 0.22576771676540375, 0.011620165780186653, 0.04384046420454979, 0.04857516661286354, 0.021235767751932144, 0.017245551571249962, 0.03522114083170891, -0.15098680555820465, 0.13829192519187927, -0.07057751715183258, 0.0035949628800153732, 0.03866252303123474, 0.042004287242889404, 0.010325318202376366, -0.007851218804717064, -0.20717376470565796, 0.09105156362056732, 0.032682090997...\n",
       "26357  [0.023860616609454155, -0.11182335019111633, -0.011121172457933426, 0.02152729220688343, -0.12321177870035172, 0.016969189047813416, -0.3578570485115051, 0.004920703358948231, -0.03336606174707413, 0.05210229754447937, 0.2930184602737427, 0.018214378505945206, 0.04739605635404587, 0.26733675599098206, 0.021457700058817863, 0.017539402469992638, 0.045082125812768936, -0.04124514013528824, -0.10002901405096054, -0.0502447672188282, 0.01409282349050045, 0.02582470141351223, 0.05066731572151184, -0.13248524069786072, 0.06069105491042137, 0.04038538038730621, 0.09897836297750473, 0.029832459986...\n",
       "26358  [0.0220795851200819, -0.0032089976593852043, -0.003715106751769781, 0.02193770557641983, -0.02561749890446663, 0.031697485595941544, -0.46136191487312317, -0.005910424515604973, -0.034239400178194046, 0.19234508275985718, 0.08227146416902542, 0.023954378440976143, 0.0428282767534256, 0.22740855813026428, 0.02359062246978283, 0.01734965294599533, 0.04509333148598671, 0.00021410451154224575, -0.03067576140165329, -0.021782230585813522, 0.0157422237098217, 0.04272513836622238, 0.0713859274983406, -0.25268757343292236, 0.025774510577321053, 0.06818567961454391, 0.13879913091659546, 0.057338073...\n",
       "26359  [0.024670349434018135, -0.04348437860608101, -0.0043207816779613495, 0.021305741742253304, -0.07873472571372986, 0.05777618661522865, -0.5186329483985901, -0.009307102300226688, -0.02659173496067524, 0.26067882776260376, 0.27467864751815796, 0.015328658744692802, 0.044852737337350845, 0.14176872372627258, 0.02107737585902214, 0.016657933592796326, 0.027819132432341576, -0.0062135858461260796, 0.07013368606567383, -0.02216959185898304, 0.016140151768922806, 0.03250237554311752, 0.07744377106428146, -0.09691926836967468, 0.04939504340291023, -0.015300760976970196, 0.07653413712978363, 0.0427...\n",
       "26360  [0.01750006154179573, -0.30035871267318726, 0.021210096776485443, 0.015011887066066265, -0.469089537858963, 0.01999136246740818, -0.30817773938179016, 0.04393145814538002, -0.017987117171287537, 0.22049850225448608, 0.6349100470542908, -0.051537755876779556, 0.043352991342544556, 0.055509280413389206, 0.015454576350748539, 0.009915020316839218, 0.05263889580965042, -0.09686477482318878, 0.03124208189547062, -0.14327391982078552, -0.01236592885106802, 0.06620960682630539, 0.03337473422288895, -0.055267177522182465, 0.006277650129050016, -0.15837351977825165, 0.05801704153418541, 0.123163536...\n",
       "\n",
       "[26055 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ready_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocpMvCNJmX6R"
   },
   "source": [
    "## Chanage format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "raBn6r7zmX6T",
    "outputId": "a4c742de-63a8-4e22-fe0d-d5137e5374e6"
   },
   "outputs": [],
   "source": [
    "df_new = ready_df.reps.apply(pd.Series).astype(np.float64)\n",
    "df_new.columns = df_new.columns.astype(str)\n",
    "#to = TabularPandas(df_new , cont_names =reps_columns, y_block = CategoryBlock, splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1890</th>\n",
       "      <th>1891</th>\n",
       "      <th>1892</th>\n",
       "      <th>1893</th>\n",
       "      <th>1894</th>\n",
       "      <th>1895</th>\n",
       "      <th>1896</th>\n",
       "      <th>1897</th>\n",
       "      <th>1898</th>\n",
       "      <th>1899</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018954</td>\n",
       "      <td>-0.054595</td>\n",
       "      <td>-0.017604</td>\n",
       "      <td>-0.000359</td>\n",
       "      <td>-0.016081</td>\n",
       "      <td>0.059523</td>\n",
       "      <td>-0.501822</td>\n",
       "      <td>-0.062431</td>\n",
       "      <td>-0.006975</td>\n",
       "      <td>0.082645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012340</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>-0.037209</td>\n",
       "      <td>0.013662</td>\n",
       "      <td>-0.023809</td>\n",
       "      <td>-0.080424</td>\n",
       "      <td>0.109916</td>\n",
       "      <td>0.043068</td>\n",
       "      <td>0.481828</td>\n",
       "      <td>-0.140297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.012356</td>\n",
       "      <td>0.019349</td>\n",
       "      <td>0.035545</td>\n",
       "      <td>-0.011909</td>\n",
       "      <td>-0.593642</td>\n",
       "      <td>0.042230</td>\n",
       "      <td>-0.398359</td>\n",
       "      <td>-0.004493</td>\n",
       "      <td>-0.008300</td>\n",
       "      <td>0.203590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.052349</td>\n",
       "      <td>0.069966</td>\n",
       "      <td>-0.025274</td>\n",
       "      <td>0.016474</td>\n",
       "      <td>-0.088788</td>\n",
       "      <td>0.042309</td>\n",
       "      <td>-0.001724</td>\n",
       "      <td>0.056446</td>\n",
       "      <td>0.450799</td>\n",
       "      <td>0.085444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039828</td>\n",
       "      <td>-0.024319</td>\n",
       "      <td>-0.008493</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>-0.065900</td>\n",
       "      <td>0.027548</td>\n",
       "      <td>-0.446510</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>-0.009969</td>\n",
       "      <td>0.101370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011978</td>\n",
       "      <td>0.045198</td>\n",
       "      <td>-0.029640</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>-0.029539</td>\n",
       "      <td>-0.107355</td>\n",
       "      <td>0.074577</td>\n",
       "      <td>0.032368</td>\n",
       "      <td>0.722810</td>\n",
       "      <td>-0.157371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010476</td>\n",
       "      <td>-0.037006</td>\n",
       "      <td>0.002051</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>-0.113214</td>\n",
       "      <td>-0.004195</td>\n",
       "      <td>-0.430979</td>\n",
       "      <td>-0.001758</td>\n",
       "      <td>-0.006596</td>\n",
       "      <td>0.178857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006351</td>\n",
       "      <td>0.012263</td>\n",
       "      <td>-0.027498</td>\n",
       "      <td>0.040094</td>\n",
       "      <td>-0.036113</td>\n",
       "      <td>-0.013577</td>\n",
       "      <td>-0.024200</td>\n",
       "      <td>0.011160</td>\n",
       "      <td>0.923180</td>\n",
       "      <td>0.012899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002385</td>\n",
       "      <td>-0.018008</td>\n",
       "      <td>0.031652</td>\n",
       "      <td>-0.027014</td>\n",
       "      <td>0.289688</td>\n",
       "      <td>0.015430</td>\n",
       "      <td>-0.044752</td>\n",
       "      <td>-0.010881</td>\n",
       "      <td>-0.004009</td>\n",
       "      <td>-0.017476</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003778</td>\n",
       "      <td>-0.035071</td>\n",
       "      <td>-0.254518</td>\n",
       "      <td>0.003655</td>\n",
       "      <td>-0.109212</td>\n",
       "      <td>0.116663</td>\n",
       "      <td>0.320068</td>\n",
       "      <td>0.094852</td>\n",
       "      <td>-0.064797</td>\n",
       "      <td>-0.009261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26356</th>\n",
       "      <td>0.018876</td>\n",
       "      <td>-0.159777</td>\n",
       "      <td>0.018811</td>\n",
       "      <td>0.017510</td>\n",
       "      <td>-0.200810</td>\n",
       "      <td>0.044610</td>\n",
       "      <td>-0.373076</td>\n",
       "      <td>0.008507</td>\n",
       "      <td>-0.022264</td>\n",
       "      <td>0.264808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>0.104505</td>\n",
       "      <td>-0.034343</td>\n",
       "      <td>0.024809</td>\n",
       "      <td>-0.090041</td>\n",
       "      <td>0.036515</td>\n",
       "      <td>0.066031</td>\n",
       "      <td>-0.075598</td>\n",
       "      <td>0.126426</td>\n",
       "      <td>0.031520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26357</th>\n",
       "      <td>0.023861</td>\n",
       "      <td>-0.111823</td>\n",
       "      <td>-0.011121</td>\n",
       "      <td>0.021527</td>\n",
       "      <td>-0.123212</td>\n",
       "      <td>0.016969</td>\n",
       "      <td>-0.357857</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>-0.033366</td>\n",
       "      <td>0.052102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054554</td>\n",
       "      <td>-0.064859</td>\n",
       "      <td>-0.112490</td>\n",
       "      <td>0.034962</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>0.007011</td>\n",
       "      <td>0.074737</td>\n",
       "      <td>-0.008404</td>\n",
       "      <td>0.156601</td>\n",
       "      <td>0.068240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26358</th>\n",
       "      <td>0.022080</td>\n",
       "      <td>-0.003209</td>\n",
       "      <td>-0.003715</td>\n",
       "      <td>0.021938</td>\n",
       "      <td>-0.025617</td>\n",
       "      <td>0.031697</td>\n",
       "      <td>-0.461362</td>\n",
       "      <td>-0.005910</td>\n",
       "      <td>-0.034239</td>\n",
       "      <td>0.192345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034636</td>\n",
       "      <td>0.073107</td>\n",
       "      <td>-0.159889</td>\n",
       "      <td>0.020721</td>\n",
       "      <td>0.013630</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.226136</td>\n",
       "      <td>0.117120</td>\n",
       "      <td>0.307682</td>\n",
       "      <td>0.020546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26359</th>\n",
       "      <td>0.024670</td>\n",
       "      <td>-0.043484</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>0.021306</td>\n",
       "      <td>-0.078735</td>\n",
       "      <td>0.057776</td>\n",
       "      <td>-0.518633</td>\n",
       "      <td>-0.009307</td>\n",
       "      <td>-0.026592</td>\n",
       "      <td>0.260679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031337</td>\n",
       "      <td>0.127345</td>\n",
       "      <td>-0.156634</td>\n",
       "      <td>0.040992</td>\n",
       "      <td>0.011071</td>\n",
       "      <td>0.031052</td>\n",
       "      <td>0.088730</td>\n",
       "      <td>0.023151</td>\n",
       "      <td>0.153120</td>\n",
       "      <td>-0.023730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26360</th>\n",
       "      <td>0.017500</td>\n",
       "      <td>-0.300359</td>\n",
       "      <td>0.021210</td>\n",
       "      <td>0.015012</td>\n",
       "      <td>-0.469090</td>\n",
       "      <td>0.019991</td>\n",
       "      <td>-0.308178</td>\n",
       "      <td>0.043931</td>\n",
       "      <td>-0.017987</td>\n",
       "      <td>0.220499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108845</td>\n",
       "      <td>0.048778</td>\n",
       "      <td>-0.019685</td>\n",
       "      <td>0.022650</td>\n",
       "      <td>-0.087909</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>-0.118320</td>\n",
       "      <td>0.112882</td>\n",
       "      <td>0.055183</td>\n",
       "      <td>-0.101745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26055 rows × 1900 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0      0.018954 -0.054595 -0.017604 -0.000359 -0.016081  0.059523 -0.501822   \n",
       "1      0.012356  0.019349  0.035545 -0.011909 -0.593642  0.042230 -0.398359   \n",
       "2      0.039828 -0.024319 -0.008493  0.004779 -0.065900  0.027548 -0.446510   \n",
       "3      0.010476 -0.037006  0.002051  0.000836 -0.113214 -0.004195 -0.430979   \n",
       "4      0.002385 -0.018008  0.031652 -0.027014  0.289688  0.015430 -0.044752   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "26356  0.018876 -0.159777  0.018811  0.017510 -0.200810  0.044610 -0.373076   \n",
       "26357  0.023861 -0.111823 -0.011121  0.021527 -0.123212  0.016969 -0.357857   \n",
       "26358  0.022080 -0.003209 -0.003715  0.021938 -0.025617  0.031697 -0.461362   \n",
       "26359  0.024670 -0.043484 -0.004321  0.021306 -0.078735  0.057776 -0.518633   \n",
       "26360  0.017500 -0.300359  0.021210  0.015012 -0.469090  0.019991 -0.308178   \n",
       "\n",
       "              7         8         9  ...      1890      1891      1892  \\\n",
       "0     -0.062431 -0.006975  0.082645  ...  0.012340  0.001870 -0.037209   \n",
       "1     -0.004493 -0.008300  0.203590  ...  0.052349  0.069966 -0.025274   \n",
       "2      0.003079 -0.009969  0.101370  ...  0.011978  0.045198 -0.029640   \n",
       "3     -0.001758 -0.006596  0.178857  ...  0.006351  0.012263 -0.027498   \n",
       "4     -0.010881 -0.004009 -0.017476  ... -0.003778 -0.035071 -0.254518   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "26356  0.008507 -0.022264  0.264808  ...  0.026245  0.104505 -0.034343   \n",
       "26357  0.004921 -0.033366  0.052102  ...  0.054554 -0.064859 -0.112490   \n",
       "26358 -0.005910 -0.034239  0.192345  ...  0.034636  0.073107 -0.159889   \n",
       "26359 -0.009307 -0.026592  0.260679  ...  0.031337  0.127345 -0.156634   \n",
       "26360  0.043931 -0.017987  0.220499  ...  0.108845  0.048778 -0.019685   \n",
       "\n",
       "           1893      1894      1895      1896      1897      1898      1899  \n",
       "0      0.013662 -0.023809 -0.080424  0.109916  0.043068  0.481828 -0.140297  \n",
       "1      0.016474 -0.088788  0.042309 -0.001724  0.056446  0.450799  0.085444  \n",
       "2      0.005333 -0.029539 -0.107355  0.074577  0.032368  0.722810 -0.157371  \n",
       "3      0.040094 -0.036113 -0.013577 -0.024200  0.011160  0.923180  0.012899  \n",
       "4      0.003655 -0.109212  0.116663  0.320068  0.094852 -0.064797 -0.009261  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "26356  0.024809 -0.090041  0.036515  0.066031 -0.075598  0.126426  0.031520  \n",
       "26357  0.034962  0.005067  0.007011  0.074737 -0.008404  0.156601  0.068240  \n",
       "26358  0.020721  0.013630  0.005062  0.226136  0.117120  0.307682  0.020546  \n",
       "26359  0.040992  0.011071  0.031052  0.088730  0.023151  0.153120 -0.023730  \n",
       "26360  0.022650 -0.087909  0.001578 -0.118320  0.112882  0.055183 -0.101745  \n",
       "\n",
       "[26055 rows x 1900 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to = TabularPandas(df_new , cont_names  =reps_columns,\n",
    "                   y_names='class', y_block = CategoryBlock,\n",
    "                   splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_new.loc[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwZkln6HmX6Z"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(0,30):\n",
    "    p = learner.predict(df_new.iloc[i])\n",
    "    preds.append(p)\n",
    "\n",
    "#dl = learner.dls.test_dl(df_new)\n",
    "#_preds,_none ,_y =learner.get_preds(dl=dl, with_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-e2a156e783dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/jupyter_NB/lib/python3.8/site-packages/fastai/tabular/learner.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;34m\"Predict on a Pandas Series\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_decoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter_NB/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5138\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_frame'"
     ]
    }
   ],
   "source": [
    " learner.predict(df_new.loc[2:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'_FakeLoader' object has no attribute 'persistent_workers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-58d2af148997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_preds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_none\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_decoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/jupyter_NB/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(self, ds_idx, dl, with_input, with_decoded, with_loss, act, inner, reorder, cbs, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_loss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mctx_mgrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_not_reduced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_mgrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'activation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter_NB/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch_validate\u001b[0;34m(self, ds_idx, dl)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelValidException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter_NB/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;34m;\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;34m;\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter_NB/lib/python3.8/site-packages/fastai/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter_NB/lib/python3.8/site-packages/fastai/data/load.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__idxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_idxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# called in context of main process (not workers/subprocesses)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter_NB/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BaseDataLoaderIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter_NB/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_seed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_FakeLoader' object has no attribute 'persistent_workers'"
     ]
    }
   ],
   "source": [
    "dl = learner.dls.test_dl(df_new.loc[2:3])\n",
    "_preds,_none ,_y = learner.get_preds(dl=dl, with_decoded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "FASTAI_predictor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
